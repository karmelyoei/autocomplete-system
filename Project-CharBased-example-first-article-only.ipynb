{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "339eafae",
   "metadata": {
    "id": "339eafae"
   },
   "source": [
    "# Final Project NLP Smart Scientific Content Auto-completion System\n",
    "\n",
    "\n",
    "a text auto-complete system that supports scientists in writing articles in:\n",
    "- completing some typed words/letters.\n",
    "- Identifying words/phrases that are very unlikely to occur in some text contexts and suggesting proper replacements.\n",
    "\n",
    "\n",
    "Karmel Salah\n",
    "15/4/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FZVlVeATK31U",
   "metadata": {
    "id": "FZVlVeATK31U"
   },
   "source": [
    "# We will use Colab for a Free GPU so if you are using Colab frist connect to your drive to get the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ntkBblK8IURV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntkBblK8IURV",
    "outputId": "582dff94-4d99-4f0d-e2c5-f80a6b29def6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "J-f70JR2IvsP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J-f70JR2IvsP",
    "outputId": "82d9f6ff-762d-46da-b5c7-bda6367d0397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./drive/MyDrive/dataset.zip\n",
      "   creating: dataset/\n",
      "  inflating: dataset/1012.2609.txt   \n",
      "  inflating: dataset/1207.1847v1.txt  \n",
      "  inflating: dataset/1209.3126v1.txt  \n",
      "  inflating: dataset/1210.0852.txt   \n",
      "  inflating: dataset/1305.6143.txt   \n",
      "  inflating: dataset/1306.6802v2.txt  \n",
      "  inflating: dataset/1308.0850v5.txt  \n",
      "  inflating: dataset/1309.4168v1.txt  \n",
      "  inflating: dataset/1312.3005v3.txt  \n",
      "  inflating: dataset/1403.6652v2.txt  \n",
      "  inflating: dataset/1404.3377v1.txt  \n",
      "  inflating: dataset/1404.4326v1.txt  \n",
      "  inflating: dataset/1405.4053v2.txt  \n",
      "  inflating: dataset/1405.4273v1.txt  \n",
      "  inflating: dataset/1406.1078v3.txt  \n",
      "  inflating: dataset/1408.5882v2.txt  \n",
      "  inflating: dataset/1409.0473v7.txt  \n",
      "  inflating: dataset/1409.2329v5.txt  \n",
      "  inflating: dataset/1409.3215v3.txt  \n",
      "  inflating: dataset/1410.5329v4.txt  \n",
      "  inflating: dataset/1410.8206v4.txt  \n",
      "  inflating: dataset/1411.4555v2.txt  \n",
      "  inflating: dataset/1412.1058v2.txt  \n",
      "  inflating: dataset/1412.2007v2.txt  \n",
      "  inflating: dataset/1412.5335v7.txt  \n",
      "  inflating: dataset/1412.6622v4.txt  \n",
      "  inflating: dataset/1412.7753v2.txt  \n",
      "  inflating: dataset/1502.01710v5.txt  \n",
      "  inflating: dataset/1503.00075v3.txt  \n",
      "  inflating: dataset/1503.06733v2.txt  \n",
      "  inflating: dataset/1503.08895v5.txt  \n",
      "  inflating: dataset/1505.00641v3.txt  \n",
      "  inflating: dataset/1505.07818v4.txt  \n",
      "  inflating: dataset/1506.00019v4.txt  \n",
      "  inflating: dataset/1506.03140v2.txt  \n",
      "  inflating: dataset/1506.07285v5.txt  \n",
      "  inflating: dataset/1507.07998v1.txt  \n",
      "  inflating: dataset/1508.01211v2.txt  \n",
      "  inflating: dataset/1508.01745v2.txt  \n",
      "  inflating: dataset/1508.02096v2.txt  \n",
      "  inflating: dataset/1508.04025v5.txt  \n",
      "  inflating: dataset/1508.04395v2.txt  \n",
      "  inflating: dataset/1508.06034v1.txt  \n",
      "  inflating: dataset/1508.06615v4.txt  \n",
      "  inflating: dataset/1508.07909v5.txt  \n",
      "  inflating: dataset/1509.01626v3.txt  \n",
      "  inflating: dataset/1509.01626v3_18-29-38.txt  \n",
      "  inflating: dataset/1509.01626v3_18-29-55.txt  \n",
      "  inflating: dataset/1510.01784v1.txt  \n",
      "  inflating: dataset/1510.03820v4.txt  \n",
      "  inflating: dataset/1511.01432v1.txt  \n",
      "  inflating: dataset/1511.02301v4.txt  \n",
      "  inflating: dataset/1511.03745v4.txt  \n",
      "  inflating: dataset/1511.04590v5.txt  \n",
      "  inflating: dataset/1511.06349v4.txt  \n",
      "  inflating: dataset/1511.06391v4.txt  \n",
      "  inflating: dataset/1511.06456v4.txt  \n",
      "  inflating: dataset/1511.06732v7.txt  \n",
      "  inflating: dataset/1511.06909v7.txt  \n",
      "  inflating: dataset/1511.06939v4.txt  \n",
      "  inflating: dataset/1511.08630v2.txt  \n",
      "  inflating: dataset/1512.01337v4.txt  \n",
      "  inflating: dataset/1512.04906v1.txt  \n",
      "  inflating: dataset/1512.05287v5.txt  \n",
      "  inflating: dataset/1512.07982v1.txt  \n",
      "  inflating: dataset/1512.08183v5.txt  \n",
      "  inflating: dataset/1601.01892v2.txt  \n",
      "  inflating: dataset/1601.03313v2.txt  \n",
      "  inflating: dataset/1601.04811v6.txt  \n",
      "  inflating: dataset/1601.06733v7.txt  \n",
      "  inflating: dataset/1602.02410v2.txt  \n",
      "  inflating: dataset/1602.05568v1.txt  \n",
      "  inflating: dataset/1602.06023v5.txt  \n",
      "  inflating: dataset/1602.07776v4.txt  \n",
      "  inflating: dataset/1602.07783v2.txt  \n",
      "  inflating: dataset/1603.01913v2.txt  \n",
      "  inflating: dataset/1603.03116v3.txt  \n",
      "  inflating: dataset/1603.03827v1.txt  \n",
      "  inflating: dataset/1603.04466v1.txt  \n",
      "  inflating: dataset/1603.06075v3.txt  \n",
      "  inflating: dataset/1603.06147v4.txt  \n",
      "  inflating: dataset/1603.06318v5.txt  \n",
      "  inflating: dataset/1603.06393v3.txt  \n",
      "  inflating: dataset/1603.06744v2.txt  \n",
      "  inflating: dataset/1603.06807v2.txt  \n",
      "  inflating: dataset/1603.07771v3.txt  \n",
      "  inflating: dataset/1603.08861v2.txt  \n",
      "  inflating: dataset/1603.08884v1.txt  \n",
      "  inflating: dataset/1603.09025v5.txt  \n",
      "  inflating: dataset/1603.09457v1.txt  \n",
      "  inflating: dataset/1603.09727v1.txt  \n",
      "  inflating: dataset/1604.00837v1.txt  \n",
      "  inflating: dataset/1604.02201v1.txt  \n",
      "  inflating: dataset/1604.02748v2.txt  \n",
      "  inflating: dataset/1604.03390v2.txt  \n",
      "  inflating: dataset/1604.03489v2.txt  \n",
      "  inflating: dataset/1605.00937v2.txt  \n",
      "  inflating: dataset/1605.04469v3.txt  \n",
      "  inflating: dataset/1605.04655v2.txt  \n",
      "  inflating: dataset/1605.05101v1.txt  \n",
      "  inflating: dataset/1605.07422v3.txt  \n",
      "  inflating: dataset/1605.07722v3.txt  \n",
      "  inflating: dataset/1605.07725v3.txt  \n",
      "  inflating: dataset/1605.09186v4.txt  \n",
      "  inflating: dataset/1605.09507v3.txt  \n",
      "  inflating: dataset/1606.00253v1.txt  \n",
      "  inflating: dataset/1606.00499v2.txt  \n",
      "  inflating: dataset/1606.00776v2.txt  \n",
      "  inflating: dataset/1606.00931v3.txt  \n",
      "  inflating: dataset/1606.01305v4.txt  \n",
      "  inflating: dataset/1606.01781v2.txt  \n",
      "  inflating: dataset/1606.02006v2.txt  \n",
      "  inflating: dataset/1606.02892v2.txt  \n",
      "  inflating: dataset/1606.02960v2.txt  \n",
      "  inflating: dataset/1606.03821v2.txt  \n",
      "  inflating: dataset/1606.04080v2.txt  \n",
      "  inflating: dataset/1606.04582v6.txt  \n",
      "  inflating: dataset/1606.04621v3.txt  \n",
      "  inflating: dataset/1606.04838v3.txt  \n",
      "  inflating: dataset/1606.05819v1.txt  \n",
      "  inflating: dataset/1606.06259.txt  \n",
      "  inflating: dataset/1606.07659v3.txt  \n",
      "  inflating: dataset/1606.07947v4.txt  \n",
      "  inflating: dataset/1607.01759v3.txt  \n",
      "  inflating: dataset/1607.02501v2.txt  \n",
      "  inflating: dataset/1607.03474v5.txt  \n",
      "  inflating: dataset/1607.04228v1.txt  \n",
      "  inflating: dataset/1607.04315v3.txt  \n",
      "  inflating: dataset/1607.07086v3.txt  \n",
      "  inflating: dataset/1608.00272v3.txt  \n",
      "  inflating: dataset/1608.02021v1.txt  \n",
      "  inflating: dataset/1608.04738v2.txt  \n",
      "  inflating: dataset/1608.06043v3.txt  \n",
      "  inflating: dataset/1608.07076v1.txt  \n",
      "  inflating: dataset/1609.03976v1.txt  \n",
      "  inflating: dataset/1609.05244v2.txt  \n",
      "  inflating: dataset/1609.05473v6.txt  \n",
      "  inflating: dataset/1609.06686v1.txt  \n",
      "  inflating: dataset/1609.08144v2.txt  \n",
      "  inflating: dataset/1609.08264v1.txt  \n",
      "  inflating: dataset/1609.08359v2.txt  \n",
      "  inflating: dataset/1609.09552v1.txt  \n",
      "  inflating: dataset/1610.01588v3.txt  \n",
      "  inflating: dataset/1610.03017v3.txt  \n",
      "  inflating: dataset/1610.05838v3.txt  \n",
      "  inflating: dataset/1610.08613v2.txt  \n",
      "  inflating: dataset/1610.09225v1.txt  \n",
      "  inflating: dataset/1610.09565v1.txt  \n",
      "  inflating: dataset/1610.10099v2.txt  \n",
      "  inflating: dataset/1611.00144v1.txt  \n",
      "  inflating: dataset/1611.00472v1.txt  \n",
      "  inflating: dataset/1611.01368v1.txt  \n",
      "  inflating: dataset/1611.01462v3.txt  \n",
      "  inflating: dataset/1611.01576v2.txt  \n",
      "  inflating: dataset/1611.01578v2.txt  \n",
      "  inflating: dataset/1611.01874v2.txt  \n",
      "  inflating: dataset/1611.01884v3.txt  \n",
      "  inflating: dataset/1611.02344v3.txt  \n",
      "  inflating: dataset/1611.02639v2.txt  \n",
      "  inflating: dataset/1611.04358v2.txt  \n",
      "  inflating: dataset/1611.07804v1.txt  \n",
      "  inflating: dataset/1611.08307v1.txt  \n",
      "  inflating: dataset/1611.08480v2.txt  \n",
      "  inflating: dataset/1611.09414v2.txt  \n",
      "  inflating: dataset/1612.00385v2.txt  \n",
      "  inflating: dataset/1612.01887v2.txt  \n",
      "  inflating: dataset/1612.03651v1.txt  \n",
      "  inflating: dataset/1612.04426v1.txt  \n",
      "  inflating: dataset/1612.06935v6.txt  \n",
      "  inflating: dataset/1612.07640v1.txt  \n",
      "  inflating: dataset/1612.08083v3.txt  \n",
      "  inflating: dataset/1701.02810v2.txt  \n",
      "  inflating: dataset/1701.02870v3.txt  \n",
      "  inflating: dataset/1701.04831v2.txt  \n",
      "  inflating: dataset/1701.06511v3.txt  \n",
      "  inflating: dataset/1701.07274v6.txt  \n",
      "  inflating: dataset/1702.00887v3.txt  \n",
      "  inflating: dataset/1702.01417v2.txt  \n",
      "  inflating: dataset/1702.01824v2.txt  \n",
      "  inflating: dataset/1702.02390v1.txt  \n",
      "  inflating: dataset/1702.08400v3.txt  \n",
      "  inflating: dataset/1702.08431v4.txt  \n",
      "  inflating: dataset/1703.00535v3.txt  \n",
      "  inflating: dataset/1703.01442v1.txt  \n",
      "  inflating: dataset/1703.02504v1.txt  \n",
      "  inflating: dataset/1703.02620v1.txt  \n",
      "  inflating: dataset/1703.03130v1.txt  \n",
      "  inflating: dataset/1703.03906v2.txt  \n",
      "  inflating: dataset/1703.04247v1.txt  \n",
      "  inflating: dataset/1703.04357v1.txt  \n",
      "  inflating: dataset/1703.06103v4.txt  \n",
      "  inflating: dataset/1703.09570.txt  \n",
      "  inflating: dataset/1703.10152v1.txt  \n",
      "  inflating: dataset/1703.10722v3.txt  \n",
      "  inflating: dataset/1703.10931v2.txt  \n",
      "  inflating: dataset/1703.10960v3.txt  \n",
      "  inflating: dataset/1704.00784v2.txt  \n",
      "  inflating: dataset/1704.01087v1.txt  \n",
      "  inflating: dataset/1704.01444v2.txt  \n",
      "  inflating: dataset/1704.02798v4.txt  \n",
      "  inflating: dataset/1704.04368v2.txt  \n",
      "  inflating: dataset/1704.06125v1.txt  \n",
      "  inflating: dataset/1704.06803v1.txt  \n",
      "  inflating: dataset/1704.06960v5.txt  \n",
      "  inflating: dataset/1704.07138v2.txt  \n",
      "  inflating: dataset/1704.07156v1.txt  \n",
      "  inflating: dataset/1704.08803v2.txt  \n",
      "  inflating: dataset/1705.00823v1.txt  \n",
      "  inflating: dataset/1705.03122v3.txt  \n",
      "  inflating: dataset/1705.05311v2.txt  \n",
      "  inflating: dataset/1705.05414v2.txt  \n",
      "  inflating: dataset/1705.06463v1.txt  \n",
      "  inflating: dataset/1705.07393v2.txt  \n",
      "  inflating: dataset/1705.07704v3.txt  \n",
      "  inflating: dataset/1705.07830v3.txt  \n",
      "  inflating: dataset/1705.09037v3.txt  \n",
      "  inflating: dataset/1705.09655v2.txt  \n",
      "  inflating: dataset/1705.10513v2.txt  \n",
      "  inflating: dataset/1706.00043v2.txt  \n",
      "  inflating: dataset/1706.00061v1.txt  \n",
      "  inflating: dataset/1706.00188v1.txt  \n",
      "  inflating: dataset/1706.00457v1.txt  \n",
      "  inflating: dataset/1706.01399v3.txt  \n",
      "  inflating: dataset/1706.01449v3.txt  \n",
      "  inflating: dataset/1706.02263v2.txt  \n",
      "  inflating: dataset/1706.02459v1.txt  \n",
      "  inflating: dataset/1706.03059v2.txt  \n",
      "  inflating: dataset/1706.03196v1.txt  \n",
      "  inflating: dataset/1706.03471v2.txt  \n",
      "  inflating: dataset/1706.03762v5.txt  \n",
      "  inflating: dataset/1706.05565v8.txt  \n",
      "  inflating: dataset/1706.06363v1.txt  \n",
      "  inflating: dataset/1706.06415v1.txt  \n",
      "  inflating: dataset/1706.07206v2.txt  \n",
      "  inflating: dataset/1706.09799v1.txt  \n",
      "  inflating: dataset/1707.01166v3.txt  \n",
      "  inflating: dataset/1707.01780v3.txt  \n",
      "  inflating: dataset/1707.02275v1.txt  \n",
      "  inflating: dataset/1707.02377v1.txt  \n",
      "  inflating: dataset/1707.02485v1.txt  \n",
      "  inflating: dataset/1707.02786v4.txt  \n",
      "  inflating: dataset/1707.02968v2.txt  \n",
      "  inflating: dataset/1707.04412v1.txt  \n",
      "  inflating: dataset/1707.06885v1.txt  \n",
      "  inflating: dataset/1707.07402v4.txt  \n",
      "  inflating: dataset/1707.07847v3.txt  \n",
      "  inflating: dataset/1707.08052v1.txt  \n",
      "  inflating: dataset/1707.09569v1.txt  \n",
      "  inflating: dataset/1708.00077v1.txt  \n",
      "  inflating: dataset/1708.00107v2.txt  \n",
      "  inflating: dataset/1708.00524v2.txt  \n",
      "  inflating: dataset/1708.02182v1.txt  \n",
      "  inflating: dataset/1708.02657v2.txt  \n",
      "  inflating: dataset/1708.02702v4.txt  \n",
      "  inflating: dataset/1708.03629v3.txt  \n",
      "  inflating: dataset/1708.04439v2.txt  \n",
      "  inflating: dataset/1708.04729v3.txt  \n",
      "  inflating: dataset/1708.05031.txt  \n",
      "  inflating: dataset/1708.05045v2.txt  \n",
      "  inflating: dataset/1708.05891v1.txt  \n",
      "  inflating: dataset/1709.01584v2.txt  \n",
      "  inflating: dataset/1709.02755v5.txt  \n",
      "  inflating: dataset/1709.02984.txt  \n",
      "  inflating: dataset/1709.03082v8.txt  \n",
      "  inflating: dataset/1709.03714v1.txt  \n",
      "  inflating: dataset/1709.03856v5.txt  \n",
      "  inflating: dataset/1709.04054v3.txt  \n",
      "  inflating: dataset/1709.04396v2.txt  \n",
      "  inflating: dataset/1709.05074v1.txt  \n",
      "  inflating: dataset/1709.06671v1.txt  \n",
      "  inflating: dataset/1709.07417v2.txt  \n",
      "  inflating: dataset/1709.07432v2.txt  \n",
      "  inflating: dataset/1709.07809v1.txt  \n",
      "  inflating: dataset/1709.08267v2.txt  \n",
      "  inflating: dataset/1709.08624v2.txt  \n",
      "  inflating: dataset/1709.08878v2.txt  \n",
      "  inflating: dataset/1709.09500v1.txt  \n",
      "  inflating: dataset/1710.00482v1.txt  \n",
      "  inflating: dataset/1710.02318v1.txt  \n",
      "  inflating: dataset/1710.04087v3.txt  \n",
      "  inflating: dataset/1710.05649.txt  \n",
      "  inflating: dataset/1710.06071v1.txt  \n",
      "  inflating: dataset/1710.09537v1.txt  \n",
      "  inflating: dataset/1710.10296.txt  \n",
      "  inflating: dataset/1710.11035v2.txt  \n",
      "  inflating: dataset/1710.11342v2.txt  \n",
      "  inflating: dataset/1711.00043v2.txt  \n",
      "  inflating: dataset/1711.00066v4.txt  \n",
      "  inflating: dataset/1711.00350v3.txt  \n",
      "  inflating: dataset/1711.01068v2.txt  \n",
      "  inflating: dataset/1711.01731v3.txt  \n",
      "  inflating: dataset/1711.02013v2.txt  \n",
      "  inflating: dataset/1711.02132v1.txt  \n",
      "  inflating: dataset/1711.02281v2.txt  \n",
      "  inflating: dataset/1711.03953v4.txt  \n",
      "  inflating: dataset/1711.04956v5.txt  \n",
      "  inflating: dataset/1711.06104v4.txt  \n",
      "  inflating: dataset/1711.07601v1.txt  \n",
      "  inflating: dataset/1711.09151v1.txt  \n",
      "  inflating: dataset/1711.09357v1.txt  \n",
      "  inflating: dataset/1711.09645v2.txt  \n",
      "  inflating: dataset/1711.09724v1.txt  \n",
      "  inflating: dataset/1712.02616v3.txt  \n",
      "  inflating: dataset/1712.05690v2.txt  \n",
      "  inflating: dataset/1712.05846v2.txt  \n",
      "  inflating: dataset/1712.05972v2.txt  \n",
      "  inflating: dataset/1712.06751v2.txt  \n",
      "  inflating: dataset/1712.07040v1.txt  \n",
      "  inflating: dataset/1712.07525.txt  \n",
      "  inflating: dataset/1712.09948v1.txt  \n",
      "  inflating: dataset/1801.00209v3.txt  \n",
      "  inflating: dataset/1801.00632v2.txt  \n",
      "  inflating: dataset/1801.01315v1.txt  \n",
      "  inflating: dataset/1801.02203v1.txt  \n",
      "  inflating: dataset/1801.02294v5.txt  \n",
      "  inflating: dataset/1801.04354v5.txt  \n",
      "  inflating: dataset/1801.06146v5.txt  \n",
      "  inflating: dataset/1801.06261v2.txt  \n",
      "  inflating: dataset/1801.07736v3.txt  \n",
      "  inflating: dataset/1801.08284v2.txt  \n",
      "  inflating: dataset/1801.08831v1.txt  \n",
      "  inflating: dataset/1801.09251v2.txt  \n",
      "  inflating: dataset/1801.09797v1.txt  \n",
      "  inflating: dataset/1801.10308v1.txt  \n",
      "  inflating: dataset/1802.00889v1.txt  \n",
      "  inflating: dataset/1802.00923v1.txt  \n",
      "  inflating: dataset/1802.00924v1.txt  \n",
      "  inflating: dataset/1802.01345v3.txt  \n",
      "  inflating: dataset/1802.02550v7.txt  \n",
      "  inflating: dataset/1802.03238v2.txt  \n",
      "  inflating: dataset/1802.03268v2.txt  \n",
      "  inflating: dataset/1802.03594v2.txt  \n",
      "  inflating: dataset/1802.03938v1.txt  \n",
      "  inflating: dataset/1802.04051v4.txt  \n",
      "  inflating: dataset/1802.04591v2.txt  \n",
      "  inflating: dataset/1802.05335v3.txt  \n",
      "  inflating: dataset/1802.05365v2.txt  \n",
      "  inflating: dataset/1802.05694v1.txt  \n",
      "  inflating: dataset/1802.05814v1.txt  \n",
      "  inflating: dataset/1802.06182v1.txt  \n",
      "  inflating: dataset/1802.06901v3.txt  \n",
      "  inflating: dataset/1802.08452v1.txt  \n",
      "  inflating: dataset/1802.09957v1.txt  \n",
      "  inflating: dataset/1803.00114v3.txt  \n",
      "  inflating: dataset/1803.00188v1.txt  \n",
      "  inflating: dataset/1803.01271v2.txt  \n",
      "  inflating: dataset/1803.02155v2.txt  \n",
      "  inflating: dataset/1803.02218v1.txt  \n",
      "  inflating: dataset/1803.02781v3.txt  \n",
      "  inflating: dataset/1803.02879v2.txt  \n",
      "  inflating: dataset/1803.03467v4.txt  \n",
      "  inflating: dataset/1803.03816v2.txt  \n",
      "  inflating: dataset/1803.05030v1.txt  \n",
      "  inflating: dataset/1803.07416v1.txt  \n",
      "  inflating: dataset/1803.08240v1.txt  \n",
      "  inflating: dataset/1803.09065v3.txt  \n",
      "  inflating: dataset/1803.10109v1.txt  \n",
      "  inflating: dataset/1803.11175v2.txt  \n",
      "  inflating: dataset/1803.11175v2_18-30-23.txt  \n",
      "  inflating: dataset/1804.00247v2.txt  \n",
      "  inflating: dataset/1804.00538v4.txt  \n",
      "  inflating: dataset/1804.00823v4.txt  \n",
      "  inflating: dataset/1804.02063v1.txt  \n",
      "  inflating: dataset/1804.04095v1.txt  \n",
      "  inflating: dataset/1804.04950v2.txt  \n",
      "  inflating: dataset/1804.06087v1.txt  \n",
      "  inflating: dataset/1804.06323v2.txt  \n",
      "  inflating: dataset/1804.07755v2.txt  \n",
      "  inflating: dataset/1804.07827v2.txt  \n",
      "  inflating: dataset/1804.07998v2.txt  \n",
      "  inflating: dataset/1804.08069v1.txt  \n",
      "  inflating: dataset/1804.08166v1.txt  \n",
      "  inflating: dataset/1804.08771v2.txt  \n",
      "  inflating: dataset/1804.08875v1.txt  \n",
      "  inflating: dataset/1804.09541v1.txt  \n",
      "  inflating: dataset/1804.09779v2.txt  \n",
      "  inflating: dataset/1804.09849v2.txt  \n",
      "  inflating: dataset/1804.10862.txt  \n",
      "  inflating: dataset/1804.10959v1.txt  \n",
      "  inflating: dataset/1804.11019v1.txt  \n",
      "  inflating: dataset/1804.11258v3.txt  \n",
      "  inflating: dataset/1805.01070v2.txt  \n",
      "  inflating: dataset/1805.02473v3.txt  \n",
      "  inflating: dataset/1805.02474v1.txt  \n",
      "  inflating: dataset/1805.03294v1.txt  \n",
      "  inflating: dataset/1805.03352v2.txt  \n",
      "  inflating: dataset/1805.04174v1.txt  \n",
      "  inflating: dataset/1805.04437v1.txt  \n",
      "  inflating: dataset/1805.04601v1.txt  \n",
      "  inflating: dataset/1805.06064v1.txt  \n",
      "  inflating: dataset/1805.06201v1.txt  \n",
      "  inflating: dataset/1805.07030v1.txt  \n",
      "  inflating: dataset/1805.07043v1.txt  \n",
      "  inflating: dataset/1805.07513v1.txt  \n",
      "  inflating: dataset/1805.08159v2.txt  \n",
      "  inflating: dataset/1805.08297v1.txt  \n",
      "  inflating: dataset/1805.08705v2.txt  \n",
      "  inflating: dataset/1805.09016v1.txt  \n",
      "  inflating: dataset/1805.09461v4.txt  \n",
      "  inflating: dataset/1805.10212v1.txt  \n",
      "  inflating: dataset/1805.10387v2.txt  \n",
      "  inflating: dataset/1805.11462v1.txt  \n",
      "  inflating: dataset/1806.00187v3.txt  \n",
      "  inflating: dataset/1806.01501v1.txt  \n",
      "  inflating: dataset/1806.01822v2.txt  \n",
      "  inflating: dataset/1806.02557v2.txt  \n",
      "  inflating: dataset/1806.02960v1.txt  \n",
      "  inflating: dataset/1806.03529v2.txt  \n",
      "  inflating: dataset/1806.04381v2.txt  \n",
      "  inflating: dataset/1806.05219v2.txt  \n",
      "  inflating: dataset/1806.05507v1.txt  \n",
      "  inflating: dataset/1806.05516v1.txt  \n",
      "  inflating: dataset/1806.05559v2.txt  \n",
      "  inflating: dataset/1806.06219v3.txt  \n",
      "  inflating: dataset/1806.06228v1.txt  \n",
      "  inflating: dataset/1806.06259v1.txt  \n",
      "  inflating: dataset/1806.08462v2.txt  \n",
      "  inflating: dataset/1806.08730v1.txt  \n",
      "  inflating: dataset/1806.09055v2.txt  \n",
      "  inflating: dataset/1806.09828v1.txt  \n",
      "  inflating: dataset/1806.09835v1.txt  \n",
      "  inflating: dataset/1806.10478v2.txt  \n",
      "  inflating: dataset/1807.00311v1.txt  \n",
      "  inflating: dataset/1807.02291.txt  \n",
      "  inflating: dataset/1807.02478v1.txt  \n",
      "  inflating: dataset/1807.03491v1.txt  \n",
      "  inflating: dataset/1807.03819v3.txt  \n",
      "  inflating: dataset/1807.04271v3.txt  \n",
      "  inflating: dataset/1807.04990v1.txt  \n",
      "  inflating: dataset/1807.06786v1.txt  \n",
      "  inflating: dataset/1807.07741v1.txt  \n",
      "  inflating: dataset/1808.00076v3.txt  \n",
      "  inflating: dataset/1808.00720v2.txt  \n",
      "  inflating: dataset/1808.01371v2.txt  \n",
      "  inflating: dataset/1808.02733v1.txt  \n",
      "  inflating: dataset/1808.03867v3.txt  \n",
      "  inflating: dataset/1808.03908v1.txt  \n",
      "  inflating: dataset/1808.04189v1.txt  \n",
      "  inflating: dataset/1808.04469v2.txt  \n",
      "  inflating: dataset/1808.04819v1.txt  \n",
      "  inflating: dataset/1808.05326v1.txt  \n",
      "  inflating: dataset/1808.05505v3.txt  \n",
      "  inflating: dataset/1808.05784v2.txt  \n",
      "  inflating: dataset/1808.06161v1.txt  \n",
      "  inflating: dataset/1808.06226v1.txt  \n",
      "  inflating: dataset/1808.07233v5.txt  \n",
      "  inflating: dataset/1808.07325.txt  \n",
      "  inflating: dataset/1808.07733v1.txt  \n",
      "  inflating: dataset/1808.08703v2.txt  \n",
      "  inflating: dataset/1808.08795v1.txt  \n",
      "  inflating: dataset/1808.08931v2.txt  \n",
      "  inflating: dataset/1808.09031v1.txt  \n",
      "  inflating: dataset/1808.09160v1.txt  \n",
      "  inflating: dataset/1808.09644v1.txt  \n",
      "  inflating: dataset/1808.09744v1.txt  \n",
      "  inflating: dataset/1808.09781v1.txt  \n",
      "  inflating: dataset/1808.10000v1.txt  \n",
      "  inflating: dataset/1808.10122v3.txt  \n",
      "  inflating: dataset/1808.10245v1.txt  \n",
      "  inflating: dataset/1808.10805v2.txt  \n",
      "  inflating: dataset/1809.00366v1.txt  \n",
      "  inflating: dataset/1809.00530v1.txt  \n",
      "  inflating: dataset/1809.00582v2.txt  \n",
      "  inflating: dataset/1809.00717v1.txt  \n",
      "  inflating: dataset/1809.00794v2.txt  \n",
      "  inflating: dataset/1809.01272v1.txt  \n",
      "  inflating: dataset/1809.01478v2.txt  \n",
      "  inflating: dataset/1809.01576v2.txt  \n",
      "  inflating: dataset/1809.01694v2.txt  \n",
      "  inflating: dataset/1809.01797v2.txt  \n",
      "  inflating: dataset/1809.01829v4.txt  \n",
      "  inflating: dataset/1809.02836v1.txt  \n",
      "  inflating: dataset/1809.03999v1.txt  \n",
      "  inflating: dataset/1809.05053v1.txt  \n",
      "  inflating: dataset/1809.05255v2.txt  \n",
      "  inflating: dataset/1809.05679v3.txt  \n",
      "  inflating: dataset/1809.06858v1.txt  \n",
      "  inflating: dataset/1809.06963v3.txt  \n",
      "  inflating: dataset/1809.07428v1.txt  \n",
      "  inflating: dataset/1809.08037v2.txt  \n",
      "  inflating: dataset/1809.08353v2.txt  \n",
      "  inflating: dataset/1809.08370v1.txt  \n",
      "  inflating: dataset/1809.10324v2.txt  \n",
      "  inflating: dataset/1809.10853v3.txt  \n",
      "  inflating: dataset/1810.00494v1.txt  \n",
      "  inflating: dataset/1810.00952v1.txt  \n",
      "  inflating: dataset/1810.01170v1.txt  \n",
      "  inflating: dataset/1810.01861v2.txt  \n",
      "  inflating: dataset/1810.03167v1.txt  \n",
      "  inflating: dataset/1810.03552v3.txt  \n",
      "  inflating: dataset/1810.03660v1.txt  \n",
      "  inflating: dataset/1810.04805v2.txt  \n",
      "  inflating: dataset/1810.05334v5.txt  \n",
      "  inflating: dataset/1810.06682v2.txt  \n",
      "  inflating: dataset/1810.06683v3.txt  \n",
      "  inflating: dataset/1810.06825v1.txt  \n",
      "  inflating: dataset/1810.06860v1.txt  \n",
      "  inflating: dataset/1810.07091v1.txt  \n",
      "  inflating: dataset/1810.07150v3.txt  \n",
      "  inflating: dataset/1810.09177v3.txt  \n",
      "  inflating: dataset/1810.09305v1.txt  \n",
      "  inflating: dataset/1810.09311v1.txt  \n",
      "  inflating: dataset/1810.09536v6.txt  \n",
      "  inflating: dataset/1810.09995v1.txt  \n",
      "  inflating: dataset/1810.10181v1.txt  \n",
      "  inflating: dataset/1810.10804v3.txt  \n",
      "  inflating: dataset/1810.11921v2.txt  \n",
      "  inflating: dataset/1810.12836v4.txt  \n",
      "  inflating: dataset/1810.13338v1.txt  \n",
      "  inflating: dataset/1811.00606v2.txt  \n",
      "  inflating: dataset/1811.00854v1.txt  \n",
      "  inflating: dataset/1811.01088v2.txt  \n",
      "  inflating: dataset/1811.01136v2.txt  \n",
      "  inflating: dataset/1811.01713v1.txt  \n",
      "  inflating: dataset/1811.01908v1.txt  \n",
      "  inflating: dataset/1811.01910v2.txt  \n",
      "  inflating: dataset/1811.02084v1.txt  \n",
      "  inflating: dataset/1811.02549v5.txt  \n",
      "  inflating: dataset/1811.04540v1.txt  \n",
      "  inflating: dataset/1811.05082v2.txt  \n",
      "  inflating: dataset/1811.05475.txt  \n",
      "  inflating: dataset/1811.05544v1.txt  \n",
      "  inflating: dataset/1811.05949v1.txt  \n",
      "  inflating: dataset/1811.06965v5.txt  \n",
      "  inflating: dataset/1811.08129v1.txt  \n",
      "  inflating: dataset/1811.08883v1.txt  \n",
      "  inflating: dataset/1811.09362v2.txt  \n",
      "  inflating: dataset/1811.09386v1.txt  \n",
      "  inflating: dataset/1811.10792v3.txt  \n",
      "  inflating: dataset/1811.10804v1.txt  \n",
      "  inflating: dataset/1811.11431v3.txt  \n",
      "  inflating: dataset/1812.00176v1.txt  \n",
      "  inflating: dataset/1812.01187v2.txt  \n",
      "  inflating: dataset/1812.01207v1.txt  \n",
      "  inflating: dataset/1812.01353v5.txt  \n",
      "  inflating: dataset/1812.01504v4.txt  \n",
      "  inflating: dataset/1812.02303v2.txt  \n",
      "  inflating: dataset/1812.02971.txt  \n",
      "  inflating: dataset/1812.03825v1.txt  \n",
      "  inflating: dataset/1812.04412v1.txt  \n",
      "  inflating: dataset/1812.04616v3.txt  \n",
      "  inflating: dataset/1812.05271v1.txt  \n",
      "  inflating: dataset/1812.06705v1.txt  \n",
      "  inflating: dataset/1812.07617v2.txt  \n",
      "  inflating: dataset/1812.07809v1.txt  \n",
      "  inflating: dataset/1812.08928v1.txt  \n",
      "  inflating: dataset/1901.00158v2.txt  \n",
      "  inflating: dataset/1901.00603v2.txt  \n",
      "  inflating: dataset/1901.01703v4.txt  \n",
      "  inflating: dataset/1901.02262v2.txt  \n",
      "  inflating: dataset/1901.02860v3.txt  \n",
      "  inflating: dataset/1901.02985v2.txt  \n",
      "  inflating: dataset/1901.04112v1.txt  \n",
      "  inflating: dataset/1901.04555v2.txt  \n",
      "  inflating: dataset/1901.05534v2.txt  \n",
      "  inflating: dataset/1901.05816v1.txt  \n",
      "  inflating: dataset/1901.07291v1.txt  \n",
      "  inflating: dataset/1901.07696v2.txt  \n",
      "  inflating: dataset/1901.07786v1.txt  \n",
      "  inflating: dataset/1901.08149v2.txt  \n",
      "  inflating: dataset/1901.08634v2.txt  \n",
      "  inflating: dataset/1901.08746v3.txt  \n",
      "  inflating: dataset/1901.08907v1.txt  \n",
      "  inflating: dataset/1901.10430v2.txt  \n",
      "  inflating: dataset/1901.10444v1.txt  \n",
      "  inflating: dataset/1901.10548v4.txt  \n",
      "  inflating: dataset/1901.11117v4.txt  \n",
      "  inflating: dataset/1901.11196v2.txt  \n",
      "  inflating: dataset/1901.11459v2.txt  \n",
      "  inflating: dataset/1901.11504v2.txt  \n",
      "  inflating: dataset/1902.00087v4.txt  \n",
      "  inflating: dataset/1902.00438v2.txt  \n",
      "  inflating: dataset/1902.00863v2.txt  \n",
      "  inflating: dataset/1902.01382v3.txt  \n",
      "  inflating: dataset/1902.05196v1.txt  \n",
      "  inflating: dataset/1902.06022v1.txt  \n",
      "  inflating: dataset/1902.06188v2.txt  \n",
      "  inflating: dataset/1902.06236v1.txt  \n",
      "  inflating: dataset/1902.07153v2.txt  \n",
      "  inflating: dataset/1902.07153v2_18-31-32.txt  \n",
      "  inflating: dataset/1902.07816v2.txt  \n",
      "  inflating: dataset/1902.08009v1.txt  \n",
      "  inflating: dataset/1902.08850v3.txt  \n",
      "  inflating: dataset/1902.08955v1.txt  \n",
      "  inflating: dataset/1902.09113v2.txt  \n",
      "  inflating: dataset/1902.09243v2.txt  \n",
      "  inflating: dataset/1902.09314v2.txt  \n",
      "  inflating: dataset/1902.09362v2.txt  \n",
      "  inflating: dataset/1902.09757v1.txt  \n",
      "  inflating: dataset/1902.10339v4.txt  \n",
      "  inflating: dataset/1902.10461v3.txt  \n",
      "  inflating: dataset/1902.10547v3.txt  \n",
      "  inflating: dataset/1902.10623v2.txt  \n",
      "  inflating: dataset/1902.11049v2.txt  \n",
      "  inflating: dataset/1903.00138.txt  \n",
      "  inflating: dataset/1903.00138v3.txt  \n",
      "  inflating: dataset/1903.00142v1.txt  \n",
      "  inflating: dataset/1903.00241v1.txt  \n",
      "  inflating: dataset/1903.00905v2.txt  \n",
      "  inflating: dataset/1903.02188v3.txt  \n",
      "  inflating: dataset/1903.02831v1.txt  \n",
      "  inflating: dataset/1903.03714v1.txt  \n",
      "  inflating: dataset/1903.04561v2.txt  \n",
      "  inflating: dataset/1903.05734v1.txt  \n",
      "  inflating: dataset/1903.06620v2.txt  \n",
      "  inflating: dataset/1903.07926v2.txt  \n",
      "  inflating: dataset/1903.08289v2.txt  \n",
      "  inflating: dataset/1903.10145v3.txt  \n",
      "  inflating: dataset/1903.10433v1.txt  \n",
      "  inflating: dataset/1903.10520v1.txt  \n",
      "  inflating: dataset/1903.10676v3.txt  \n",
      "  inflating: dataset/1903.11410v2.txt  \n",
      "  inflating: dataset/1903.12090v1.txt  \n",
      "  inflating: dataset/1903.12457v3.txt  \n",
      "  inflating: dataset/1903.12626v1.txt  \n",
      "  inflating: dataset/1904.00132v2.txt  \n",
      "  inflating: dataset/1904.00648v1.txt  \n",
      "  inflating: dataset/1904.01038v1.txt  \n",
      "  inflating: dataset/1904.01301v2.txt  \n",
      "  inflating: dataset/1904.01608v1.txt  \n",
      "  inflating: dataset/1904.02020v2.txt  \n",
      "  inflating: dataset/1904.02399v4.txt  \n",
      "  inflating: dataset/1904.02514v3.txt  \n",
      "  inflating: dataset/1904.02682v1.txt  \n",
      "  inflating: dataset/1904.02792v1.txt  \n",
      "  inflating: dataset/1904.02839v1.txt  \n",
      "  inflating: dataset/1904.02954v1.txt  \n",
      "  inflating: dataset/1904.03279v2.txt  \n",
      "  inflating: dataset/1904.03288v3.txt  \n",
      "  inflating: dataset/1904.03396v2.txt  \n",
      "  inflating: dataset/1904.03651v2.txt  \n",
      "  inflating: dataset/1904.03746v6.txt  \n",
      "  inflating: dataset/1904.03889v1.txt  \n",
      "  inflating: dataset/1904.04365v4.txt  \n",
      "  inflating: dataset/1904.04447v1.txt  \n",
      "  inflating: dataset/1904.04514v1.txt  \n",
      "  inflating: dataset/1904.04547v1.txt  \n",
      "  inflating: dataset/1904.07418v1.txt  \n",
      "  inflating: dataset/1904.08067v4.txt  \n",
      "  inflating: dataset/1904.08128v1.txt  \n",
      "  inflating: dataset/1904.08378v1.txt  \n",
      "  inflating: dataset/1904.08398v3.txt  \n",
      "  inflating: dataset/1904.08779v2.txt  \n",
      "  inflating: dataset/1904.09223v1.txt  \n",
      "  inflating: dataset/1904.09675v1.txt  \n",
      "  inflating: dataset/1904.09816v1.txt  \n",
      "  inflating: dataset/1904.10273v1.txt  \n",
      "  inflating: dataset/1904.10322v1.txt  \n",
      "  inflating: dataset/1904.10367v1.txt  \n",
      "  inflating: dataset/1904.11475v1.txt  \n",
      "  inflating: dataset/1904.11544v2.txt  \n",
      "  inflating: dataset/1904.11829v3.txt  \n",
      "  inflating: dataset/1904.12575v1.txt  \n",
      "  inflating: dataset/1904.12683v2.txt  \n",
      "  inflating: dataset/1904.12796v3.txt  \n",
      "  inflating: dataset/1904.12848.txt  \n",
      "  inflating: dataset/1905.00453v1.txt  \n",
      "  inflating: dataset/1905.00616v2.txt  \n",
      "  inflating: dataset/1905.01263v1.txt  \n",
      "  inflating: dataset/1905.01338v1.txt  \n",
      "  inflating: dataset/1905.01395v1.txt  \n",
      "  inflating: dataset/1905.01976v1.txt  \n",
      "  inflating: dataset/1905.02244v4.txt  \n",
      "  inflating: dataset/1905.02423v3.txt  \n",
      "  inflating: dataset/1905.02450v5.txt  \n",
      "  inflating: dataset/1905.03072v3.txt  \n",
      "  inflating: dataset/1905.03704v1.txt  \n",
      "  inflating: dataset/1905.03752v1.txt  \n",
      "  inflating: dataset/1905.04363v2.txt  \n",
      "  inflating: dataset/1905.04413v3.txt  \n",
      "  inflating: dataset/1905.04564.txt  \n",
      "  inflating: dataset/1905.04847v1.txt  \n",
      "  inflating: dataset/1905.04877v1.txt  \n",
      "  inflating: dataset/1905.05583v2.txt  \n",
      "  inflating: dataset/1905.06316v1.txt  \n",
      "  inflating: dataset/1905.06482v1.txt  \n",
      "  inflating: dataset/1905.07129v3.txt  \n",
      "  inflating: dataset/1905.07504v2.txt  \n",
      "  inflating: dataset/1905.07799v2.txt  \n",
      "  inflating: dataset/1905.07854v2.txt  \n",
      "  inflating: dataset/1905.07870v4.txt  \n",
      "  inflating: dataset/1905.08108v1.txt  \n",
      "  inflating: dataset/1905.08836v1.txt  \n",
      "  inflating: dataset/1905.08880v1.txt  \n",
      "  inflating: dataset/1905.09205v2.txt  \n",
      "  inflating: dataset/1905.09217v1.txt  \n",
      "  inflating: dataset/1905.09248v3.txt  \n",
      "  inflating: dataset/1905.10072v1.txt  \n",
      "  inflating: dataset/1905.10536v1.txt  \n",
      "  inflating: dataset/1905.10630v1.txt  \n",
      "  inflating: dataset/1905.10752v1.txt  \n",
      "  inflating: dataset/1905.11096v2.txt  \n",
      "  inflating: dataset/1905.11596v1.txt  \n",
      "  inflating: dataset/1905.12616v1.txt  \n",
      "  inflating: dataset/1905.12897v2.txt  \n",
      "  inflating: dataset/1905.13127v1.txt  \n",
      "  inflating: dataset/1905.13129v1.txt  \n",
      "  inflating: dataset/1905.13132v1.txt  \n",
      "  inflating: dataset/1905.13656v3.txt  \n",
      "  inflating: dataset/1906.00091v1.txt  \n",
      "  inflating: dataset/1906.00346v1.txt  \n",
      "  inflating: dataset/1906.01213v3.txt  \n",
      "  inflating: dataset/1906.01684v2.txt  \n",
      "  inflating: dataset/1906.01910v1.txt  \n",
      "  inflating: dataset/1906.02192v1.txt  \n",
      "  inflating: dataset/1906.02242v1.txt  \n",
      "  inflating: dataset/1906.02365v1.txt  \n",
      "  inflating: dataset/1906.02780v1.txt  \n",
      "  inflating: dataset/1906.02829v1.txt  \n",
      "  inflating: dataset/1906.02900v1.txt  \n",
      "  inflating: dataset/1906.03134.txt  \n",
      "  inflating: dataset/1906.03221v1.txt  \n",
      "  inflating: dataset/1906.03672v1.txt  \n",
      "  inflating: dataset/1906.04165.txt  \n",
      "  inflating: dataset/1906.04281v1.txt  \n",
      "  inflating: dataset/1906.04341v1.txt  \n",
      "  inflating: dataset/1906.04501v1.txt  \n",
      "  inflating: dataset/1906.07155v1.txt  \n",
      "  inflating: dataset/1906.07241v2.txt  \n",
      "  inflating: dataset/1906.08101v1.txt  \n",
      "  inflating: dataset/1906.08237v1 (2).txt  \n",
      "  inflating: dataset/1906.08237v1 (3).txt  \n",
      "  inflating: dataset/1906.08237v1 (4).txt  \n",
      "  inflating: dataset/1906.08237v1 (5).txt  \n",
      "  inflating: dataset/1906.08237v1 (6).txt  \n",
      "  inflating: dataset/1906.08237v1.txt  \n",
      "  inflating: dataset/1906.08237v1_18-28-27.txt  \n",
      "  inflating: dataset/1906.08511v2.txt  \n",
      "  inflating: dataset/1906.08646.txt  \n",
      "  inflating: dataset/1906.08646v1.txt  \n",
      "  inflating: dataset/1906.08934v2.txt  \n",
      "  inflating: dataset/1906.09217v1.txt  \n",
      "  inflating: dataset/1906.09506v1.txt  \n",
      "  inflating: dataset/1906.09978v1.txt  \n",
      "  inflating: dataset/1906.12230v1.txt  \n",
      "  inflating: dataset/1906.12330v1.txt  \n",
      "  inflating: dataset/1907.01968v1.txt  \n",
      "  inflating: dataset/1907.03752v1.txt  \n",
      "  inflating: dataset/1907.03802.txt  \n",
      "  inflating: dataset/1907.03802v1.txt  \n",
      "  inflating: dataset/1907.05190v1.txt  \n",
      "  inflating: dataset/1907.05242v1.txt  \n",
      "  inflating: dataset/1907.05338v1.txt  \n",
      "  inflating: dataset/1907.05982v1.txt  \n",
      "  inflating: dataset/1907.06902v3.txt  \n",
      "  inflating: dataset/1907.07247v1.txt  \n",
      "  inflating: dataset/1907.08346v1.txt  \n",
      "  inflating: dataset/1907.08610v1.txt  \n",
      "  inflating: dataset/1907.08679v1.txt  \n",
      "  inflating: dataset/1907.10371v1.txt  \n",
      "  inflating: dataset/1907.10529v2.txt  \n",
      "  inflating: dataset/1907.11692v1.txt  \n",
      "  inflating: dataset/1907.11932v2.txt  \n",
      "  inflating: dataset/1907.11983v1.txt  \n",
      "  inflating: dataset/1907.12412v1.txt  \n",
      "  inflating: dataset/1907.12484v1.txt  \n",
      "  inflating: dataset/1907.12665v1.txt  \n",
      "  inflating: dataset/1908.00413v1.txt  \n",
      "  inflating: dataset/1908.01832v1.txt  \n",
      "  inflating: dataset/1908.01853v1.txt  \n",
      "  inflating: dataset/1908.03265v1.txt  \n",
      "  inflating: dataset/1908.03557v1.txt  \n",
      "  inflating: dataset/1908.04364v2.txt  \n",
      "  inflating: dataset/1908.05391v2.txt  \n",
      "  inflating: dataset/1908.05428v1.txt  \n",
      "  inflating: dataset/1908.06039v1.txt  \n",
      "  inflating: dataset/1908.06267v1.txt  \n",
      "  inflating: dataset/1908.07123v2.txt  \n",
      "  inflating: dataset/1908.07125v2.txt  \n",
      "  inflating: dataset/1908.07195v1.txt  \n",
      "  inflating: dataset/1908.07490v2.txt  \n",
      "  inflating: dataset/1908.07738v1.txt  \n",
      "  inflating: dataset/1908.08326v8.txt  \n",
      "  inflating: dataset/1908.08345v2.txt  \n",
      "  inflating: dataset/1908.08835v1.txt  \n",
      "  inflating: dataset/1908.09876v1.txt  \n",
      "  inflating: dataset/1908.09972v1.txt  \n",
      "  inflating: dataset/1908.10383v1.txt  \n",
      "  inflating: dataset/1908.10419v1.txt  \n",
      "  inflating: dataset/1908.11355v1.txt  \n",
      "  inflating: dataset/1909.00015v2.txt  \n",
      "  inflating: dataset/1909.00161v1.txt  \n",
      "  inflating: dataset/1909.00385v1.txt  \n",
      "  inflating: dataset/1909.00430v1.txt  \n",
      "  inflating: dataset/1909.00599v1.txt  \n",
      "  inflating: dataset/1909.01066v2.txt  \n",
      "  inflating: dataset/1909.01259.txt  \n",
      "  inflating: dataset/1909.01259v2.txt  \n",
      "  inflating: dataset/1909.01259_18-35-01.txt  \n",
      "  inflating: dataset/1909.01377v1.txt  \n",
      "  inflating: dataset/1909.02027v1.txt  \n",
      "  inflating: dataset/1909.02107v1.txt  \n",
      "  inflating: dataset/1909.02164v2.txt  \n",
      "  inflating: dataset/1909.02480v1.txt  \n",
      "  inflating: dataset/1909.03477v1.txt  \n",
      "  inflating: dataset/1909.03564v1.txt  \n",
      "  inflating: dataset/1909.04076v2.txt  \n",
      "  inflating: dataset/1909.04985v1.txt  \n",
      "  inflating: dataset/1909.06695v1.txt  \n",
      "  inflating: dataset/1909.08053v2.txt  \n",
      "  inflating: dataset/1909.08593v1.txt  \n",
      "  inflating: dataset/1909.08723v1.txt  \n",
      "  inflating: dataset/1909.08837v1.txt  \n",
      "  inflating: dataset/2016_interspeech_mmi.txt  \n",
      "  inflating: dataset/28ff6712d62fef4d4846fca5be5df06a8ffd41d2.txt  \n",
      "  inflating: dataset/5360-content-based-recommendations-with-poisson-factorization.txt  \n",
      "  inflating: dataset/6139-supervised-word-movers-distance.txt  \n",
      "  inflating: dataset/684662.full.txt  \n",
      "  inflating: dataset/7859-deep-neural-networks-with-box-convolutions.txt  \n",
      "  inflating: dataset/aa0bc6d2b33199a5dc3aba019adbc0440fbeecc6.txt  \n",
      "  inflating: dataset/Abushariah - 2012 - Automatic Continuous Speech Recognition based on Phonetically Rich and Balanced Arabic Speech Corpus-annotated.txt  \n",
      "  inflating: dataset/Abushariah et al. - 2010 - Natural speaker-independent Arabic speech recognition system based on Hidden Markov Models using Sphinx tools-annotated.txt  \n",
      "  inflating: dataset/Abuzeina, Al-Khatib, Elshafei - 2011 - Small-word pronunciation modeling for Arabic speech recognition A data-driven approach-annotated.txt  \n",
      "  inflating: dataset/agarwal18b.txt  \n",
      "  inflating: dataset/Ahmed, Ghabayen - 2017 - Arabic Automatic Speech Recognition Enhancement-annotated.txt  \n",
      "  inflating: dataset/Al-Abdullah et al. - 2019 - Artificial Neural Network for Arabic Speech Recognition in Humanoid Robotic Systems-annotated.txt  \n",
      "  inflating: dataset/Al-Anzi, Abuzeina - Unknown - Literature Survey of Arabic Speech Recognition-annotated.txt  \n",
      "  inflating: dataset/Al-Qatab, Ainon - 2010 - Arabic speech recognition using Hidden Markov Model Toolkit(HTK)-annotated.txt  \n",
      "  inflating: dataset/Ali et al. - 2008 - Generation of arabic phonetic dictionaries for speech recognition-annotated.txt  \n",
      "  inflating: dataset/Beier_Fusion_Moves_for_2015_CVPR_paper.txt  \n",
      "  inflating: dataset/Besacier et al. - 2014 - Automatic speech recognition for under-resourced languages A survey-annotated.txt  \n",
      "  inflating: dataset/blocksparsepaper.txt  \n",
      "  inflating: dataset/C14-1212.txt    \n",
      "  inflating: dataset/C16-1116.txt    \n",
      "  inflating: dataset/C16-1147.txt    \n",
      "  inflating: dataset/C16-1150.txt    \n",
      "  inflating: dataset/C16-2064.txt    \n",
      "  inflating: dataset/C18-1003.txt    \n",
      "  inflating: dataset/C18-1005.txt    \n",
      "  inflating: dataset/C18-1041.txt    \n",
      "  inflating: dataset/C18-1070.txt    \n",
      "  inflating: dataset/C18-1079.txt    \n",
      "  inflating: dataset/C18-1139.txt    \n",
      "  inflating: dataset/C18-1168.txt    \n",
      "  inflating: dataset/C18-1172.txt    \n",
      "  inflating: dataset/C18-1174.txt    \n",
      "  inflating: dataset/C18-1230.txt    \n",
      "  inflating: dataset/C18-1237.txt    \n",
      "  inflating: dataset/C18-1239.txt    \n",
      "  inflating: dataset/C18-1312.txt    \n",
      "  inflating: dataset/C18-2029.txt    \n",
      "  inflating: dataset/Camgoz_Neural_Sign_Language_CVPR_2018_paper.txt  \n",
      "  inflating: dataset/Categorization of Security Design Patterns.txt  \n",
      "  inflating: dataset/ctrl.txt        \n",
      "  inflating: dataset/D14-1162.txt    \n",
      "  inflating: dataset/D15-1063.txt    \n",
      "  inflating: dataset/D15-1243.txt    \n",
      "  inflating: dataset/D16-1030.txt    \n",
      "  inflating: dataset/D16-1123.txt    \n",
      "  inflating: dataset/D16-1169.txt    \n",
      "  inflating: dataset/D16-1171.txt    \n",
      "  inflating: dataset/D16-1191.txt    \n",
      "  inflating: dataset/D16-1193.txt    \n",
      "  inflating: dataset/D16-1250.txt    \n",
      "  inflating: dataset/D16-1257.txt    \n",
      "  inflating: dataset/D17-1024.txt    \n",
      "  inflating: dataset/D17-1047.txt    \n",
      "  inflating: dataset/D17-1296.txt    \n",
      "  inflating: dataset/D18-1032.txt    \n",
      "  inflating: dataset/D18-1033.txt    \n",
      "  inflating: dataset/D18-1084.txt    \n",
      "  inflating: dataset/D18-1094.txt    \n",
      "  inflating: dataset/D18-1129.txt    \n",
      "  inflating: dataset/D18-1181.txt    \n",
      "  inflating: dataset/D18-1237.txt    \n",
      "  inflating: dataset/D18-1274.txt    \n",
      "  inflating: dataset/D18-1280.txt    \n",
      "  inflating: dataset/D18-1390.txt    \n",
      "  inflating: dataset/D18-1424.txt    \n",
      "  inflating: dataset/D18-1495.txt    \n",
      "  inflating: dataset/D18-1532.txt    \n",
      "  inflating: dataset/D18-2010.txt    \n",
      "  inflating: dataset/D18-2024.txt    \n",
      "  inflating: dataset/E14-1056.txt    \n",
      "  inflating: dataset/E17-1011.txt    \n",
      "  inflating: dataset/E17-1067.txt    \n",
      "  inflating: dataset/E17-2082.txt    \n",
      "  inflating: dataset/E17-2092.txt    \n",
      "  inflating: dataset/e850be08cfaa00ece0d2dcba1c26fddd2107.txt  \n",
      "  inflating: dataset/Essa, Tolba, Elmougy - 2008 - A comparison of combined classifier architectures for arabic speech recognition-annotated.txt  \n",
      "  inflating: dataset/fulltext.txt    \n",
      "  inflating: dataset/Gayar et al. - 2018 - Arabic Speech Recognition Challenges and State of the Art-annotated.txt  \n",
      "  inflating: dataset/gcbc19-wsdm.txt  \n",
      "  inflating: dataset/Ghai, Singh - 2012 - Literature Review on Automatic Speech Recognition-annotated.txt  \n",
      "  inflating: dataset/Hmad - 2015 - Deep Neural Network Acoustic models for Multi-dialect Arabic Speech Recognition Arabic Speech Recognition-annotated.txt  \n",
      "  inflating: dataset/Hyassat, Abu Zitar - 2006 - Arabic speech recognition using SPHINX engine-annotated.txt  \n",
      "  inflating: dataset/IR-1132.txt     \n",
      "  inflating: dataset/J18-2005.txt    \n",
      "  inflating: dataset/K17-1009.txt    \n",
      "  inflating: dataset/K17-1010.txt    \n",
      "  inflating: dataset/K18-1018.txt    \n",
      "  inflating: dataset/K18-2004.txt    \n",
      "  inflating: dataset/Kirchhoff et al. - 2006 - Morphology-based language modeling for conversational Arabic speech recognition-annotated.txt  \n",
      "  inflating: dataset/kumar, Hussian, Ali - 2014 - Experimental Investigation on Turbulent Heat Transfer, Nusselt Number and Friction Factor Characteristics o-annotated.txt  \n",
      "  inflating: dataset/L16-1662.txt    \n",
      "  inflating: dataset/L18-1043.txt    \n",
      "  inflating: dataset/L18-1085.txt    \n",
      "  inflating: dataset/L18-1221.txt    \n",
      "  inflating: dataset/L18-1404.txt    \n",
      "  inflating: dataset/L18-1470.txt    \n",
      "  inflating: dataset/L18-1510.txt    \n",
      "  inflating: dataset/L18-1530.txt    \n",
      "  inflating: dataset/L18-1714.txt    \n",
      "  inflating: dataset/Labidi, Maraoui, Zrigui - Unknown - Unsupervised Method for Improving Arabic Speech Recognition Systems-annotated.txt  \n",
      "  inflating: dataset/Langley2011_Article_TheChangingScienceOfMachineLea.txt  \n",
      "  inflating: dataset/language-models.txt  \n",
      "  inflating: dataset/language_understanding_paper.txt  \n",
      "  inflating: dataset/Lee - Unknown - Lee_K_F_1990_1.Pdf-annotated.txt  \n",
      "  inflating: dataset/Li et al. - 2016 - Introduction-annotated.txt  \n",
      "  inflating: dataset/Likelihood of a Personal Computer to Be Infected with Malware.txt  \n",
      "  inflating: dataset/liu2014.txt     \n",
      "  inflating: dataset/muller18a.txt   \n",
      "  inflating: dataset/N13-1030.txt    \n",
      "  inflating: dataset/N13-1073.txt    \n",
      "  inflating: dataset/N15-3009.txt    \n",
      "  inflating: dataset/N16-1119.txt    \n",
      "  inflating: dataset/N16-1139.txt    \n",
      "  inflating: dataset/N18-1103.txt    \n",
      "  inflating: dataset/N18-1140.txt    \n",
      "  inflating: dataset/N18-2045.txt    \n",
      "  inflating: dataset/N19-1259.txt    \n",
      "  inflating: dataset/N19-1408.txt    \n",
      "  inflating: dataset/N19-2001.txt    \n",
      "  inflating: dataset/N19-3018.txt    \n",
      "  inflating: dataset/N19-4017.txt    \n",
      "  inflating: dataset/Narang, Divya Gupta - 2015 - International Journal of Computer Science and Mobile Computing Speech Feature Extraction Techniques A Revie-annotated.txt  \n",
      "  inflating: dataset/neural-network-language.txt  \n",
      "  inflating: dataset/ocz149.txt      \n",
      "  inflating: dataset/P13-1013.txt    \n",
      "  inflating: dataset/P13-1115.txt    \n",
      "  inflating: dataset/P14-1022.txt    \n",
      "  inflating: dataset/P14-1024.txt    \n",
      "  inflating: dataset/P14-1108.txt    \n",
      "  inflating: dataset/P15-1044.txt    \n",
      "  inflating: dataset/P16-2008.txt    \n",
      "  inflating: dataset/P18-1215.txt    \n",
      "  inflating: dataset/S19-1015.txt    \n",
      "  inflating: dataset/W17-4714.txt    \n",
      "  inflating: dataset/1-s2.0-S2211381912001087-main.txt  \n",
      "  inflating: dataset/1409.2944v2.txt  \n",
      "  inflating: dataset/1411.4952v3.txt  \n",
      "  inflating: dataset/1504.04317v1.txt  \n",
      "  inflating: dataset/1508.04999v3.txt  \n",
      "  inflating: dataset/1511.07916v1.txt  \n",
      "  inflating: dataset/1605.03795v3.txt  \n",
      "  inflating: dataset/1606.02891v2.txt  \n",
      "  inflating: dataset/1606.07792v1.txt  \n",
      "  inflating: dataset/1609.04621v1.txt  \n",
      "  inflating: dataset/1609.07033v1.txt  \n",
      "  inflating: dataset/1610.02424v2.txt  \n",
      "  inflating: dataset/1611.06639v1.txt  \n",
      "  inflating: dataset/1701.04783v1.txt  \n",
      "  inflating: dataset/1702.08139v2.txt  \n",
      "  inflating: dataset/1704.05091v1.txt  \n",
      "  inflating: dataset/1704.07657v3.txt  \n",
      "  inflating: dataset/1705.10498v2.txt  \n",
      "  inflating: dataset/1706.09254v2.txt  \n",
      "  inflating: dataset/1708.00781v1.txt  \n",
      "  inflating: dataset/1708.03436v1.txt  \n",
      "  inflating: dataset/1709.01121v2.txt  \n",
      "  inflating: dataset/1709.05027v7.txt  \n",
      "  inflating: dataset/1710.05370v1.txt  \n",
      "  inflating: dataset/1710.11041v2.txt  \n",
      "  inflating: dataset/1803.05170v3.txt  \n",
      "  inflating: dataset/1804.00344v3.txt  \n",
      "  inflating: dataset/1804.09057v1.txt  \n",
      "  inflating: dataset/1805.04803v1.txt  \n",
      "  inflating: dataset/1806.05662v3.txt  \n",
      "  inflating: dataset/1807.03096v3.txt  \n",
      "  inflating: dataset/1807.03756v2.txt  \n",
      "  inflating: dataset/1808.09381v2.txt  \n",
      "  inflating: dataset/1809.02847v2.txt  \n",
      "  inflating: dataset/1810.12936v1.txt  \n",
      "  inflating: dataset/1811.00347v2.txt  \n",
      "  inflating: dataset/1811.10996v1.txt  \n",
      "  inflating: dataset/1812.09355v1.txt  \n",
      "  inflating: dataset/1901.06168v1.txt  \n",
      "  inflating: dataset/1901.09321v2.txt  \n",
      "  inflating: dataset/1901.09821v1.txt  \n",
      "  inflating: dataset/1901.10125v4.txt  \n",
      "  inflating: dataset/1902.04094v2.txt  \n",
      "  inflating: dataset/1903.09588v1.txt  \n",
      "  inflating: dataset/1904.02338v2.txt  \n",
      "  inflating: dataset/1904.12058v1.txt  \n",
      "  inflating: dataset/1905.10070v2.txt  \n",
      "  inflating: dataset/1905.12777v1.txt  \n",
      "  inflating: dataset/1906.01161v2.txt  \n",
      "  inflating: dataset/1906.03820v1.txt  \n",
      "  inflating: dataset/1906.04386v1.txt  \n",
      "  inflating: dataset/1907.05572v1.txt  \n",
      "  inflating: dataset/1908.04319v1.txt  \n",
      "  inflating: dataset/1908.08788v1.txt  \n",
      "  inflating: dataset/1908.09701v1.txt  \n",
      "  inflating: dataset/1909.09436v1.txt  \n",
      "  inflating: dataset/7081-dropoutnet-addressing-cold-start-in-recommender-systems.txt  \n",
      "  inflating: dataset/D15-1299.txt    \n",
      "  inflating: dataset/L18-1241.txt    \n",
      "  inflating: dataset/Mubarak, Darwish - 2015 - Using Twitter to Collect a Multi-Dialectal Corpus of Arabic-annotated.txt  \n",
      "  inflating: dataset/N15-1142.txt    \n",
      "  inflating: dataset/P14-2089.txt    \n",
      "  inflating: dataset/P15-1119.txt    \n",
      "  inflating: dataset/P15-1162.txt    \n",
      "  inflating: dataset/P16-1085.txt    \n",
      "  inflating: dataset/P16-1110.txt    \n",
      "  inflating: dataset/P16-2036.txt    \n",
      "  inflating: dataset/P16-2080.txt    \n",
      "  inflating: dataset/P16-4016.txt    \n",
      "  inflating: dataset/P17-1052.txt    \n",
      "  inflating: dataset/P17-1081.txt    \n",
      "  inflating: dataset/P17-1121.txt    \n",
      "  inflating: dataset/P17-1158.txt    \n",
      "  inflating: dataset/P17-1187.txt    \n",
      "  inflating: dataset/P17-3007.txt    \n",
      "  inflating: dataset/P18-1014.txt    \n",
      "  inflating: dataset/P18-1032.txt    \n",
      "  inflating: dataset/P18-1079.txt    \n",
      "  inflating: dataset/P18-1103.txt    \n",
      "  inflating: dataset/P18-1114.txt    \n",
      "  inflating: dataset/P18-1161.txt    \n",
      "  inflating: dataset/P18-1171.txt    \n",
      "  inflating: dataset/P18-1226.txt    \n",
      "  inflating: dataset/P18-2112.txt    \n",
      "  inflating: dataset/P18-4005.txt    \n",
      "  inflating: dataset/P19-1103.txt    \n",
      "  inflating: dataset/P19-1189.txt    \n",
      "  inflating: dataset/P19-1227.txt    \n",
      "  inflating: dataset/P19-1501.txt    \n",
      "  inflating: dataset/P19-2057.txt    \n",
      "  inflating: dataset/Q16-1017.txt    \n",
      "  inflating: dataset/Q18-1028.txt    \n",
      "  inflating: dataset/R13-1074.txt    \n",
      "  inflating: dataset/Richard_Temporal_Action_Detection_CVPR_2016_paper.txt  \n",
      "  inflating: dataset/S14-2139.txt    \n",
      "  inflating: dataset/S17-2126.txt    \n",
      "  inflating: dataset/Saini, Kaur - 2013 - Automatic Speech Recognition A Review-annotated.txt  \n",
      "  inflating: dataset/sparse_transformers.txt  \n",
      "  inflating: dataset/suri2002.txt    \n",
      "  inflating: dataset/swj1738.txt     \n",
      "  inflating: dataset/U17-1006.txt    \n",
      "  inflating: dataset/Unknown - 2009 - Arabic statistical N-Gram models-annotated.txt  \n",
      "  inflating: dataset/Vergyri, Kirchhoff - Unknown - Automatic diacritization of Arabic for Acoustic Modeling in Speech Recognition-annotated.txt  \n",
      "  inflating: dataset/VimalaC - Unknown - A Review on Speech Recognition Challenges and Approaches-annotated.txt  \n",
      "  inflating: dataset/W15-2709v2.txt  \n",
      "  inflating: dataset/W16-1626.txt    \n",
      "  inflating: dataset/W16-3622.txt    \n",
      "  inflating: dataset/W16-4616.txt    \n",
      "  inflating: dataset/W17-1003.txt    \n",
      "  inflating: dataset/W17-1902.txt    \n",
      "  inflating: dataset/W17-5714.txt    \n",
      "  inflating: dataset/W18-0205.txt    \n",
      "  inflating: dataset/W18-1707.txt    \n",
      "  inflating: dataset/W18-2506.txt    \n",
      "  inflating: dataset/W18-3012.txt    \n",
      "  inflating: dataset/W18-6230.txt    \n",
      "  inflating: dataset/W18-6402.txt    \n",
      "  inflating: dataset/W18-6521.txt    \n",
      "  inflating: dataset/W18-6557.txt    \n",
      "  inflating: dataset/W19-2006.txt    \n",
      "  inflating: dataset/W19-3506.txt    \n",
      "  inflating: dataset/W19-4608.txt    \n",
      "  inflating: dataset/W19-4621.txt    \n",
      "  inflating: dataset/yin1993.txt     \n",
      "  inflating: dataset/Q16-1029.txt    \n",
      "  inflating: dataset/Wahyuni - 2018 - Arabic speech recognition using MFCC feature extraction and ANN classification-annotated.txt  \n"
     ]
    }
   ],
   "source": [
    "#unzip the dataset folder\n",
    "!unzip ./drive/MyDrive/dataset.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c979c",
   "metadata": {
    "id": "198c979c"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac58d753",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac58d753",
    "outputId": "4bb33356-7b9e-4275-c74a-f036df561a9c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/karmel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/karmel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/karmel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/karmel/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2022-05-09 17:02:53.047170: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-09 17:02:53.047207: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ec03206",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ec03206",
    "outputId": "7206b479-5978-4372-b5f4-063c48e03e52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1064"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the names of each folders\n",
    "files_names = os.listdir('./dataset')\n",
    "len(files_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0f3999",
   "metadata": {
    "id": "cc0f3999"
   },
   "source": [
    "# Convert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24dd0950",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24dd0950",
    "outputId": "777058cd-b335-4367-864a-a83d343da852"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data frame is Empty DataFrame\n",
      "Columns: [id, articles]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame([], columns = [\"id\", \"articles\"])\n",
    "print(f\"data frame is {df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "706513d6",
   "metadata": {
    "id": "706513d6"
   },
   "outputs": [],
   "source": [
    "def convert_to_df(df, files_names):\n",
    "        id = []\n",
    "        articles= []\n",
    "        # Define files place and read each sub file for cleaning\n",
    "        for index, file in enumerate(files_names):\n",
    "                # read each article files\n",
    "                path = os.getcwd()\n",
    "                file_path = '{}/dataset/{}'.format(path,file)\n",
    "                with open(file_path,'r', encoding='utf-16',  errors=\"ignore\") as f:\n",
    "                    texts = f.read() \n",
    "                    articles.append(texts) \n",
    "                    id.append(index)\n",
    "                    f.close()\n",
    "        print(\"the ids\", id)\n",
    "        print(\"the articles\", articles)\n",
    "        df['id'] = id\n",
    "        df[\"articles\"] = articles\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd7c238f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd7c238f",
    "outputId": "b10c00b9-9a40-430e-8495-c6684d5be17d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = convert_to_df(df, files_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "174e36e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "174e36e3",
    "outputId": "f423a30c-89ef-4b57-d8f6-a4ed35418313"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Statistical Machine Translation\\n\\nDraft of Ch...\n",
       "1       Found in Translation:\\n\\nLearning Robust Joint...\n",
       "2       Available online at www.sciencedirect.com\\n\\nS...\n",
       "3       J OURNAL OF I NFORMATION S CIENCE AND E NGINEE...\n",
       "4       The University of Sheeld\\n\\nT. E. Dunning\\n\\n...\n",
       "                              ...                        \n",
       "1059    Multi-label Hate Speech and Abusive Language D...\n",
       "1060    hULMonA (      ): The Universal Language Model...\n",
       "1061    Mazajak: An Online Arabic Sentiment Analyser\\n...\n",
       "1062    2017 2nd International Conferences on Informat...\n",
       "1063    INVESTIGATIVE RADIOLOGY Volume 28, Number 6, 4...\n",
       "Name: articles, Length: 1064, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"articles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b4ef795",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_df = pd.DataFrame(df['articles'].iloc[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d16683f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d16683f2",
    "outputId": "ca54dc4e-938a-4f42-c68f-a27e6f52b0aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     48052\n",
       "1      7759\n",
       "2      4223\n",
       "3      8574\n",
       "4     71427\n",
       "5      8025\n",
       "6      7159\n",
       "7      2685\n",
       "8     18542\n",
       "9     13682\n",
       "10     5449\n",
       "11     3177\n",
       "12     8716\n",
       "13     7259\n",
       "14     7244\n",
       "15     6694\n",
       "16     7691\n",
       "17     8333\n",
       "18     4001\n",
       "19     9149\n",
       "20    10087\n",
       "21     5835\n",
       "22     7457\n",
       "23     6153\n",
       "24     7287\n",
       "25     8170\n",
       "26     7348\n",
       "27     6668\n",
       "28     2837\n",
       "29     3721\n",
       "30     5282\n",
       "31     7388\n",
       "32     7437\n",
       "33     8702\n",
       "34     8665\n",
       "35     3796\n",
       "36     1819\n",
       "37    17959\n",
       "38    14282\n",
       "39     6846\n",
       "40     2847\n",
       "41     6549\n",
       "42     7226\n",
       "43     7515\n",
       "44     7194\n",
       "45     6416\n",
       "46     7524\n",
       "47     3659\n",
       "48     8019\n",
       "49     7734\n",
       "Name: word_count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of words for each doc\n",
    "limited_df['word_count'] = limited_df['articles'].apply(lambda x: len(str(x).split()))\n",
    "limited_df[\"word_count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66466052",
   "metadata": {
    "id": "66466052"
   },
   "source": [
    " # Text pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8352e695",
   "metadata": {
    "id": "8352e695"
   },
   "outputs": [],
   "source": [
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    # Remove leading and trailing spaces\n",
    "    text= text.strip()  \n",
    "    text= re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text= re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    # get ride of one letter word!\n",
    "    text = re.sub(r'\\b\\w\\b','',text) \n",
    "    \n",
    "      \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "HqeYuDXZNB-h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HqeYuDXZNB-h",
    "outputId": "18645fd2-10c3-455f-9ab7-c18de2692ae8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/karmel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42fe2349",
   "metadata": {
    "id": "42fe2349"
   },
   "outputs": [],
   "source": [
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe6a93b8",
   "metadata": {
    "id": "fe6a93b8"
   },
   "outputs": [],
   "source": [
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dccdf09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd034880",
   "metadata": {
    "id": "fd034880"
   },
   "outputs": [],
   "source": [
    "#Remove non English words\n",
    "dictionary = enchant.Dict(\"en_US\")\n",
    "   \n",
    "def remove_non_english(text):\n",
    "        en_words= []\n",
    "        for word in text:\n",
    "            if dictionary.check(word):\n",
    "                en_words.append(word)\n",
    "        text = ' '.join(str(e) for e in en_words)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f837434",
   "metadata": {
    "id": "2f837434"
   },
   "outputs": [],
   "source": [
    "limited_df['english_text'] =limited_df['articles'].apply(lambda text: remove_non_english(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf3b5414",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "bf3b5414",
    "outputId": "acc7d6a8-1596-4cbe-f333-958985071431"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Statistical Machine Translation Draft of Chapter Neural Machine Translation Center for Speech and Language Processing Department of Computer Science Johns Hopkins University 1st public draft August 2015 2nd public draft September 2017 2 Contents 13 Neural Machine Translation 5 13.1 A Short History . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 13.2 Introduction to Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 13.2.1 Linear Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 13.2.2 Multiple Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 13.2.3 Non-Linearity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 13.2.4 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 13.2.5 Back-Propagation Training . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 13.2.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 13.3 Computation Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 13.3.1 Neural Networks as Computation Graphs . . . . . . . . . . . . . . . . . . 24 13.3.2 Gradient Computations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 13.3.3 Deep Learning Frameworks . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 13.4 Neural Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 13.4.1 Feed-Forward Neural Language Models . . . . . . . . . . . . . . . . . . . 32 13.4.2 Word Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 13.4.3 Inference and Training . . . . . . . . . . . . . . . . . . . . . . . . . 37 13.4.4 Recurrent Neural Language Models . . . . . . . . . . . . . . . . . . . . . . 39 13.4.5 Long Short-Term Memory Models . . . . . . . . . . . . . . . . . . . . . . . 41 13.4.6 Gated Recurrent Units . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 13.4.7 Deep Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 13.5 Neural Translation Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 13.5.1 Encoder-Decoder Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 13.5.2 Adding an Alignment Model . . . . . . . . . . . . . . . . . . . . . . . . . . 48 13.5.3 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 13.5.4 Beam Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 3 4 CONTENTS 13.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 13.6.1 Ensemble Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 13.6.2 Large Vocabularies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 13.6.3 Using Monolingual Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 13.6.4 Deep Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 13.6.5 Guided Alignment Training . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 13.6.6 Modeling Coverage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 13.6.7 Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 13.6.8 Adding Linguistic Annotation . . . . . . . . . . . . . . . . . . . . . . . . . 77 13.6.9 Multiple Language Pairs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 13.7 Alternate Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 13.7.1 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . 83 13.7.2 Neural Networks With Attention . . . . . . . . . . . . . . . 85 13.7.3 Self-Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 13.8 Current Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 13.8.1 Domain Mismatch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 13.8.2 Amount of Training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 13.8.3 Noisy Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 13.8.4 Word Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 13.8.5 Beam Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 13.8.6 Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 13.9 Additional Topics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117 Chapter 13 Neural Machine Translation A major recent development in statistical machine translation is the adoption of neural net- works. Neural network models promise better sharing of statistical evidence between similar words and inclusion of rich context. This chapter introduces several neural network modeling techniques and explains how they are applied to problems in machine translation 13.1 A Short History Already during the last wave of neural network research in the and machine trans- was in the sight of researchers exploring these methods In the models proposed by and and are striking similar to the current dominant neural machine translation approaches. none of these models were trained on data sizes large enough to produce reasonable results for anything but toy ex- The computational complexity involved by far exceeded the computational resources of that and hence the idea was abandoned for almost two decades. During this hibernation data-driven approaches such as phrase-based statistical ma- chine translation rose from obscurity to dominance and made machine translation a useful tool for many from information to increasing the productivity of professional translators. The modern resurrection of neural methods in machine translation started with the of neural language models into traditional statistical machine translation systems. The work by showed large improvements in public evaluation campaigns. these ideas were only slowly mainly due to computational concerns. The use of for training also posed a challenge for many research groups that simply lacked such hardware or the experience to exploit it. Moving beyond the use in language neural network methods crept into other com- of traditional statistical machine such as providing additional scores or ex- tending translation tables Lu reordering Li 5 6 CHAPTER 13. NEURAL MACHINE TRANSLATION and models and so on. For the joint translation and language model by was since it showed large quality improvements on top of a very competitive statistical machine translation system. More ambitious efforts aimed at pure neural machine abandoning existing approaches completely. Early steps were the use of models and and sequence-to-sequence models These were able to produce reasonable translations for short but fell apart with in- creasing sentence length. The addition of the attention mechanism yielded competitive results Jean With a few more such as byte pair encoding and back-translation of target-side monolingual neural machine translation became the new state of the art. Within a year or the entire research of machine translation went neural. To give some indication of the speed of At the shared task for machine translation organized by the Conference on Machine Translation only one pure neural machine translation system was submitted in 2015. It was but outperformed by traditional statistical systems. A year in a neural machine translation system won in almost all language pairs. In almost all submissions were neural machine translation systems. At the time of neural machine translation research is progressing at rapid pace. There are many directions that are and will be explored in the coming ranging from core machine learning improvements such as deeper models to more linguistically informed models. More insight into the strength and weaknesses of neural machine translation is being gathered and will inform future work. There is an extensive proliferation of available for and of neural machine translation systems. At the time of the number of is rather than consolidating. it is quite hard and premature to make recommendations. some of the promising on Marian re-implementation of on on Sockeye on on 13.2 Introduction to Neural Networks A neural network is a machine learning technique that takes a number of inputs and predicts outputs. In many they are not very different from other machine learning methods but have distinct strengths. 13.2. INTRODUCTION TO NEURAL NETWORKS 7 Figure Graphical illustration of a linear model as a feature values are input arrows are and the score is an output node. 13.2.1 Linear Models Linear models are a core element of statistical machine translation. A potential translation x of a sentence is represented by a set of features h i x . Each feature is weighted by a parameter i to obtain an overall score. Ignoring the exponential function that we used previously to turn the linear model into a log-linear the following formula sums up the model. score x j h j x j a linear model can be illustrated by a where feature values are input arrows are and the score is an output node Figure Most we use linear models to combine different components of a machine translation such as the language the phrase translation the reordering and properties such as the length of the or the accumulated jump distance between phrase translations. Training methods assign a weight value i to each such feature h i x related to their importance in contributing to scoring better translations higher. In machine this is called tuning . linear models do not allow us to more complex relationships between the features. Let us say that we that for short sentences the language model is less important than the translation or that average phrase translation probabilities higher than 0.1 are similarly reasonable but any value below that is really terrible. The hypothetical example implies dependence between features and the second example implies non-linear relationship between the feature value and its impact on the score. Linear models cannot handle these cases. A commonly cited counter-example to the use of linear models is the operator with the truth table 0 0 0 1 0 1 0 1 1 and 1 1 0 . For a linear model with two features the it is not possible to come up with weights that give the correct output in all cases. Linear models assume that all represented as points in the feature are linearly separable. This is not the case with and may not be the case for type of features we use in machine translation. 8 CHAPTER 13. NEURAL MACHINE TRANSLATION Figure A neural network with a hidden layer. 13.2.2 Multiple Layers Neural networks modify linear models in two important ways. The is the use of multiple layers. Instead of computing the output value directly from the input a hidden layer is introduced. It is called because we can observe inputs and outputs in training in- but not the mechanism that connects them this use of the concept hidden is similar to its meaning in hidden Markov models. See Figure 13.2 for on illustration. The network is processed in two steps. a linear combination of weighted input node is computed to produce each hidden node value. Then a linear combination of weighted hidden nodes is computed to produce each output node value. At this let us introduce mathematical notations from the neural network literature. A neural network with a hidden layer consists of a vector of input nodes with values x x 1 x 2 x 3 n T a vector of hidden nodes with values h h 1 h 2 h 3 m T a vector of output nodes with values y y 1 y 2 y 3 l T a matrix of weights connecting input nodes with hidden nodes W w a matrix of weights connecting hidden nodes with output nodes U u The computations in a neural network with a hidden as sketched out so are h j x i w i y k h j u j Note that we in the possibility of multiple output nodes y k although our so far only showed one. 13.2. INTRODUCTION TO NEURAL NETWORKS 9 Hyperbolic tangent Logistic function linear unit cosh x x e x e x x 1 output ranges output ranges output ranges from to from 0 to from 0 to Figure Typical activation functions in neural networks. 13.2.3 Non-Linearity If we carefully think about the addition of a hidden we realize that we have not gained anything so far to model relationships. We can easily do away with the hidden layer by multiplying out the weights y k h j u j j i x i w u x i u w i j a salient element of neural networks is the use of a non-linear activation function . After computing the linear combination of weighted feature values s j i x i w we obtain the value of a node only after applying such a function h j f s j . Popular choices are the hyperbolic tangent and the logistic function See Figure 13.3 for more details on these functions. A good way to think about these activation functions is that they segment the range of values for the linear combination s j into a segment where the node is turned off close to 0 for or for a transition segment where the node is partly turned on a segment where the node is turned on close to A different popular choice is the activation function for the linear unit It does not allow for negative values and them at but does not alter the value of positive values. It is simpler and faster to compute than x or x You could view each hidden node as a feature detector. For a certain of input node it is turned for others it is turned off. Advocates of neural networks claim that x x e e x e x x x 10 CHAPTER 13. NEURAL MACHINE TRANSLATION x 0 h 0 3 x 1 4 3 2 h 1 5 -5 y 0 x 2 1 -4 h 2 1 -2 Figure A simple neural network with bias nodes in input and hidden layers. the use of hidden nodes obviates at least drastically the need for feature engineer- Instead of manually detecting useful patterns in input training of the hidden nodes discovers them automatically. We do not have to stop at a single hidden layer. The currently fashionable name deep learning for neural networks stems from the fact that often better performance can be achieved by deeply stacking together layers and layers of hidden nodes. 13.2.4 Inference Let us walk through neural network i how output values are computed from input with a concrete example. Consider the neural network in Figure 13.4. This network has one additional innovation that we have not presented so bias units. These are nodes that always have the value 1. Such bias units give the network something to work with in the case that all input values are 0. the weighted sum s j would be 0 no matter the weights. Let us use this neural network to process some say the value 1 for the input node x 0 and 0 for the second input node x 1 . The value of the bias input node x 2 is to 1. To compute the value of the hidden node h 0 we have to carry out the following calculation. h 0 x i w 1 i i 3 0 4 1 0 . 73 The calculations for the other nodes are summarized in Table 13.1. The output value in node y 0 for the input is 0.743. If we expect binary we would understand this result as the value since it is over the threshold of 0.5 in the range of possible output values the output for all possible binary 13.2. INTRODUCTION TO NEURAL NETWORKS 11 Layer Node Summation Activation hidden h 0 1 3 0 4 1 2 1 0.731 hidden h 1 1 2 0 3 1 4 2 0.119 output y 0 0 . 731 5 0 . 119 5 1 2 1 . 060 0.743 Table Calculations for input to the network in Figure 13.4. Input x 0 Input x 1 Hidden h 0 Hidden h 1 Output y 0 0 0 0.119 0.018 0.183 0 0 1 0.881 0.269 0.743 1 1 0 0.731 0.119 0.743 1 1 1 0.993 0.731 0.334 0 Our neural network computes XOR . How does it do If we look at the hidden nodes h 0 and h 1 we notice that h 0 acts like the Boolean OR Its value is high if at least of the two input values is 1 h 0 for the three it otherwise has a low value The other hidden node h 1 acts like the Boolean AND it only has a high value if both inputs are 1. XOR is effectively implemented as the subtraction of the AND from the OR hidden node. Note that the non-linearity is key here. Since the value for the OR node h 0 is not that much higher for the input of opposed to a single 1 in the input vs. 0.881 and the distinct high value for the AND node h 1 in this case manages to push the output y 0 below the threshold. This would not be possible if the values of the inputs would be simply summed up as in linear models. As mentioned recently the use of the name deep learning for neural networks has become fashionable. It emphasizes that often higher performance can be achieved by using networks with multiple hidden layers. Our XOR example hints at where this power comes from. With a single input-output layer network it is possible to mimic basic Boolean operations such as AND and OR since they can be modeled with linear XOR can be expressed as x AND y x OR y and our neural network example implements the Boolean operations AND and OR in the and the subtraction in the second layer. For functions that require more intricate more operations may be chained and hence a neural network architecture with more hidden layers may be needed. It may be possible train- to build neural networks for any computer if the number of hidden layers matches the depth of the computation. There is a line of research under the banner neural Turing machines that explores what kind of architectures are needed to implement basic For a neural network with two hidden layers is to implement an algorithm that sorts n -bit numbers. 13.2.5 Back-Propagation Training Training neural networks requires the optimization of weight values so that the network the correct output for a set of training examples. We repeatedly feed the input from the 12 CHAPTER 13. NEURAL MACHINE TRANSLATION Optimum Gradient for w 1 Comb Current Point Figure Gradient descent We compute the gradient with regard to every dimension. In this case the gradient with respect to weight w 2 smaller than the gradient with respect to the weight w 1 so we move more to the left than down arrows point in negative gradient pointing to the training examples into the compare the computed output of the network with the correct output from the training and update the weights. several passes over the training data are carried out. Each pass over the data is called an epoch . The most common training method for neural networks is called back-propagation since it updates the weights to the output and propagates back error information to earlier layers. Whenever a training example is then for each node in the an error term is computed which is the basis for updating the values for incoming weights. The formulas used to compute updated values for weights follows principles of gradient descent training. The error for a node is understood as a function of the incoming weights. To reduce the error given this we compute the gradient of the error function with respect to each of the and move against the gradient to reduce the error. Why is moving alongside the gradient a good Consider that we optimize multiple dimensions at the same time. If you are looking for the lowest point in an area you are looking for water in a and the ground falls off steep to the west of and also slightly south of then you would go in a direction that is mainly west and only slightly south. In other you go alongside the gradient. See Figure 13.5 for an illustration. In the following two we will derive the formulae for updating weights for our example network. If you are less interested in the why and more in the how you can skip these sections and continue reading when we summarize the update formulae on page 16. Weights to the output nodes Let us review and extend our notation. At an output node y i we compute a linear combination of weight and hidden node values. s i w i j h j j 13.2. INTRODUCTION TO NEURAL NETWORKS 13 This sum s i is passed through an activation function such as to compute the output value y . y i s i We compare the computed output values y i against the target output values t i from the training example. There are various ways to compute an error value E from these values. Let us use the norm. 1 E t i y i 2 2 i As we stated our goal is to compute the gradient of the error E with respect to the weights w k to out in which direction how we should move the weight value. We do this for each weight w k separately. We break up the computation of the gradient into three essentially unfolding the Equations 13.6 to 13.8. i i Let us now work through each of these three steps. Since we the error E in terms of the output values y i we can compute the component as follows. d 1 2 t i y i t i y i i i 2 The derivative of the output value y i with respect to s i linear combination of weight and hidden node depends on the activation function. In the case of we i d s i s i s i y i y i i i To keep our treatment below as general as possible and not commit to the as an activation we will use the shorthand y i for i below. Note that for any given training example and any given differentiable activation this value can always be computed. we compute the derivative of s i with respect to the weight w i j which turns out to be quite simply the value to the hidden node h j . d i j i j j w i j h j h j Where are In Equations 13.10 to we computed the three steps needed to compute the gradient for the error function given the unfolded laid out in Equation 13.9. Putting it all we have i i j i i i j t i y i y i h j i j i i i j 14 CHAPTER 13. NEURAL MACHINE TRANSLATION Factoring in a learning rate gives us the following update formula for weight w i j . Note that we also remove the minus since we move against the gradient towards the minimum. w i j t i y i y i h j It is useful to introduce the concept of an error term i . Note that this term is associated with a while the weight updates concern weights. The error term has to be computed only once for the and it can be then used for each of the incoming weights. i t i y i y i This reduces the update formula w i j i h j Weights to the hidden nodes The computation of the gradient and hence the update formula for hidden nodes is quite anal- As we the linear combination z j s i of input values x k hidden values h j weighted by weights u j k weights w i j z j u j k x k k This leads to the computation of the value of the hidden node h j . h j z j Following the principles of gradient we need to compute the derivative of the error E with respect to the weights u j k . We decompose this derivative as before. j dz j the error is in terms of output values y i not values for hidden nodes h j . The idea behind back-propagation is to track how the error caused by the hidden node contributed to the error in the next layer. Applying the chain rule gives j i i i i i j j k j dz j j k the computation of j is more complex than in the case of output since 13.2. INTRODUCTION TO NEURAL NETWORKS 15 We already encountered the two terms i and previously. To i i d i i i i 1 2 t i y i y 2 i d 1 t i y i 2 y i 2 i t i y i y i i The third term in Equation 13.19 is computed straightforward. i d j j i w i j h j w i j Putting Equation 13.20 and Equation 13.21 Equation 13.19 can be solved j i i w i j This gives rise to a quite intuitive interpretation. The error that matters at the hidden node h j depends on the error terms i in the subsequent nodes y i weighted by w i j the impact the hidden node h j has on the output node y i . Let us tie up the remaining loose ends. The missing pieces from Equation 13.18 are the second term j d z j z j z j h j h j h j dz j dz j and third term dz j d j k j k k u j k x k x k Putting Equation Equation and Equation 13.24 together gives us the gradient j dz j j k j dz j j k i w i j h j x k i If we an error term j for hidden nodes analogous to output nodes j i w i j h j i then we have an analogous update formula u j k j x k i 16 CHAPTER 13. NEURAL MACHINE TRANSLATION Summary We train neural networks by processing training one at a and update weights each time. What drives weight updates is the gradient towards a smaller error. Weight updates are computed based on error terms i associated with each non-input node in the network. For output the error term i is computed from the actual output y i of the node for our current and the target output t i for the node. i t i y i y i For hidden the error term j is computed via back-propagating the error term i from subsequent nodes connected by weights w i j . j i w i j h j i Computing y i and h j requires the derivative of the activation to which the weighted sum of incoming values is passed. Given the error weights w i j u j k from each proceeding node h j x k are tempered by a learning rate . w i j i h j u j k j x k Once weights are the next training example is processed. There are typically passes over the training called epochs. Example Given the neural network in Figure let us see how the training example 1 is pro- Let us start with the calculation of the error term for the output node y 0 . During inference Table 13.1 on page we computed the linear combination of weighted hidden node values s 0 1 . 060 and the node value y 0 0 . 743 . The target value is t 0 1 . t 0 y 0 y 0 0 . . 0 . 257 0 . 191 0 . 049 With this we can compute weight such as for weight w 0 0 . w 0 0 0 h 0 0 . 049 0 . 731 0 . 036 Since the hidden node h 0 leads only to one output node y 0 the calculation of its error term 0 is not more computationally complex. j i u i 0 h 0 w 0 0 z 0 0 . 049 5 0 . 197 0 . 049 i Table 13.2 summarizes the updates for all weights. 13.2. INTRODUCTION TO NEURAL NETWORKS 17 x 0 1 3 h 0 x 1 0 4 3 2 h 1 5 -5 y 0 x 2 1 -4 h 2 1 -2 Node Error term Weight updates t 0 y 0 s 0 w 0 j h j y 0 0 . 0 . 191 0 . 049 w 0 0 0 . 049 0 . 731 0 . 036 w 0 1 0 . 049 0 . 119 0 . 006 w 0 2 0 . 049 1 0 . 049 j w i j z j u j i j x i h 0 0 0 . 049 5 0 . 197 0 . 048 u 0 0 0 . 048 1 0 . 048 u 0 1 0 . 048 0 0 u 0 2 0 . 048 1 0 . 048 h 1 1 0 . 049 5 0 . 105 0 . 026 u 1 0 0 . 026 1 0 . 026 u 1 1 0 . 026 0 0 u 1 2 0 . 026 1 0 . 026 Table Weight updates learning rate for the neural network in Figure 13.4 above the when the training example 1 is presented. local optimum global optimum Too high learning rate Bad initialization Local optimum Figure Problems with gradient descent training that motivate some of the detailed in Section a too high learning rate may lead to too drastic parameter overshooting the bad initialization may require many updates to escape a and the existence of local optima which trap training. 18 CHAPTER 13. NEURAL MACHINE TRANSLATION error validation minimum validation training training progress Figure Training progress over time. The error on the training set continuously decreases. on a validation set used for at some point the error increases. Training is stopped at the validation minimum before such sets in. 13.2.6 We conclude our introduction to neural networks with some basic and To motivate some of the consider Figure 13.6. While gradient descent training is a it may run into practical problems. Setting the learning rate too high leads to updates that overshoot the optimum. Con- a too low learning rate leads to slow convergence. Bad initialization of weights may lead to long paths of many update steps to reach the optimum. This is especially a problem with activation functions like which only have a short interval of change. The existence of local optima lead the search to get trapped and miss the global optimum. Validation Set Neural network training proceeds for several full iterations over the training data. When to When we track training we see that the error on the training set continuously decreases. at some point sets where the training data is memorized and not generalized. We can check this with an additional set of called the validation set that is not used during training. See Figure 13.7 for an illustration. When we measure the error on the validation set at each point of we see that at some point this error increases. we stop when the minimum on the validation set is reached. 13.2. INTRODUCTION TO NEURAL NETWORKS 19 Weight Initialization Before training weights are initialized to random values. The values are from a uniform distribution. We prefer initial weights that lead to node values that are in the transition area for the activation and not in the low or high shallow slope where it would take a long time to push towards a change. For for the activation feeding values in the range to the activation function leads to activation values in the range of For the activation commonly used formula for weights to the layer of a network are 1 1 where n is the size of the previous layer. For hidden we chose weights from the range 6 6 where n j is the size of the previous n j size of next layer. Momentum Term Consider the case where a weight value is far from its optimum. Even if most training exam- push the weight value in the same it may still take a while for each of these small updates to accumulate until the weight reaches its optimum. A common trick is to use a momentum term to speed up training. This momentum term m t gets updated at each time step t for each training We combine the previous value of the momentum term m t 1 with the current raw weight update value w t and use the resulting momentum term value to update the weights. For with a decay rate of the update formula changes to m t 0 . 9 m t 1 w t w t w t 1 m t Adapting Learning Rate per Parameter A common training strategy is to reduce the learning rate over time. At the beginning the parameters are far away from optimal values and have to change a but in later training stages we are concerned with and a large learning rate may cause a parameter to bounce around an optimum. But different parameters may be at different stages on the path to their optimal so a different learning rate for each parameter may be helpful. One such called records the gradients that were computed for each parameter and accumulates their square values over and uses this sum to adjust the learning rate. n n n j n j n j n j 20 CHAPTER 13. NEURAL MACHINE TRANSLATION The update formula is based on the sum of gradients of the error E with respect to the weight w at all time steps t g t t . We divide the learning rate for this weight this accumulated sum. w t g t t 2 big changes in the parameter value to big gradients g t lead to a reduction of the learning rate of the weight parameter. Combining the idea of momentum term and adjusting parameter update by their change is the inspiration of Adam another method to transform the raw gradient into a parameter update. there is the idea of which is computed as in Equation 13.36 above. m t 1 m t 1 1 g t there is the idea of the squares of gradients in for adjusting the learning rate. Since raw accumulation does run the risk of becoming too large and hence permanently depressing the learning Adam uses exponential just like for the momentum term. v t 2 v t 1 2 g t 2 The hyper parameters 1 and 2 are set typically close to but this also means that early in training the values for m t and v t are close to their initialization values of 0. To adjust for they are corrected for this bias. m t m t 1 1 t v t v t 1 2 t With increasing training time steps t this correction goes t 1 1 t 1 . Having these pieces in hand rate momentum m t accumulated change v t weight update per Adam is computed as w t m t Common values for the hyper parameters are 1 0 . 9 2 0 . 999 and 10 8 . There are various other adaptation schemes. This is an active area of research. For the second order gradient gives some useful information about the rate of change. it is often expensive to so other shortcuts are taken. g v t 13.2. INTRODUCTION TO NEURAL NETWORKS 21 Dropout The parameter space in which back-propagation learning and its variants are operating is lit- with local optima. The hill-climbing algorithm may just climb a mole hill and be stuck instead of moving towards a climb of the highest mountain. Various methods have been proposed to get training out of these local optima. One currently popular method in neural machine translation is called drop-out . It sounds a bit simplistic and wacky. During some of the nodes of the neural network are ignored. Their values are set to and their associated parameters are not updated. These dropped-out nodes are chosen at and may account for as much as or even more of all the nodes. Training resumes for some number of iterations without the and then a different set of drop-out nodes are selected. The dropped-out nodes played some useful role in the model trained up to the point where they are ignored. After other nodes have to pick up the slack. The end result is a more robust model where several nodes share similar roles. Layer Normalization Layer normalization addresses a problem that arises especially in the deep neural networks that we are using in neural machine where computing proceeds through a large sequence of layers. For some training average values at one layer may become very which feed into the following also producing large output and so on. This is especially a problem with activation functions that do not limit the output to a narrow such as linear units. For other training examples the average values at the same layers may be very small. This causes a problem for training. Recall from Equation that gradient updates are strongly effected by node values. Too large node values lead to exploding gradients and too small node values lead to diminishing gradients. To remedy the idea is to normalize the values on a per-layer basis. This is done by adding additional computational steps to the neural network. Recall that a feed-forward layer consists of the the matrix multiplication of the weight matrix W with the node values from the previous layer h l 1 resulting in a weighted sum s l followed by an activation function such as s l W h l 1 h l h l We can compute the mean and variance of the values in the weighted sum vector s l by H l 1 s H i 1 H l 1 H i 1 s l 2 22 CHAPTER 13. NEURAL MACHINE TRANSLATION Using these we normalize the vector s l using two additional bias vectors g and b g where is element-wise multiplication and the difference subtracts the scalar average from each vector element. The formula normalizes the values in s l by shifting them against their average hence ensuring that their average afterwards is 0. The resulting vector is then divided by the variance l . The additional bias vectors give some they may be shared across multiple layers of the same such as multiple time steps in a recurrent neural network will introduce these in Section 13.4.4 on page Mini Batches Each training example yields a set of weight updates w i . We may process all the training examples and only afterwards apply all the updates. But neural networks have the advantage that they can immediately learn from each training example. A training method that updates the model with each training example is called online learning . The online learning variant of gradient descent training is called stochastic gradient descent . Online learning generally takes fewer passes over the training set epochs for con- since training constantly changes the it is hard to we may want to process the training data in accumulate the weight up- and then apply them collectively. These smaller sets of training examples are called mini batches to distinguish this approach from batch training where the entire training set is considered one batch. There are other variations to organize the processing of the training typically motivated by restrictions of parallel processing. If we process the training data in mini then we can the computation of weight update values w but have to synchronize their summation and application to the weights. If we want to distribute training over a number of it is computationally more convenient to break up the training data in equally sized perform online learning for each of the parts using smaller mini and then average the weights. breaking up training this often leads to better results than straightforward linear processing. a scheme called runs several training threads that immediately update even though other threads still use the weight values to compute gradients. While this is clearly violates the safe guards typically taken in parallel it does not hurt in practical experience. Vector and Matrix Operations We can express the calculations needed for handling neural networks as vector and matrix operations. s l l s l l b 13.3. COMPUTATION GRAPHS 23 Forward s W h Activation y h Error t y s Propagation of error i W i s Weight W h T Executing these operations is computationally expensive. If our layers 200 then the matrix operation W h requires 200 200 40 000 multiplications. Such matrix are also common in another highly used area of computer graphics processing. When rendering images on the the geometric properties of 3-dimensional objects have to be processed to generate the color values of the 2-dimensional image on the screen. Since there is high demand for fast graphics for instance for the use in realistic looking computer specialized hardware has become graphics processing units . These processors have a massive number of cores the NVIDIA GPU provides 3584 thread but a rather lightweight instruction set. provide instructions that are applied to many data points at which is exactly what is needed out the vector space computations listed above. Programming for is supported by various such as for and has become an essential part of developing large scale neural network applications. The general term for and matrices is tensors . A tensor may also have more a sequence of matrices can be packed into a 3-dimensional tensor. Such large objects are actually frequently used in todays neural network Further Readings A good introduction to modern neural network research is the textbook There is also book on neural network methods applied to the natural language processing in general A number of key techniques that have been recently developed have entered the standard of neural machine translation research. Training is made more robust by methods such as drop-out where during training intervals a number of nodes are randomly masked. To avoid exploding or vanishing gradients during back-propagation over several gradients are clipped Layer normalization Ba has similar by ensuring that node values are within reasonable bounds. An active topic of research are optimization methods that adjust the learning rate of gradient descent training. Popular methods are and currently Adam and 13.3 Computation Graphs For our example neural network from Section we painstakingly worked out for gradient computations needed by gradient descent training. After all this hard it may 24 CHAPTER 13. NEURAL MACHINE TRANSLATION 1 . 0 0 . 0 x W 1 3 4 2 3 3 2 prod b 1 2 4 1 2 . 731 . 119 sum W 2 5 5 3 . 06 prod b 2 2 1 . 06 . 743 sum Figure Two layer feed-forward neural network as a computation consisting of the input value x weight parameters W 1 W 2 b 1 b 2 and computation nodes To the right of each parameter its value is shown. To the left of input and computation we show how the input T is processed by the graph. come as surprise that you will likely never have to do this again. It can be done even for arbitrarily complex neural network architectures. There are a number of that allow you to the network and it will take care of the rest. In this we will take a close look at how this works. 13.3.1 Neural Networks as Computation Graphs we will take a different look at the networks we are building. We previously represented neural networks as graphs consisting of nodes and their connections Figure 13.4 on page or by mathematical equations such as h W 1 x b 1 y W 2 h b 2 The equations above describe the feed-forward neural network that we use as our running example. We now represent this math in form of a computation graph . See Figure 13.8 for an illustration of the computation graph for our network. The graph contains as nodes the parameters of the models weight matrices W 1 W 2 and bias vectors b 1 b 2 the input x and the mathematical operations that are carried out between them and Next to each we show their values. 13.3. COMPUTATION GRAPHS 25 Neural viewed as computation are any arbitrary connected operations between an input and any number of parameters. Some of these operations may have little to do with any inspiration from neurons in the so we are stretching the term neural networks quite a bit here. The graph does not have to have a nice tree structure as in our but may be any directed graph anything goes as long there is a straightforward processing direction and no cycles. Another way to view such a graph is as a fancy way to visualize a sequence of function calls that take as arguments the previously computed or any combination but have no recursion or loops. Processing an input with the neural network requires placing the input value into the node x and carrying out the computations. In the we show this with the input vector T . The resulting numbers should look familiar since they are the same as when previously worked through this example in Section 13.2.4. Before we move let us take stock of what each computation node in the graph has to accomplish. It consists of the a function that executes its computation operation links to input nodes when processing an the computed value We will add two more items to each node in the following section. 13.3.2 Gradient Computations So we showed how the computation graph can be used process an input value. Now we will examine how it can be used to vastly simply model training. Model training requires an error function and the computation of gradients to derive update rules for parameters. The of these is quite straightforward. To compute the we need to add another computation at the end of the computation graph. This computation takes the computed out- put value y and the given correct output value t from the training data and produces an error value. A typical error function is the norm 12 t y 2 . From the view of the result of the execution of the computation graph is an error value. for the more part update rules for the parameters. Look- at the computation model updates originate from the error values and propagate back to the model parameter. we call the computations needed to compute the up- date values also the backward pass through the opposed to the forward pass that computed output and error. Calculus Refresher In the chain rule is a formula for computing the derivative of the of two or more functions. That if f and g are then the chain rule ex- presses the derivative of their composition f g function which maps x to f g x in terms of the derivatives of f and g and the product of functions as f g f g g . This can be written more explicitly in terms of the variable. Let F f g or 26 CHAPTER 13. NEURAL MACHINE TRANSLATION Consider the chain of operations that con- the weight matrix W 2 to the error com- e t y s s sum b 2 p prod W 2 where h are the values of the hidden layer resulting from earlier computations. To compute the update rule for the pa- matrix W 2 we view the error as a function of these parameters and take the derivative with respect to in our case W 2 we broke it up into steps using the chain rule. We now do the same here. d W 2 d sum prod W 2 b 2 t d t d s d sum b 2 d prod W 2 2 2 2 Note that for the purpose for computing an update rule for W 2 we treat all the other in this computation target value t the bias vector b 2 the hidden node values h as constants. This breaks up the derivative of the error with respect to the parameters W 2 into a chain of derivatives along the line of the nodes of the computation graph. all we have to do for gradient computations is to come up with derivatives for each node in the computation graph. In our example these are d t t y d s s s d sum b 2 b 2 d prod W 2 2 h 2 2 h If we want to compute the gradient update for a parameter such as W 2 we compute values in a backward starting from the error term y . See Figure 13.9 for an illustration. To give more detail on the computation of the gradients in the backward starting at the bottom of the 2 . Recall that when we computed this d 12 t y 2 13.3. COMPUTATION GRAPHS 27 1 . 0 0 . 0 x W 1 3 4 0484 0 2 3 . 0258 0 3 2 prod . 116 . 0258 0 1 2 . 0484 4 . 0258 1 . 0484 . 0484 sum 2 . 0258 . 0258 . 731 . 119 . 0484 . 0258 W 2 5 5 . 0360 . 00587 3 . 06 prod . 246 . 246 . 0360 . 00587 b 2 2 . 0492 1 . 06 . 743 sum . 0492 . 0492 . 191 . 257 . 0492 t 1 . 0 . 0331 . 257 Figure Computation graph with gradients computed in the backward pass for the training example T 1 . 0 . Gradients are computed with respect to the input of the so some nodes that have two inputs also have two gradients. See text for details on the computations of the values. . 0935 . 0484 0 b 28 CHAPTER 13. NEURAL MACHINE TRANSLATION For the we use the formula d t d 12 t y 2 t y The given target output value given as training data is t 1 while we computed y 0 . 743 in the forward pass. the gradient for the norm is 1 0 . 743 0 . 257 . Note that we are using values computed in the forward pass for these gradient computations. For the lower we use the formula d s s s Recall that the formula for the is s 1 e s . Plugging in the value for s 1 . 06 computed in the forward pass into this formula gives us 0.191. The chain rule requires us to multiply this with the value that we just computed for the which gives us 0 . 191 0 . 257 0 . 0492 . For the lower sum we simply copy the previous since the is d sum b 2 b 2 Note that there are two gradients associated with the sum node. One with respect to the output of the prod and one with the b 2 parameter. In both the derivative is so both values are the same. the gradient in both cases is 0.0492. For the lower prod we use the formula d prod W 2 2 h h 2 2 So we dealt with scalar values. Here we encounter vectors for the the value of the hidden nodes h . 731 0 . T . The chain rule requires us to multiply this with the previously computed scalar 0 . 731 0 . 119 0 . 0492 T 0 . 0360 0 . 00587 As for the sum there are two inputs and hence two gradients. The other gradient is with respect to the output of the upper node. d prod W 2 2 h W 2 Similarly to we compute W 2 0 . T 5 5 0 . 0492 T 0 . 246 0 . 246 13.3. COMPUTATION GRAPHS 29 Having all the gradients in we can now read of the relevant values for weight up- dates. These are the gradients associated with trainable parameters. For the W 2 weight this is the second gradient of the prod node. So the new value for W 2 at time step t 1 is W 2 t W 2 t d prod W 2 t 2 t 5 5 0 . 0360 0 . 00587 The remaining computations are carried out in very similar since they form simply another layer of the feed-forward neural network. Our example did not include one special the output of a computation may be used multiple times in subsequent steps of a computation graphs. there are multiple output nodes that feed back gradients in the back-propagation pass. In this we add up the from these steps to factor in their added impact. Let us take a second look at what a node in a computation graph a function that computes its value links to input nodes obtain argument when processing an example in the forward the computed value a function that executes its gradient computation links to children nodes obtain downstream gradient when processing an example in the forward the computed gradient From an object oriented programming a node in a computation graph provides a forward and backward function for value and gradient computations. As instantiated in an computation it is connected with inputs and and is also aware of the dimensions of its variables its value and gradient. During forward and backward these variables are in. 13.3.3 Deep Learning Frameworks In the next we will encounter various network architectures. What all of these are the need for vector and matrix as well as the computation of to obtain weight update formulas. It would be quite tedious to write almost identical code to deal with each these variants. a number of frameworks have emerged to sup- port developing neural network methods for any chosen problem. At the time of the most prominent ones are 1 Python library that generates and compiles code and is build on Torch 2 machine learning library and a script language based on the programming 3 Python variant of 4 1 2 3 4 30 CHAPTER 13. NEURAL MACHINE TRANSLATION implementation by natural language processing researchers that can be used as a library in or and 5 more recent entry to the genre from These frameworks are less geared towards ready-to-use neural network but provide implementations of the vector space operations and computation of with seamless support of Our example from Section 13.2 can be implemented in a few lines of Python as we will show in this using the example of frameworks are quite You can execute the following commands on the Python command line interface if you installed pip install import import import as T The mapping of the input layer x to the hidden layer h uses a weight matrix W a bias vector b and a mapping function which consists of the linear combination and the activation function. x W b h Note that we x as a matrix. This allows us to process several training examples at once sequence of A good way to think about these of x and h is in term of a functional programming language. They symbolically operations. To actually a function that can be the method function is used. This example call to computes the values for the hidden nodes to the numbers in Table 13.1 on page The mapping from the hidden layer h to the output layer y is in the same fashion. we can a callable function to test the full network. predict 5 13.4. NEURAL LANGUAGE MODELS 31 Model training requires the of a cost function use the To formulate we need to the variable for the correct output. The overall cost is computed as average over all training examples. y cost Gradient descent training requires the computation of the derivative of the cost function with respect to the model parameters the values in the weight matrices W and and the bias vectors b and . A great of using is that it computes the derivatives for you. The following is also an example of a function with multiple inputs and multiple outputs. We have now all we need to training. The function updates the model parameters and returns the current predictions and cost. It uses a learning rate of 0.1. train Let us the training data. 0.7425526 0.7425526 0.7425526 0.7425526 The training function returns the prediction and cost before the updates. If we call the training function then the predictions and cost have changed for the better. we would loop over the training function until convergence. As discussed we may also break up the training data into mini-batches and train on one mini-batch at a time. 13.4 Neural Language Models Neural networks are a very powerful method to model conditional probability distributions with multiple inputs p a d . They are robust to unseen data points an unobserved in the training data. Using traditional statistical estimation we may address such a sparse data problem with back-off and which require insight into the prob- part of the conditioning context to drop and arbitrary choices many 32 CHAPTER 13. NEURAL MACHINE TRANSLATION Word 1 Word 2 Word 5 Word 3 Word 4 Figure Sketch of a neural language We predict a word w i based on its preceding words. N-gram language models which reduce the probability of a sentence to the product of word probabilities in the context of a few previous words p w i w i 4 w i 3 w i 2 w i 1 . Such models are a prime example for a conditional probability distribution with a rich conditioning context for which we often lack data points and would like to cluster information. In statistical language complex discounting and back-off schemes are used to balance rich evidence from lower order models the model p w i w i 1 with the sparse estimates from high order models. we turn to neural networks for help. 13.4.1 Feed-Forward Neural Language Models Figure 13.10 gives a basic sketch of a 5-gram neural network language model. Network nodes representing the context words have connections to a hidden which connects to the out- put layer for the predicted word. Representing Words We are immediately faced with a How do we rep- resent Nodes in a neural network carry real-numbered but words are discrete items out of a very large vocabulary. We cannot simply use token since the neural network will assume that token 124,321 is very similar to token 124,322 while in practice these are completely arbitrary. The same arguments applies to the idea of using bit encoding for token IDs. The words 1 1 1 0 0 0 T and 1 1 1 0 0 0 T have very similar but may have nothing to do with each other. While the idea of using such bit vectors is occasionally it does not appear to have any over what we consider next. we will represent each word with a high-dimensional one dimension per word in the and the value 1 for the dimension that matches the and 0 for the rest. The type of vectors are called one hot vector . For dog 0 0 0 1 0 0 0 0 ... T cat 0 0 0 0 0 0 1 0 ... T eat 1 0 0 0 0 0 0 0 ... T 13.4. NEURAL LANGUAGE MODELS 33 Word 1 Word 2 Word 3 Word 4 C C C C Word 5 Figure Full architecture of a feed-forward neural network language model. Context words w i 4 w i 3 w i 2 w i 1 are represented in a one-hot then projected into continuous space as word the same weight matrix C for all The predicted word is computed as a one-hot vector via a hidden layer. These are very large and we will continue to wrestle with the impact of this choice to represent words. One stopgap is to limit the vocabulary to the most 20,000 and pool all the other words in an OTHER token. We could also use word classes automatic clusters or linguistically motivated classes such as part-of-speech to reduce the of the vectors. We will revisit the problem of large vocabularies later. To pool evidence between we introduce another layer between the input layer and the hidden layer. In this each context word is individually projected into a lower space. We use the same weight matrix for each of the context thus generating a continuous space representation for each independent of its position in the conditioning context. This representation is commonly referred to as word embedding . Words that occur in similar contexts should have similar word For if the training data for a language model frequently contains the n-grams but the cute dog jumped but the cute cat jumped child hugged the cat tightly child hugged the dog tightly like to watch cat videos like to watch dog videos then the language model would from the knowledge that dog and cat occur in similar contexts and hence are somewhat interchangeable. If we like to predict from a context where dog occurs but we have seen this context only with the word cat then we would still like to treat this as positive evidence. Word enable generalizing between words and hence having robust predictions in unseen contexts 34 CHAPTER 13. NEURAL MACHINE TRANSLATION Neural Network Architecture See Figure 13.11 for a visualization of the architecture the fully feed forward neural network language consisting of the context words as one- hot-vector input the word embedding the hidden layer and predicted output word layer. The context words are encoded as one-hot vectors. These are then passed through the embedding matrix C resulting in a vector of point the word embedding. This embedding vector has typically in the order of 500 or 1000 nodes. Note that we use the same embedding matrix C for all the context words. Also note that mathematically there is not all that much going on here. Since the input to the multiplication to the matrix C is a one hot most of the input values to the matrix multiplication are zeros. we are selecting the one column in the matrix that corresponds to the input word ID. there is no use for an activation function here. In a the embedding matrix a lookup table C w j for word indexed by the word ID w j . C w j C w j Mapping to the hidden layer in the model requires concatenation of all context word em- C w j as input to a typical feed-forward using as activation function. h b h H j C w j j The output layer is interpreted as a probability distribution over words. As the linear combination s i of weights w and hidden node values h j is computed for each node i . s W h To ensure that it is indeed a proper probability we use the activation function to ensure that all values add up to one. p i s i s e s i j e s j What we described here is close to the neural probabilistic language model proposed by This model had one more it added direct connections of the context word to the output word. Equation 13.57 is replaced by s W h U C w j j Their paper reports that having such direct connections from context words to output words speeds up although does not ultimately improve performance. We will en- counter the idea of short-cutting hidden layers again a bit later when we discuss deeper models with more hidden layers. They are also called residual connections skip connections or even highway connections . 13.4. NEURAL LANGUAGE MODELS 35 Training We train the parameters of a neural language model embedding weight bias by processing all the n-grams in the training corpus. For each we feed the context words into the network and match the networks output against the one- hot vector of the correct word to be predicted. Weights are updated using back-propagation will go into details in the next Language models are commonly evaluated by which is related to the probability given to proper English text. A language model that likes proper English is a good language model. the training objective for language models is to increase the likelihood of the training data. During given a context x w n 4 w n 3 w n 2 w n 1 we have the correct value for the 1-hot vector y . For each training example x y likelihood is as L x y W y k log p k k Note that only one value y k is the others are 0. So this really comes down to the probability p k given to the correct word k . likelihood this way allows us to update all also the one that lead to the wrong output words. 13.4.2 Word Embedding Before we move it is worth the role of word in neural machine trans- and many other natural language processing tasks. We introduced them here as compact encoding of words in relatively high-dimensional say 500 or 1000 point In the of natural language at the time of this word have acquired the reputation of almost magical quality. Consider the role they play in the neural language language that we just described. They represent context words to enable prediction the next word in a sequence. Recall part of our earlier but the cute dog jumped but the cute cat jumped Since dog and cat occur in similar their on predicting the word jumped should be similar. It should be different from words such as dress which is unlikely to trigger the completion jumped . The idea that words that occur in similar contexts are semantically similar is a powerful idea in lexical semantics. At this point in the researchers love to cite John Rupert You shall know a word by the company it keeps. as Ludwig Wittgenstein put it a bit more 36 CHAPTER 13. NEURAL MACHINE TRANSLATION Figure Word projected into Semantically similar words occur close to each other. The meaning of a word is its use. Meaning and semantics are quite concepts with largely unresolved The idea of distributional lexical semantics is to word meaning by their distributional prop- in which contexts they occur. Words that occur in similar contexts dog and cat should have similar representations. In vector space such as the word that we use similarity can be measured by a distance the cosine distance the angle between the vectors. If we project the high-dimensional word down to two we can visualize word as shown in Figure 13.12. In this words that are similar festival are clustered together. But why stop We would like to have semantic representations so we can carry out semantic inference such as queen king queens queen Indeed there is some evidence that word embedding allow just that we better stop here and just note that word are a crucial tool in neural machine translation. 13.4. NEURAL LANGUAGE MODELS 37 13.4.3 Inference and Training Training a neural language model is computationally expensive. For billion word even with the use of training takes several days with modern compute clusters. Even using a neural language model as a scoring component in statistical machine translation decoding requires a lot of computation. We could restrict its use only to re-ranking n-best lists or or consider more methods for inference and training. Caching for Inference with a few it is actually possible to use this neural language model within the decoder. Word are for the so do not actually need to carry out the map- ping from one-hot vectors to word but just store them beforehand. The computation between and the hidden layer can be also partly carried out Note that each word can occur in one of the 4 slots for conditioning context a 5-gram language For each of the we can the matrix multiplication of word embedding vector and the corresponding of weights. at run we only have to sum up these at the hidden layer and apply the activation function. Computing the value for each output node is insanely since there are as many output nodes as vocabulary items. we are interested only in the score for a given word that was produced by the translation model. If we only compute its node we have a score that we can use. The last point requires a longer discussion. If we compute the node value only for the word that we want to score with the language we are missing an important step. To obtain a proper we need to normalize which requires the computation of the values for all the other nodes. We could simply ignore this problem and use the scores at face value. More likely words given a context will get higher scores than less likely and that is the main objective. But since we place no constraints on the we may work with models where some contexts give high scores to many while some contexts do not give preference for any. It would be if the node values in the layer were already normalized probabilities. There are methods to enforce this during training. Let us discuss training in and then move to these methods in Section 13.4.3. Noise Estimation We discussed earlier the problem that computing ties with a neural language model is very expensive due to the need to normalize the output node values y i using the function. This requires computing values for all output even if we are only interested in the score for a particular n-gram. To overcome the need for 38 CHAPTER 13. NEURAL MACHINE TRANSLATION this explicit normalization we would like to train a model that already has y i values that are normalized. One way is to include the constraint that the normalization factor Z x j e s j is close to 1 in the objective function. instead of the just the simple likelihood we may include the norm of the log of this factor. Note that if log Z x 0 then Z x 1 . L x y W y k log p k log 2 Z x k Another way to train a self-normalizing model is called noise estimation. The main idea is to optimize the model so that it can separate correct training examples from created noise examples. This method needs less computation during since it does not require the computation of all output node values. we are trying to learn the model distribution p m y x W . Given a noise p n y x in our case of language modeling a model p n y is a good choice we generate a set of noise examples U n in addition to the correct training examples U t . If both sets have the same size U n U t then the probability that a given example x y U n U t is predicted to be a correct training example is p correct x y p m y x W p m y x W p n y x The objective of noise estimation is to maximize p correct x y for correct train- examples x y U t and to minimize it for noise examples x y U n . Using log- we the objective function as L 1 2 U t x y U t log p correct x y 1 2 U n x y U n log p correct x y Returning the the original goal of a self-normalizing note that the noise p n y x is normalized. the model distribution is encouraged to produce values. If p m y x W would generally overshoot y p m y x W 1 then it would also give too high values for noise examples. generally undershooting would give too low values to correct translation examples. Training is since we only need to compute the output node value for the given train- and noise examples there is no need to compute the other since we do not nor- with the function. Given the of the training objective L we have now a complete computation graph that we can implement using standard deep learning as we have done before. These via gradient descent training its It may not be immediately obvious why optimizing towards classifying correct against noise examples gives rise to a model that also predicts the correct probabilities for n-grams. But this is a variant of methods that are common in statistical machine translation in the tuning phase. MIRA infused relaxation and PRO ranked follow the same principle. compute the gradients for all parameters W and use them for parameter updates 13.4. NEURAL LANGUAGE MODELS 39 1 Word 1 C E Word 2 Word 2 Word 3 copy values C E copy values C E Word 3 Word 4 Figure Recurrent neural language After predicting Word 2 in the context of following Word we re-use this hidden layer the correct Word to predict Word 3. the hidden layer of this prediction is re-used for the prediction of Word 4. 13.4.4 Recurrent Neural Language Models The feed-forward neural language model that we described above is able to use longer con- texts than traditional statistical back-off since it has more means to deal with unknown contexts. the use of word to make use of similar and the robust handling of unseen words in any context position. it is possible to condition on much larger contexts than traditional statistical models. In large 20-gram have been reported to be used. instead of using a context word recurrent neural networks may condition on context sequences of any length. The trick is to re-use the hidden layer when predicting word w n as additional input to predict word w n 1 . See Figure 13.13 for an illustration. the model does not look any different from the feed-forward neural language model that we discussed so far. The inputs to the network is the word of the sentence w 1 and a second set of neurons which at this point indicate the start of the sentence. The word embedding of w 1 and the start-of-sentence neurons map into a hidden layer h 1 which is then used to predict the output word w 2 . This model uses the same architecture as Words and are represented with one-hot word and the hidden layer 500 real valued neurons. We use a activation function at the hidden layer and the function at the output layer. Things get interesting when we move to predicting the third word w 3 in the sequence. One input is the directly preceding now word w 2 as before. the neurons in the network that we used to represent start-of-sentence are now with values from the hidden layer of the previous prediction of word w 2 . In a these neurons encode the 40 CHAPTER 13. NEURAL MACHINE TRANSLATION H Word 1 E H Word 2 Word 2 Word 3 E H E H Word 3 Word 4 Figure Back-propagation through By unfolding the recurrent neural network over a number of prediction steps we can derive update formulas based on the training objective of predicting all output words and back-propagation of the error via gradient descent. previous sentence context. They are enriched at each step with information about a new input word and are hence conditioned on the full history of the sentence. even the last word of the sentence is conditioned in part on the word of the sentence. the model is it has less weights than a 3-gram feed-forward neural language model. How do we train such a model with arbitrarily long One At the initial stage the second word from the we have the same architecture and hence the same training procedure as for feed-forward neural networks. We assess the error at the output layer and propagate updates back to the input layer. We could process every training example this way essentially by treating the hidden layer from the previous training example as input the current example. this we never provide feedback to the representation of prior history in the hidden layer. The back-propagation through time training procedure Figure unfolds the re- current neural network over a number of by going back 5 word Note despite limiting the unfolding to 5 time the network is still able to learn dependencies over longer distances. Back-propagation through time can be either applied for each training example called time but this is computationally quite expensive. Each time computations have to be carried out over several steps. we can compute and apply weight updates in mini- batches Section we process a larger number of training examples or the entire and then update the weights. Given modern compute fully unfolding the recurrent neural network has become more common. While recurrent neural networks have in theory arbitrary given a training its size is actually known and so we can fully construct the com- graph for each given training the error as the sum of word prediction and then carry out back-propagation over the entire sentence. This does require that we can quickly build computation graphs so-called dynamic computation graphs which is currently supported by some better than others. 13.4. NEURAL LANGUAGE MODELS 41 13.4.5 Long Short-Term Memory Models Consider the following step during word prediction in a sequential language After much economic progress over the the country has The directly preceding word country will be the most informative for the prediction of the word has all the previous words are much less relevant. In the importance of words decays with distance. The hidden state in the recurrent neural network will always be updated with the most recent and its memory of older words is likely to diminish over time. But more distant words are much more as the following example The country which has made much economic progress over the years still has In this the of the verb have depends on the subject country which is separated by a long subordinate clause. Recurrent neural networks allow modeling of arbitrarily long sequences. Their architecture is very simple. But this simplicity causes a number of problems. The hidden layer plays double duty as memory of the network and as continuous space representation used to predict output words. While we may sometimes want to pay more attention to the directly previous and sometimes pay more attention to the longer there is no clear mechanism to con- that. If we train the model on long then any update needs to back propagate to the beginning of the sentence. propagating through so many steps raises concerns that the impact of recent information at any step drowns out older information. 6 The rather confusingly named long short-term memory neural network addresses these issues. Its design is quite although it is not very to use in practice. A core distinction is that the basic building block of the so-called cell contains an explicit memory state. The memory state in the cell is motivated by digital memory cells in ordinary computers. Digital memory cells offer operations to and reset. While a digital memory cell may store just a single a cell stores a real number. the operations in a cell are regulated with a real which are called gates Figure 6 Note that there is a corresponding exploding gradient where over long distance gradient values become too large. This is typically suppressed by clipping limiting them to a maximum value set as a hyper parameter. 42 CHAPTER 13. NEURAL MACHINE TRANSLATION Layer Time t-1 m Preceding Layer X i forget gate m o h Next Layer Y Layer Time t Figure A cell in a neural network. As recurrent neural it receives input from the layer x and the hidden layer values from the previous time step t 1 . The memory state m is updated from the input state i and the previous times value of the memory state m t 1 . Various gates channel information in the cell towards the output value o . The input gate parameter regulates how much new input changes the memory state. The forget gate parameter regulates how much of the prior memory state is retained The output gate parameter regulates how strongly the memory state is passed on to the next layer. marking the and output values with the time step t we the of information within a cell as follows. memory t gate input input t gate forget memory t 1 output t gate output memory t The hidden node value h t passed on to the next layer is the application of an activation function f to the output value. h t f output t An layer consists of a vector of just as traditional layers consist of a vector of nodes. The input to layer is computed in the same way as the input to a recurrent neural network node. Given the node values for the prior layer x t and the values for the hidden layer from the previous time step h t 1 the input value is the typical combination of matrix multiplication with weights W x and W h and an activation function g . input t g W x x t W h h t 1 13.4. NEURAL LANGUAGE MODELS 43 But how are the gate parameters They actually play a fairly important role. In par- we would like to give preference to recent input input 1 rather re- past memory forget 1 or pay less attention to the cell at the current point in time output 0 this decision has to be informed by a broad view of the context. How do we compute a value from such a complex conditioning we treat it like a node in a neural network. For each gate a input forget output we matrices W W ha and W ma to compute the gate parameter value by the multiplication of weights and node values in the previous layer x t the hidden layer h t 1 at the previous time and the memory states at the previous time step memory t 1 followed by an activation function h . gate a h W x t W ha h t 1 W ma memory t 1 are trained the same way as recurrent neural using back-propagation through time or fully unrolling the network. While the operations within a cell are more complex than in a recurrent neural all the operations are still based on matrix and differentiable activation functions. we can compute gradients for the objective function with respect to all parameters of the model and compute update functions. 13.4.6 Gated Recurrent Units cells add a large number of additional parameters. For each gate multiple weight matrices are added. More parameters lead to longer training times and risk As a simpler gated recurrent units have been proposed and used in neural translation models. At the time of cells seem to make a comeback in neural machine but both are still commonly used. See Figure 13.16 for an illustration for cells. There is no separate memory just a hidden state that serves both purposes. there are only two gates. These gates are predicted as before from the input and the previous state. update t g W update input t U update state t 1 bias update reset t g W reset input t U reset state t 1 bias reset The gate is used in the combination of the input and previous state. This is combination is identical to traditional recurrent neural except that the previous states impact is scaled by the reset gate. Since the gates value is between 0 and this may give preference to the current input. combination t f W input t U reset t state t 1 the update gate is used for a interpolation of the previous state and the just computed combination. This is done as a weighted where the update gate balances between the two. 44 CHAPTER 13. NEURAL MACHINE TRANSLATION Layer Time t-1 h reset gate Preceding Layer X x h Next Layer Y Layer Time t Figure Gated Recurrent Unit a of long short term memory cells. state t update t state t 1 update t combination t bias In one extreme the update gate is and the previous state is passed through directly. In another extreme the update gate is and the new state is mainly determined from the with as much impact from the previous state as the reset gate allows. It may seem a bit redundant to have two operations with a gate each that combine prior state and input. these play different roles. The operation yielding combination t is a classic recurrent neural network component that allows more complex computations in the combination of input and output. The second operation yielding the new hidden state and the output of the unit allows for bypassing of the in- enabling long-distant memory that simply passes through information during back- passes through the thus enabling long-distance dependencies. 13.4.7 Deep Models The currently fashionable name deep learning for the latest wave of neural network research has a real motivation. Large gains have been seen in tasks such as vision and speech recognition due to stacking multiple hidden layers together. More layers allow for more complex just as having sequences of traditional computation components allows for more complex computations such as and multiplication of numbers. While this has been generally recognized for a long modern hardware enabled to train such deep neural networks on real world problems. 13.4. NEURAL LANGUAGE MODELS 45 Input Hidden Layer Output Input Hidden Layer 1 Hidden Layer 2 Hidden Layer 3 Output Input Hidden Layer 1 Hidden Layer 2 Hidden Layer 3 Output Shallow Deep Stacked Deep Transition Figure Deep recurrent neural networks. The input is passed through a few hidden layers before an output prediction is made. In deep stacked the hidden layers are also connected a layer values at time step t depends on its value at time step t 1 as well as the previous layer at time step t . In deep transitional the layers at any time step t are sequentially connected and hidden layer is also informed by the last layer at time step t 1 . And we learned from experiments in vision and speech that having a and even dozens of layers does give increasingly better quality. How does the idea of deep neural networks apply to the sequence prediction tasks common in There are several options. Figure 13.17 gives two examples. In shallow neural the input is passed to a single hidden from which the output is predicted. a sequence of hidden layers is used. These hidden layers h may be deeply stacked so that each layer acts like the hidden layer in the shallow recurrent neural network. Its state is conditioned on its value at the previous time step h t 1 and the value of previous layer in the sequence h 1 . h 1 f 1 h t 1 1 x t layer h f i h t 1 h 1 for i 1 y t f i h prediction from last layer I the hidden layers may be directly connected in deep transitional where the hidden layer h 1 is informed by the last hidden layer at the previous time step h t 1 but all other hidden layers are not connected to values from previous time steps. h 1 f 1 h t 1 x t layer h f i h 1 for i 1 y t f i h prediction from last layer I 46 CHAPTER 13. NEURAL MACHINE TRANSLATION In all these the function f i may be a layer multiplication plus activation an cell or a cell. Experiments with using neural language models in traditional statistical machine have shown with hidden layers While modern hardware allows training of deep they do stretch computational resources to their practical limit. Not only are there more computations in the neural convergence of training is typically slower. Adding skip connections the input directly to the output or the hidden sometimes speeds up but we still talking about a several times longer training times than shallow networks. Further Readings The vanguard of neural network research tackled language models. A prominent reference for neural language model is who implement an n-gram model as a feed-forward neural network with the history words as input and the predicted word as output. introduce such language models to machine translation called space language and use them in similar to the earlier work in speech recognition. propose a number of speed-ups. They made their implementation avail- able as a open source toolkit which also supports training on a graphical processing unit By clustering words into classes and encoding words as pair of class and word-in-class reduce the computational complexity to allow integration of the neural network language model into the decoder. Another way to reduce computational complexity to enable decoder integration is the use of noise estimation by which roughly self-normalizes the output scores of the model during hence removing the need to compute the values for all possible output words. and compare the two techniques class-based word encoding with normalized scores vs. estimation without normalized scores and show that the letter gives better performance with much higher speed. As another way to allow straightforward decoder Wang convert a space language model for a short list of 8192 words into a traditional n-gram language model in format. Wang present a method to merge a continuous space language model with a traditional n-gram language to take advantage of both better estimate for the words in the short list and the full coverage from the traditional model. Finch use a recurrent neural network language model to n-best lists for a system. compare feed-forward with long short-term neural network language a variant of recurrent neural network language showing better performance for the latter in a speech recognition re-ranking task. reports improvements with n-best lists of machine translation systems with a recurrent neural network language model. Neural language model are not deep learning models in the sense that they use a lot of hidden layers. show that having 3-4 hidden layers improves over having just the typical 1 layer. Language Models in Neural Machine Traditional statistical machine translation models have a straightforward mechanism to integrate additional knowledge such as a large out of domain language model. It is harder for end-to-end neural machine translation. add a language model trained on additional monolingual data to this in form of a recurrently 13.5. NEURAL TRANSLATION MODELS 47 the house is big . . Given word Embedding Hidden state Predicted word the house is big . . Figure Sequence-to-sequence encoder-decoder Extending the language we con- the English input sentence the house is big with the German output sentence . The dark green box processing the end-of-sentence token contains the embedding of the entire input sentence . neural network that runs in parallel. They compare the use of the language model in re-ranking re- against deeper integration where a gated unit regulates the relative contribution of the language model and the translation model when predicting a word. 13.5 Neural Translation Models We are prepared to look at actual translation models. We have already done most of the since the most commonly used architecture for neural machine translation is a straightforward extension of neural language models with one an alignment model. 13.5.1 Encoder-Decoder Approach Our stab at a neural translation model is a straightforward extension of the language model. Recall the idea of a recurrent neural network to model language as a sequential process. Given all previous such a model predicts the next word. When we reach the end of the we now proceed to predict the translation of the one word at a time. See Figure 13.18 for an illustration. To train such a we simply concatenate the input and output sentences and use the same method as to train a language model. For we feed in the input and then go through the predictions of the model until it predicts an end of sentence token. How does such a network Once processing reaches the end of the input sentence predicted the end of sentence marker the hidden state encodes its meaning. In other the vector holding the values of the nodes of this hidden layer is the input sentence embedding . This is the encoder phase of the model. Then this hidden state is used to produce the translation in the decoder phase. 48 CHAPTER 13. NEURAL MACHINE TRANSLATION we are asking a lot from the hidden state in the recurrent neural network here. During encoder it needs to incorporate all information about the input sentence. It cannot forget the words towards the end of the sentence. During the decoder not only does it need to have enough information to predict each next there also needs to be some accounting for what part of the input sentence has been already and what still needs to be covered. In the proposed models works reasonable well for short sentences but fails for long sentences. Some minor to this model have been such using the sentence embedding state as input to all hidden states of the decoder phase of the model. This makes the decoder structurally different from the encoder and reduces some of the load from the hidden state during since it does not need to remember anymore the input. Another idea is to reverse the order of the output so that the last words of the input sentences are close to the last words of the output sentence. in the following we will embark on a more improvement of the by explicitly alignment of output words to input words. 13.5.2 Adding an Alignment Model At the time of the state of the art in neural machine translation is a sequence-to- sequence encoder-decoder model with attention. That is a but it is essentially the model we just described in the previous with a explicitly alignment mechanism. In the deep learning this alignment is called attention we are using the words alignment and attention interchangeable here. Since the attention mechanism does add a bit of complexity to the we are now slowly building up to by taking a look at the then the and the attention mechanism. Encoder The task of the encoder is to provide a representation of the input sentence. The input sentence is a sequence of for which we consult the embedding matrix. as in the basic language model described we process these words with a recurrent neural network. This results in hidden states that encode each word with its left all the preceding words. To also get the right we also build a recurrent neural network that runs right- or more from the end of the sentence to the beginning. Figure 13.19 illustrates the model. Having two recurrent neural networks running in two directions is called a bidirectional recurrent neural network . the encoder consists of the embedding lookup for each input word x j and the mapping that steps through the hidden states h j and h j h j f h j E x j h j f h j 1 E x j 13.5. NEURAL TRANSLATION MODELS 49 Input Word Left-to-Right Recurrent Right-to-Left Recurrent Figure Neural machine translation part input encoder. It consists of two recurrent neural running right to left and left to right recurrent neural The encoder states are the combination of the two hidden states of the recurrent neural networks. In the equation we used a generic function f for a cell in the recurrent neural net- work. This function may be a typical feed-forward neural network layer such as f x Ax b or the more complex gated recurrent units or long short term memory cells The original paper proposing this approached used but lately have become more popular. Note that we could train these models by adding a step that predicts the next word in the but we are actually training it in the context of the full machine translation model. Limiting the description to the its output is a sequence of word representations that concatenate the two hidden states h j h j . Decoder The decoder is also a recurrent neural network. It takes some representation of the input con- text on that in the next section on the attention and the previous hidden state and output word and generates a new hidden decoder state and a new output word prediction. See Figure 13.20 for an illustration. we start with the recurrent neural network that maintains a sequence of hidden states s i which are computed from the previous hidden state s i 1 the embedding of the previous output word i 1 and the input context c i we still have to s i f s i 1 i 1 c i there are several choices for the function f that combines these inputs to generate the next hidden linear transforms with activation etc. the choice here matches the encoder. if we use for the then we also use for the decoder. From the hidden state. we now predict the output word. This prediction takes the form of a probability distribution over the entire output vocabulary. If we have a vocabulary 50,000 then the prediction is a 50,000 dimensional each element corresponding to the probability predicted for one word in the vocabulary. 50 CHAPTER 13. NEURAL MACHINE TRANSLATION c i-1 s i-1 c i s i Context State t i-1 t i Prediction Word y i-1 y i Selected Word i-1 i Embedding Figure Neural machine translation part output decoder. Given the context from the input and the embedding of the previously selected new decoder states and word are computed. The prediction vector t i is conditioned on the decoder hidden state s i 1 the embedding of the previous output word i 1 and the input context c i . t i W U s i 1 V i 1 Cc i Note that we repeat the conditioning on i 1 since we use the hidden state s i 1 and not s 1 . This separates the encoder state progression from s i 1 to s i from the prediction of the output word t i . The is used to convert the raw vector into a probability where the sum of all values is 1. the highest value in the vector indicates the output word token y i . Its word embedding i 1 informs the next time step of the recurrent neural network. During the correct output word y i is so training proceeds with that word. The training objective is to give as much probability mass as possible to the correct output word. The cost function that drives training is hence the negative log of the probability given to the correct word translation. cost log t i y i we want to give the correct word the probability which would mean a negative log probability of but typically it is a lower hence a higher cost. Note that the cost function is tied to individual the overall sentence cost is the sum of all word costs. During inference on a new test we typically chose the word y i with the highest value in t i use its embedding i for the next steps. But we will also explore beam search where the next likely words are selected as y i creating a different conditioning context for the next words. More on that later. 13.5. NEURAL TRANSLATION MODELS 51 Encoder States Attention Input Context Hidden State Output Words Figure Neural machine translation part attention model. Associations are computed between the last hidden state of the decoder and the word representations These are used to compute a weighted sum of encoder states. Attention Mechanism We currently have two loose ends. The decoder gave us a sequence of word representations h j h j h j and the decoder expects a context c i at each step i . We now describe the attention mechanism that ties these ends together. The attention is hard to visualize using our typical neural network but Figure 13.21 gives at least an idea what the input and output relations are. The attention mechanism is informed by all input word representations h j h j and the previous hidden state of the decoder s i 1 and it produces a context state c i . The motivation is that we want to compute an association between the decoder state contains information where we are in the output sentence and each input word. Based on how strong this association or in other words how relevant each particular input word is to produce the next output we want to weight the impact of its word we compute this association with a layer weight vectors w a u a and bias value b a a s i 1 h j w s i 1 u h j b a The output of this computation is a scalar indicating how important input word j is to produce output word i . We normalize this attention so that the attention values across all input words j add up to using the exp a s i 1 h j k exp a s i 1 h k Now we use the normalized attention value to weigh the contribution of the input word representation h j to the context vector c i and we are done. c i h j j 52 CHAPTER 13. NEURAL MACHINE TRANSLATION Simply adding up word representation vectors or may at seem an odd and simplistic thing to do. But it is very common practice in deep learning for natural language processing. Researchers have no qualms about using sentence that are simply the sum of word and other such schemes. 13.5.3 Training With the complete model in we can now take a closer look at training. One challenge is that the number of steps in the decoder and the number of steps in the encoder varies with each training example. Sentence pairs consist of sentences of different so we cannot have the same computation graph for each training example but instead have to dynamically create the computation graph for each of them. This technique is called unrolling the recurrent neural and we already discussed it with regard to language models Section The fully unrolled computation graph for a short sentence pair is shown in Figure 13.22. Note a couple of things. The error computed from this one sentence pair is the sum of the errors computed for each word. When proceeding to the next word we use the correct word as conditioning context for the decoder hidden state and the word prediction. the training objective is based on the probability mass given to the correct given a perfect context. There have been some attempts to use different training such as the but they have not yet been shown to be superior. Practical training of neural machine translation models requires which are well suited to the high degree of parallelism inherent in these deep learning models think of the many matrix To increase parallelism even we process several sentence pairs at once. This implies that we increase the of all the state tensors. To given an example. We represent each input word in sentence pair with a vector h j . Since we already have a sequence of input these are lined up in a matrix. When we process a batch of sentence we again line up these matrices into a 3-dimensional tensor. to give another the decoder hidden state s i is a vector for each output word. Since we process a batch of we line up their hidden states into a matrix. Note that in this case it is not helpful to line up the states for all the output since the states are computed sequentially. Recall the computation of the attention mechanism a s i 1 h j W a s i 1 U a h j b a We can pass this computation to the GPU with a matrix of encoder states s i 1 and a 3- dimensional tensor of input h j resulting in a matrix of attention values for the sentence one dimension for the input Due to the massive re-use of values in W a U a and b a as well as the inherent parallelism of this can show their true power. You may feel that we just created a glaring contradiction. we argued that we have to process one training example at a since sentence pairs typically have different and 13.5. NEURAL TRANSLATION MODELS 53 the house is big . Input Word Left-to-Right Recurrent Right-to-Left Recurrent Attention Input Context Hidden State Output Word Predictions Error Given Output Words Output Word Embedding . Figure Fully unrolled computation graph for training example with 7 input tokens s the house is big and 6 output tokens is . The cost function is computed for each output word and summed up across the sentence. When walking through the the correct previous output words are used as conditioning context. 54 CHAPTER 13. NEURAL MACHINE TRANSLATION Figure To make better use of parallelism in we process a batch of training examples at a time. Converting a batch of training examples into a set of mini batches that have similar length. This wastes less computation on words hence computation graphs have different size. we argued for 100 sentence pairs together to better exploit parallelism. These are indeed goals. See Figure 13.23. When batching training examples we have to consider the maxi- mum sizes for input and output sentences in a batch and unroll the computation graph to these maximum sizes. For shorter we the remaining gaps with non-words and keep track of where the valid data is with a mask . This for that we have to ensure that no attention is given to words beyond the length of the input and no errors and gradient updates are computed from output words beyond the length of the output sentence. To avoid wasted computations on a nice trick is to sort the sentence pairs in the batch by length and break it up into mini-batches of similar length. 7 To training consists of the following steps the training corpus avoid undue biases due to temporal or topical Break up the corpus into maxi-batches Break up each maxi-batch into mini-batches Process each gather gradients Apply all gradients for a maxi-batch to update the parameters training neural machine translation models takes about epochs through entire training A common stopping criteria is to check progress of the model on a val- set is not part of the training and halt when the error on the validation set does not improve. Training longer would not lead to any further improvements and may even degrade performance due to 13.5.4 Beam Search Translating with neural translation models proceeds one step at a time. At each we predict one output word. In our we compute a probability distribution over all words. 7 There is a bit of confusion of the technical terms here. the entire training corpus is called a batch as used in the contrast between batch updating and online updating. In that smaller batches with a subset of the are called mini-batches Section 13.2.6 on page we use the term batch maxi-batch for such a and mini-batch for a subset of the subset. 13.5. NEURAL TRANSLATION MODELS 55 c i-1 c i Context y i the i cat s i-1 s i State this t i-1 t i Prediction Word of fish y i-1 y i Selected Word there dog i-1 i Embedding these Figure Elementary decoding The model predicts a word prediction probability distribution. We select the most likely word the Its embedding is part of the conditioning context for the next word prediction decoder We then pick the most likely word and move to the next prediction step. Since the model is conditioned on the previous output word Equation we use its word embedding in the conditioning context for the next step. See Figure 13.24 for an illustration. At each time we obtain a probability distribution over words. In this distribution is most often quite only few words or maybe even just one word amass almost all of the probability. In the the word the received the highest so we pick it as the output word. A real example of how a neural machine translation model translates a German sentence into English is shown in Figure 13.25. The model tends to give if not almost mass to the top but the sentence translation also indicates word choice such as believe vs. think or different vs. various There is also ambiguity about grammatical such as if the sentence should start with the discourse connective but or the subject I This process suggests that we perform 1-best greedy search. This makes us vulnerable to the so-called garden-path problem . Sometimes we follow a sequence of words and realize too late that we made a mistake early on. In that the best sequence consists of less probable words initially which are redeemed by subsequent words in the context of the full output. Consider the case of having to produce an idiomatic phrase that is The words of these phrases may be really odd word choices by themselves piece of cake for easy Only once the full phrase is their choice is redeemed. Note that we are faced with the same problem in traditional statistical machine translation models arguable even more so there since we rely on sparser contexts when making for the next words. Decoding algorithms for these models keep a list of the n-best candidate hypotheses expand them and keep the n-best expanded hypotheses. We can do the same for neural translation models. 56 CHAPTER 13. NEURAL MACHINE TRANSLATION Input Sentence er clever um seine so Art . Output Word Predictions Best Alternatives but however I yet and nor ... I also it in nor he ... also think do believe too ... believe think feel do ... he that it him ... is has was ... clever smart ... enough to about for in of around ... keep maintain hold be have make ... his its statements what out the ... statements testimony messages comments ... vague in ambiguous ... enough and ... so to in and just that ... they that it can you we to ... can may could are will might ... be have interpret get ... interpreted ... in on differently as to for by ... different a various several ways some ... ways way manner ... . S ... Figure Word predictions of the neural machine translation model. most of the mass is given to the top but semantically related words may rank believe vs. think The units are explain in Section 13.6.2 on page 61. 13.5. NEURAL TRANSLATION MODELS 57 y i the cat this of fish there dog the this these cat cat cats dog cats these Figure Beam search in neural machine translation. After committing to a short list of output words beam new word predictions are made for each. These differ since the committed output word is part of the conditioning context to make predictions. When predicting the word of the output we keep a beam of the top n most likely word choices. They are scored by their probability. we use each of these words in the beam in the conditioning context for the next word. Due to this we make different word predictions for each. We now multiply the score for the partial translation this point just the probability for the and the probabilities from its word predictions. We select the highest scoring word pairs for the next beam. See Figure 13.26 for an illustration. This process continues. At each time we accumulate word translation giving us scores for each hypothesis. A sentence translation is when the end of sentence token is produced. At this we remove the completed hypothesis from the beam and reduce beam size by 1. Search when no hypotheses are left in the beam. Search produces a graph of as shown in Figure 13.27. It starts with the start of sentence symbol s and its paths terminate with the end of sentence symbol . Given the compete the resulting translations can be obtained by following the back-pointers. The complete hypothesis one that ended with a with the highest score points to the best translation. When choosing among the best we score each with the product of its word prediction probabilities. In we get better results when we normalize the score by the output length of a divide by the number of words. We carry out this normalization after search is completed. During all translations in a beam have the same so the normalization would make no difference. Note that in traditional statistical machine we were able to combine if they share the same conditioning context for future feature functions. This not possible anymore for recurrent neural networks since we condition on the entire output word sequence from the beginning. As a the search graph is generally less diverse than search 58 CHAPTER 13. NEURAL MACHINE TRANSLATION Figure Search graph for beam search decoding in neural translation models. At each time the n 6 best partial translations are selected. An output sentence is complete when the end of sentence token is predicted. We reduce the beam after that and terminate when n full sentence translations are completed. Following the back-pointers from the end of sentence tokens allows us to read them off. Empty boxes represent hypotheses that are not part of any complete path. graphs in statistical machine translation models. It is really just a search tree where the number of complete paths is the same as the size of the beam. Further Readings The attention model has its roots in a sequence-to-sequence model. use recurrent neural networks for the approach. use a short- term network and reverse the order of the source sentence before decoding. The seminal work by adds an alignment model called to link generated output words to source which includes conditioning on the hidden state that produced the preceding target word. Source words are represented by the two hidden states of recurrent neural networks that process the source sentence left-to-right and right-to-left. propose variants to the attention mechanism they call attention and also a hard-constraint attention model attention which is restricted to a Gaussian distribution around a input word. To explicitly model the trade-off between source context input and target context already produced target Tu introduce an interpolation weight that scales the impact of the source context state and the previous hidden state and the last word when predicting the next hidden state in the decoder. Tu augment the attention model with a reconstruction step. The generated output is translated back into the input language and the training objective is extended to not only include the likelihood of the target sentence but also the likelihood to the reconstructed input sentence. 13.6 The previous section gave a comprehensive description of the currently most commonly used basic neural translation model architecture. It performs fairly well out of the box for many 13.6. REFINEMENTS 59 Checkpoint ensemble Multi-run ensemble Figure Two methods to generate alternative systems for Checkpoint uses model dumps from various stages of the training while multi-run starts dent training runs with different initial weights and order of the training data. language pairs. Since its a number of have been proposed. We will describe them in this section. Some of the are fairly some target particular use cases or data To given one the best performing system at the recent 2017 evaluation campaign used ensemble decoding byte pair encoding to address large added synthetic data derived from monolingual target side data and used deeper models 13.6.1 Ensemble Decoding A common technique in machine learning is to not just build one system for your but multiple ones and then combine them. This is called an ensemble of systems. It is such a successful strategy that various methods have been proposed to systematically build for instance by using different features or different subsets of the data. For neural one straightforward way is to use different or stop at different points in the training process. Why does it The intuitive argument is that each system makes different mistakes. When two systems then they are more likely both rather than both make the same mistake. One can also see the general principle at play in human such as setting up committees to make decisions or the democratic voting in elections. Applying ensemble methods to our case of neural machine we have to address two generating alternate and combining their output. Generating Alternative Systems See Figure 13.28 for an illustration of two methods for the generating alter- native system. When training a neural translation we iterate through the training data until some stopping criteria is met. This is typically a lack of improvements of the cost function applied to a validation set in or the translation performance on that validation set in During we dump out the model at intervals every 10,000 iteration of batch Once training is we can look back at the performance of the 60 CHAPTER 13. NEURAL MACHINE TRANSLATION Model Model Model Model 1 2 3 4 Model Average the cat this of fish there dog these Figure Combining predictions from a ensemble of Each model independently predicts a probability distribution over output which are averaged into a combined distribution. model these different stages. We then pick 4 models with the best performance translation quality measured in This is called checkpoint since we select the models at different checkpoints in the training process. Multi-run requires building systems in completely different training runs. As mentioned this can be accomplished by using different random initialization of which leads training to seek out different local optima. We also randomly the training so using different random order will also lead to different training outcomes. Multi-run usually works a good deal but it is also computationally much more expensive. Note that multi-run can also build on checkpoint Instead of combining the end points of we apply checkpoint to each and then combine those ensembles. Combine System Output Neural translation models allow the combination of several systems fairly deeply. Recall that the model predicts a probability distribution over possible output and then com- to one of the words. This is where we combine the different trained models. Each model predicts a probability distribution and we then combine their predictions. The combination is done by simple averaging over the distributions. The averaged distribution is then the basis for selecting an output word. See Figure 13.29 for an illustration. There may be some to weighing the different systems although in our way of generating they will all have very similar so this is not typically done. 13.6. REFINEMENTS 61 with Right-to-Left Decoding One more tweak on the idea of Instead of building multiple systems with different random we can also build one set of system as and then a second set of system where we reverse the order of the output sentences. The second set of systems are called right-to-left although arguably this is not a good name since it makes no sense for languages such as Arabic or Hebrew where the normal writing order is right to left. The deep integration we described just above does not work anymore for the combination of left-to-right and right-to-left since they produce output in different order. we have to resort to . This involves several Use an ensemble of left-to-right systems to generate an n-best list of candidate for each input sentence. Score each candidate translation with the individual left-to-right and right-to-left Combine the scores of the different models for each select the candidate with the best score for each input sentence. Scoring a given candidate translation with a right-to-left system does require the require forced decoding a special mode of running inference on an input but predicting a given output sentence. This mode is actually much closer to training also an output translation is the regular inference. 13.6.2 Large Vocabularies law tells us that words in a language are very unevenly distribution. there is always a large tail of rare words. New words come into the language all the time woke and we also have to deal with a very large inventory of including company names Microsoft On the other neural methods are not well equipped to deal with such large The ideal representations for neural networks are continuous space vectors. This is why we convert discrete objects such as words into such word ultimately the discrete nature of words shows up. On the input we need to train an embedding matrix that maps each word into its embedding. On the output side we predict a probability distribution over all output words. The latter is generally the bigger since the amount of computation involved is linear with the size of the making this a very large matrix operation. neural translation models typically restrict the vocabulary 20,000 to 80,000 words. In initial work on neural machine only the most frequent words were and all others represented by a unknown or other tag. The translation of these rare words was handled with a back-off dictionary. 62 CHAPTER 13. NEURAL MACHINE TRANSLATION Obama receives the relationship between Obama and is not exactly friendly . the two wanted to talk about the implementation of the international agreement and about activities in the Middle East . the meeting was also planned to cover the with the Palestinians and the disputed two state solution . relations between Obama and have been for years . Washington the continuous building of settlements in Israel and uses of a lack of initiative in the peace process . the relationship between the two has further deteriorated because of the deal that Obama negotiated on Iran atomic . in March at the invitation of the ans made a controversial speech to the US Congress which was partly seen as an to Obama . the speech had not been agreed with Obama who had rejected a meeting with reference to the election that was at that time pending in Israel . Figure Byte pair encoding applied to English used 49,500 Word splits are indicated with Note that the data is also and true-cased. The more common approach today is to break up rare words into units . This may seem a bit crude but is actually very similar to standard approaches in statistical machine translation to handle compounds website web site and morphology follow convolutions convolution s It is even a decent approach to the problem of transliteration of names which are traditionally handled by a sub-modular letter translation component. A popular method to create an inventory of units and legitimate words is byte pair encoding . This method is trained on the parallel corpus. the words in the corpus are split into characters original spaces with a special space the most frequent pair of characters is merged this may be t and h into This step is repeated for a given number of times. Each of these steps increases the vocabulary by beyond the original inventory of single characters. The example mirrors quite well the behavior of the algorithm on real-world data sets. It starts with grouping together with frequent letter combinations and then joins frequent words the in of At the end of this the most frequent words will emerge as single while rare words consist of still See Figure 13.30 for an where units are indicated with two symbols After 49,500 byte pair encoding the vast majority of words are while rarer words are broken up the split seem to be motivated pending but mostly they are not Note also the decomposition of the relatively rare name . Further Readings A limitation of neural machine translation models is the burden to support very large vocabularies. To avoid typically the vocabulary is reduced 13.6. REFINEMENTS 63 to a shortlist 20,000 and the remaining tokens are replaced with the unknown word to- ken To translate such an unknown Jean resort to a separate dictionary. Arthur argue that neural translation models are worse for rare words and interpolate a traditional probabilistic bilingual dictionary with the prediction of the neural machine translation model. They use the attention mechanism to link each target word to a distribution of source words and weigh the word translations accordingly. Source words such as names and numbers may also be directly copied into the target. use a so-called switching network to predict either a traditional translation operation or a copying operation aided by a layer over the source sentence. They the training data to change some target words into word positions of copied source words. augment the word prediction step of the neural translation model to either translate a word or copy a source word. They observe that the attention mechanism is mostly driven by semantics and the language model in the case of word but by location in case of copying. To speed up Mi use traditional statistical machine translation word and phrase translation models to the target vocabulary for mini batches. split up all words to sub-word using character n-gram models and a segmentation based on the byte pair encoding compression algorithm. 13.6.3 Using Monolingual Data A key feature of statistical machine translation system are language trained on very large monolingual data set. The larger the language the higher translation quality. Language models trained on up to a trillion words crawled from the general web have been used. it is a surprise that the basic neural translation model does not use any additional monolingual its language model aspect conditioning of the previous hidden decoder state and the previous is trained jointly with the translation model aspect on the input Two main ideas have been proposed to improve neural translation models with data. One is to transform additional monolingual translation into parallel data by syn- the missing half of the and the other is to integrate a language model as a component into the neural network architecture. Back Translation Language models improve of the output. Using larger amounts of monolingual data in the target language give the machine more evidence what are common sequences of words and what are not. We cannot use monolingual target side data in our neural translation model since it is missing the source side. one idea is to just synthesize this data by back translation . See Figure 13.31 for an illustration of the steps involved. Train a reverse system that translates from the intended target language into the source language. We typically use the same neural machine translation setup for this as for our just with source and target But we may use any even traditional phrase-based systems. 64 CHAPTER 13. NEURAL MACHINE TRANSLATION reverse system system Figure Creating synthetic parallel data from target-side monolingual train a system in reverse use it to translate target-side monolingual data into the source combine the generated synthetic parallel data with the true parallel data in system building. Use the reverse system to translate target side monolingual creating a synthetic parallel corpus . Combine the generated synthetic parallel data with the true parallel data when building the system. There is an open question on how much synthetic parallel data should be used in relation to the amount of existing true parallel data. there are magnitudes more monolingual data but we also do not want to drown out the actual real data. Successful of this idea used equal amounts of synthetic and true data. We may also generate much more synthetic parallel but then ensure during training that we process equal amounts of each by over-sampling the true parallel data. Adding a Language Model The other idea is to train a language model as a separate component of the neural translation model. train the large language model as a recurrent neural net- work on all available including the target side of the parallel corpus. they add this language model to the neural translation model. Since both language model and translation model predict output the natural point to connect the two models is joining them at that output prediction node in the network by concatenating their conditioning contexts. We expand Equation 13.75 to add the hidden state of the neural language model s i to the hidden state of the neural translation model s i TM the source context c i and the previous English word e i 1 . e i g c i s i TM s i e i 1 When training the combined we leave the parameters of the large neural language model and update only the parameters of the translation model and the layer. The concern is that otherwise the output side of the parallel corpus would overwrite 13.6. REFINEMENTS 65 MT f e f e MT f e Figure Round trip In addition to training two models f e and e f as done traditionally on parallel we also optimize both models to convert a sentence f into e and then restore it back into f using on monolingual data in f . We may add a corresponding round trip starting with e . the memory of the large monolingual corpus. In other the language model would over- to the parallel training data and be less general. One question How much weight should be given to the translation model and how much weight should be given to the language The above equation considers them in all instances the same way. But there may be output words for which the translation model is more relevant the translation of content words with distinct and output words where the language model is more relevant the introduction of relevant function words for The balance of the translation model and the language model can be achieved with the type of gated units that we encountered in our discussion of the long short-term memory neural network architecture Such a gated unit may be predicted solely from the model state s i and then used as a factor that is multiplied with that language model state before it is used in the prediction of Equation 13.82. gate i f s i s i gate i s i e i g c i s i TM s i e i 1 Round Trip Training Looking at the idea from a strict machine learning we can see two learning objectives. There is the objective to learn the transformations given by the parallel as done traditionally. there is the goal to learn how to convert an output sentence into the input and then back into the output language with the objective to match the traditional sentence. A good machine translation model should be able to preserve the meaning of the output language sentence when mapped into the input language and back. See Figure 13.32 for an illustration. There are two machine translation models. One that translates sentences in the language direction f e the other in the opposite direction e f . These two systems may be trained with traditional using a parallel corpus. We can also round trip a sentence f through the f e and then back through e f In this there are two objectives for model training. 66 CHAPTER 13. NEURAL MACHINE TRANSLATION The translation of the given monolingual sentence f should be a valid sentence in the language e as measured with a language model e . The reconstruction of the translation back into the original language f should be as measured with the translation model MT e f f These two objectives can be used to update model parameters in both translation models MT f e and MT e f . Typical model update is driven by correct predictions of each word. In this round-trip the translation has to be computed before we can do the usual training of model MT e f with the given sentence pair f To make better use of the training a n-best list of translations 1 n is computed and model updates are computed for each of them. We can also update the model MT f e with monolingual data in language f by scaling up- dates by the language model cost e i and the forward translation cost MT f e i f for each of the translations i in the n-best list. To use monolingual data in language e training is done in the reverse round trip direction. For details of this refer to Further Readings back-translate the monolingual data into the input and use the obtained synthetic parallel corpus as additional training data. use monolingual data in a dual learning setup. Machine translation engines are trained in both and in addition to regular model training from parallel monolingual data is translated in a round trip e to f to e and evaluated with a language model for language f and reconstruction match back to e as cost function to drive gradient descent updates to the model. 13.6.4 Deep Models Learning the lessons from other research such as vision or speech recent work in machine translation has also looked at deeper models. Simply this involves adding more intermediate layers into the baseline architecture. The core components of neural machine translation are the encoder that takes input words and converts them into a sequence of contextualized representations and the decoder that gen- a output sequence of words. Both are recurrent neural networks. Recall that we already discussed how to build deeper recurrent neural networks for back to Section 13.4.7 on page We now extend these ideas to the recurrent neural networks in the encoder and the decoder. What all these recurrent neural networks have in common is that they process an input sequence into an output and at each time step t information from a new input x t is combined with the hidden state from the previous time step h t 1 to predict a new hidden state h t . From that hidden state additional predictions may be made words y t in the case of the the next word in the sequence in the case of language or the hidden state is used otherwise the attention mechanism in case of the 13.6. REFINEMENTS 67 Context Decoder Stack Transition 1 Decoder Stack Transition 2 Decoder Stack Transition 1 Decoder Stack Transition 2 Figure Deep Instead of a single recurrent neural network layer for the decoder in a deep it consists of several layers. The illustrations shows a combination of a deep transition and stacked It omits the word word selection and output word embedding steps which are identical to the original shown in Figure 13.20 on page 50. Decoder See Figure 13.33 for part of the decoder in neural machine using a par- deeper architecture. We see that instead of a single hidden state h t for a given time step t we now have a sequence of hidden states h 1 h 2 h for a given time step t . There are various options how the hidden states may be connected. in Sec- 13.4.7 we presented two ideas. In stacked recurrent neural networks where a hidden state h is conditioned on the hidden state from a previous layer h 1 and the hidden state at the same depth from a previous time step h t 1 . In deep transition recurrent neural net- the hidden state h 1 is conditioned on the last hidden state from the previous time step h t 1 and the while the other hidden layers h i 1 are just conditioned on the previous previous layer h 1 . Figure 13.33 combines these two ideas. some layers are both stacked on the previous time step h t 1 and previous layer h 1 while others are deep transitions only on the previous layer h 1 . we can break this out into the stacked layers h h 1 f 1 x t h t 1 1 h f i h 1 h t 1 for i 1 and the deep transition layers v . v 1 g 1 in h t 1 in is either x t or h 1 v g v 1 for j 1 h v 68 CHAPTER 13. NEURAL MACHINE TRANSLATION Input Word Embedding Encoder Layer Encoder Layer Encoder Layer Encoder Layer Figure Deep alternating combination of the idea of a bidirectional recurrent neural network previously proposed for neural machine translation Figure 13.19 on page and the stacked recurrent neural network Figure 13.19 on page This architecture may be further extended with the idea of deep as shown for the decoder Figure The function f i h 1 h t 1 is computed as a sequence of function calls g . Each of the functions g may be implemented as feed-forward neural network layer multiplication plus activation long-short term memory cell or gated recurrent unit On either each function g has its own set of trainable model parameters. Encoder Deep recurrent neural networks for the encoder may draw in the same ideas as the with one in the baseline neural translation we used bidirectional recurrent neural networks to condition on both left and right context. We want to do the same for any deep version of the encoder. Figure 13.34 shows one idea how this could be called alternating recurrent neural network . It looks basically like a stacked recurrent neural with one the hidden states at each layer h are alternately conditioned on the hidden state from the previous time step h t 1 or the next time step h t . we formulate this as even numbered hidden states h 2 i being conditioned on the left context h t 1 2 i and odd numbered hidden states h 2 i conditioned on the right context h t 2 i . h 1 f x t h t 1 1 h 2 i f h 2 i 1 h t 1 2 i h 2 i f h 2 i h t 2 i As before in the we can extend this idea by having deep transitions. Note that deep models are typically augmented with direct connections from the input to the output. In the case of the this may mean a direct connection from the embedding to the encoder or connections at each layer that pass the input directly to the output. 13.6. REFINEMENTS 69 die 56 16 89 72 26 Obama 96 79 98 42 11 . 11 14 38 22 54 10 98 84 23 49 Figure Alignment vs. In this alignment points from traditional word align- methods are shown as and attention states as shaded boxes depending on the alignment value as They generally match up but note for instance that the prediction of the output auxiliary verb pays attention to the entire verb group have been strained . Such residual connections help with training. In early the deep architecture can be skipped. Only when a basic functioning model has been the deep architecture can be exploited to enrich it. We typically see the of residual connections in early training stages initial reduction of model and less so as improvement in the converged model. Further Readings Recent work has shown good results with 4 stacks and 2 deep transitions each for encoder and as well as alternating networks for the encoder There are a large number of variations the use of skip the choice of vs. number of layers of any that still need to be explored empirical for various data conditions. 13.6.5 Guided Alignment Training The attention mechanism in neural machine translation models is motivated by the need to align output words to input words. Figure 13.35 shows an example of attention weights given to English input words for each German output word during the translation of a sentence. The attention values typically match up pretty well with word alignment used in statistical machine obtained with tools such as or fast-align which implement variants of the IBM Models. There are several good uses for word alignments beyond their intrinsic value of improving the quality of translations. For instance in the next we will look at using the attention 70 CHAPTER 13. NEURAL MACHINE TRANSLATION mechanism to explicitly track coverage of the input. We may also want to override preferences of the neural machine translation model with translations of certain terminology or expressions such as or measurements that are better handled by rule-based this requires to know when the neural model is about to translate a source word. But also the end user may be interested in alignment such as translators using machine translation in a computer aided translation tool may want to check where an output word originates from. instead of trusting the attention mechanism to implicitly acquire the role as word we may enforce this role. The idea is to provide not just the parallel corpus as train- but also word alignments using traditional means. Such additional information may even training of models to converge faster or overcome data sparsity under low resource conditions. A straightforward way to add such given word alignment to the training process is to not change the model at but to just modify the training objective. the goal of training neural machine translation models is to generate the correct output words. We can add to this goal to also match the given word alignment. we assume to have access to an alignment matrix A that alignment points A input words j and output words i in a way that j A 1 each output words alignment scores add up to 1. The model estimates attention scores that also add up to 1 for each output j 1 Equation 13.79 on page The mismatch between given alignment scores A and computed attention scores can be measured in several such as cross entropy cost CE 1 I I J i j A log or mean squared error cost 1 I I J i j A 2 This cost is added to the training objective and may be weighted. Further Readings Chen add supervised word alignment information with traditional statistical word alignment to training. They augment the objective function to also optimize matching of the attention mechanism to the given alignments. 13.6.6 Modeling Coverage One impressive aspect of neural machine translation models is how well they are able to trans- late the entire input even when a lot of reordering is involved. But this as aspect is not occasionally the model translates some input words multiple and sometimes it misses to translate them. 13.6. REFINEMENTS 71 um 37 33 13 Problem 84 10 80 63 81 12 40 30 80 . 71 18 86 10 84 80 45 41 40 44 10 89 12 40 10 10 37 11 13 43 7 46 161 108 89 62 112 392 121 110 130 26 132 22 19 6 6 Figure Example for over-generation and the input tokens around Social Housing are attended too leading to hallucinated output words the company while the end of the sentence a fresh start is not attended and untranslated. See Figure 13.36 for an example. The translation has two related to of attention. The beginning of the phrase alliance receives too much resulting in a faulty translation with hallucinated or the company of the society for social education . At the end of the input the phrase a fresh start does not receive any attention and is hence untranslated in the output. an obvious idea is to more strictly model coverage . Given the attention a reasonable way to coverage is by adding up the attention states. In a complete sentence we roughly expect that each input word receives a similar amount of attention. If some input words never receive attention or too much attention that signals a problem with the translation. Enforcing Coverage during Inference We may restrict the enforcing of proper coverage to the decoder. When considering multiple hypothesis in beam then we should age the ones that pay too much attention to some input words. once hypotheses are we can penalize those that paid only little attention to some of the input. There are various ways to come up with scoring functions for over-generation and under-generation. 72 CHAPTER 13. NEURAL MACHINE TRANSLATION coverage j i k over-generation max 0 under-generation min 1 j coverage j 1 coverage j j The use of multiple scoring functions in the decoder is common practice in traditional machine translation. For it is not in neural machine translation. A challenge is to give proper weight to the different scoring functions. If there are only two or three these can be optimized with grid search over possible values. For more we may borrow methods such as or MIRA from statistical machine translation. Coverage Models The vector that accumulates coverage of input words may be directly used to inform the attention model. the attention given to a input word j was conditioned on the previous state of the decoder s i 1 and the representation of the input word h j . we also add as conditioning context the accumulated attention given to the word to Equation 13.78 on page a s i 1 h j W a s i 1 U a h j V a coverage j b a Coverage tracking may also integrated into the training objective. Taking a page from the guided alignment training the previous Section we augment the training function with a coverage penalty with some weight . log P y i x coverage j 2 i j Note that in it is problematic to add such additional functions to the learning ob- since it does distract from the main goal of producing good translations. Fertility So we described coverage as the need to cover all input words roughly evenly. even the earliest statistical machine translation models considered the fertility of the number of output words that are generated from each input word. Consider the English do not most other language do not require an equivalent of do when negating a verb. other words are translated into multiple output words. For in- the German may be translated as of course thus generating 2 output words. We may augment models of coverage by adding a fertility components that predicts the number of output words for each input words. Here one example for a model that predicts the fertility j for each input and uses it to normalize the coverage statistics. j N W j h j 1 j i k coverage j 13.6. REFINEMENTS 73 Fertility j is predicted with a neural network layer that is conditioned on the input word representation h j and uses a activation function resulting in values from 0 to which is scaled to a maximum fertility of N . Feature Engineering versus Machine Learning The work on modeling coverage in neural machine translation models is a nice example to contrast between the engineering approach and the belief in generic machine learning techniques. From an engineering a good way to improve a system is to analyze its weak points and consider changes to overcome them. we notice over-generation and under-generation with respect to the and add components to the model to overcome this problem. On the other proper coverage is one of the features of a good translation that machine learning should be able to get from the training data. If it is not able to do it may need deeper more robust estimation ways to or or other adjustments to give it just the right amount of power needed for the problem. It is hard to carry out the analysis needed to make generic machine learning given the complexity of a task like machine translation. the argument for deep learning is that it does not require feature such as adding coverage models. It remains to be seen how neural machine translation evolves over the next and if it moves more into a engineering or machine learning direction. Further Readings To better model Tu add coverage states for each input word by either summing up attention scaled by a fertility value predicted from the input word in or learning a coverage update function as a feed-forward neural network layer. This coverage state is added as additional conditioning context for the prediction of the attention state. condition the prediction of the attention state also on the previous context state and also introduce a coverage state with the sum of input source that aims to subtract covered words at each step. separate hidden states that keep track of source coverage and hidden states that keep track of produced output. add a number of biases to model and alignment inspired by traditional statistical machine translation models. They condition the prediction of the attention state on absolute word the attention state of the previous output word in a limited and coverage attention state over a limited window. They also add a fertility model and add coverage in the training objective. 13.6.7 Adaptation Text may differ by degree of and so on. A common problem in the practical development of machine translation systems is that most of the available training data is different from the data relevant to a chosen use case. For if your goal is to translate chat room you will realize that there is very little translated chat room data available. There are massive quantities of publications from international random translations crawled from the and maybe somewhat relevant movie subtitle translations. 74 CHAPTER 13. NEURAL MACHINE TRANSLATION general training data initial training general system in-domain training data adaptation adapted system Figure Online training of neural machine translation models allows a straightforward domain adaptation Having a general domain translation system trained on general-purpose a handful of additional training epochs on in-domain data allows for a domain-adapted system. This problem is generally framed as a problem of domain adaptation . In the simplest you have one set of data relevant to your use case the in-domain data and another set that is less relevant the out-of-domain data . In traditional statistical machine a vast number of methods for domain have been proposed. Models may be we may back-off from in-domain to out-of-domain we may over-sample in-domain data during training or sub-sample out-of-domain etc. For neural machine a fairly straightforward method is currently the most pop- Figure This method divides training up into two stages. we train the model on all available data until convergence. we run a few more iterations of training on the in-domain data only and stop training when performance on the in-domain validate set peaks. This the model from all the training but is still specialized to the in-domain data. Practical experience with this method shows that the second in-domain training stage may converge very quickly. The amount if in-domain data is typically relatively and only a handful of training epochs are needed. less commonly used method draws on the idea of ensemble decoding If we train separate models on different sets of we may combine their just as we did for ensemble decoding. In this we do want to choose weights for each although how to choose these weights is not a trivial task. If there is just an in-domain and out-of-domain this may be simply done by line search over possible values. Let us now look at a few special cases that arise in practical use. in-domain data from large collections A common problem is that the amount of available in-domain data is very so just training on this even in a secondary adaptation risks very good performance on the seen data but poor on everything else. 13.6. REFINEMENTS 75 Large random collections of parallel text often contain data that closely matches the in- domain data. we may want to extract this in-domain data from the large collections of mainly out-of-domain data. The general idea behind a variety of methods is to build two one in-domain detector trained on in-domain and one out-of-domain detector trained on out-of-domain data. We then score each sentence pair in the out-of-domain data with both detectors and select sentence pairs that are preferred judged relatively by the in-domain detector. The classic detectors are language models trained on the source and target side of the in- domain and out-of-domain resulting in a total of 4 language the source side in-domain model in f the target side in-domain model in e the source side out-of-domain model out f and the target side out-of-domain model out e . Any given sentence pair from the out-of-domain data is then scored based on these relevance in e e out e e in f f out f f We may use traditional n-gram language models or neural recurrent language models. Some work suggests to replace open class words with part-of-speech tags or word clusters. More sophisticated models not only consider domain- relevance but noisiness of the training data misaligned or mistranslated We may even use in-domain and out-of-domain neural translation models to score sentence pairs in- stead of source and target side sentences in isolation. The data may be used in several ways. We may only train on this data to build our system. we use it in a secondary adaptation stage as outlined above. Only monolingual in-domain data What if we have no parallel data in the domain of our use Two main ideas have been explored. we may still use the monolingual may it be in the source or target language or for parallel data from a large pile of general as outline above. Another idea is to use existing parallel data to train an out-of-domain then back- translate out-of-domain data Section to generate a synthetic in-domain and then use this data to adapt the initial model. In traditional statistical machine much adaptation success has been achieved with just interpolating the language and this idea is the neural translation equivalent to that. Multiple domains we have multiple collections of data that are clearly by domain typically categories such as information etc. We can use the techniques described above to build specialized translation models for each of these domains. For a given test we then select the appropriate model. If we do not know the domain of the test we have to build a that allows us to automatically 76 CHAPTER 13. NEURAL MACHINE TRANSLATION make this determination. The may be based on the methods for domain detectors described above. Given the decision of the we then select the most appropriate model. But we do not have to commit to a single domain. The may instead provide a distribution of relevance of the domain models domain domain domain which are then used as weights in an ensemble of models. The domain may done based on a whole document instead of each individual which brings in more context to make a more robust decision. As a it is hard to give conclusive advice on how to handle adaptation since it is such a broad topic. The style of the text may be more relevant than its content. Data may differ narrowly publications from the United Nations vs. an- from the European or dramatically chat room vs. published The amounts of in-domain and out-of-domain data differs. The data may be cleanly separated by domain or just come in a massive disorganized pile. Some of the data may be of higher translation quality than which may be polluted by noise such as or even generated by some other machine translation system. Further Readings There is often a domain mismatch between the bulk even of the training data for a translation and its test data during deployment. There is rich literature in traditional statistical machine translation on this topic. A common approach for neural models is to train on all available training and then run a few iterations on in-domain data only and as already pioneered in neural language model adaption demonstrate the effectiveness of this adaptation method with small in-domain sets consisting of as little as 500 sentence pairs. argue that given small amount of in-domain data leads to and suggest to mix in-domain and out-of-domain data during adaption. and identify the same problem and suggest to use an ensemble of baseline models and adapted models to avoid consider alternative training methods for the adaptation phase but do not consistently better results than the traditional gradient descent training. Inspired by domain adaptation work in statistical machine translation on sub-sampling and sentence Chen build an in-domain vs. out-of-domain for sentence pairs in the training and then use its prediction score to reduce the learning rate for sentence pairs that are out of domain. show that traditional statistical machine translation outperforms neural ma- chine translation when training general-purpose machine translation systems on a collection and then tested on niche domains. The adaptation technique allows neural machine translation to catch up. A multi-domain model may be trained and informed at run-time about the domain of the input sentence. apply an idea initially proposed by to augment input sentences for register with a politeness feature token to the domain adaptation problem. They add a domain token to each training and test sentence. Chen report better results over the token approach to adapt to topics by encoding the given topic membership of each sentence as an additional input vector to the conditioning context of word prediction layer. 13.6. REFINEMENTS 77 13.6.8 Adding Linguistic Annotation One of the big debates in machine translation research is the question if the key to progress is to develop relatively machine learning methods that implicitly learn the important features of or to use linguistic insight to augment data and models. Recent work in statistical machine translation has demonstrated the of motivated models. The best statistical machine translation systems in major evaluation campaigns for language pairs such as and are syntax-based. While they translate they also build up the syntactic structure of the output sentence. There have been serious efforts to move towards deeper semantics in machine translation. The turn towards neural machine translation was at hard swing back towards better machine learning while ignoring much linguistic insights. Neural machine translation views translation as a generic sequence to sequence which just happens to involve sequences of words in different languages. Methods such as byte pair encoding or character-based models even put the value of the concept of a word as a basic unit into doubt. recently there have been also attempts to add linguistic annotation into neural translation and steps towards more linguistically motivated models. We will take a look at successful efforts to integrate linguistic annotation to the input linguistic annotation to the output and build linguistically structured models. Linguistic annotation of the input One of the great of neural networks is their to cope with rich context. In the neural machine translation models we each word prediction is conditioned on the entire input sentence and all previously generated out- put words. Even as it is typically the a input sequence and partially generated output sequence has never been observed before during the neural model is able to generalize the training data and draw from relevant knowledge. In traditional statistical mod- this required carefully chosen independence assumptions and back-off schemes. adding more information to the conditioning context in neural translation models can be accommodated rather straightforwardly. what information would be like to The typical linguistic treasure chest contains part-of-speech morphological properties of syntactic phrase syntactic and maybe even some semantic annotation. All of these can be formatted as annotations to individual input words. this requires a bit more such as syntactic and semantic annotation that spans multiple words. See Figure 13.38 for an example. To just walk through the linguistic annotation of the word girl in the Part of speech is a noun. Lemma is girl the same as the surface form. The lemma differs for watched watch . Morphology is singular. 78 CHAPTER 13. NEURAL MACHINE TRANSLATION Words the girl watched attentively the beautiful Part of speech ADV Lemma the girl watch attentive the beautiful Morphology SING . PAST PLURAL Noun phrase BEGIN CONT OTHER OTHER BEGIN CONT CONT Verb phrase OTHER OTHER BEGIN CONT CONT CONT CONT dependency girl watched watched watched Depend. relation SUBJ ADV ADJ OBJ Semantic role ACTOR MANNER MOD PATIENT Semantic type HUMAN VIEW ANIMATE Figure Linguistic annotation of a formatted as word-level factored representation The word is the continuation CONT of the noun phrase that started with the . The word is not part of a verb phrase OTHER Its syntactic head is watched . The dependency relationship to the head is subject SUBJ Its semantic role is ACTOR . There are many schemes of semantic types. For instance girl could be as MAN . Note how phrasal annotations are handled. The noun phrase is the girl . It is common to use an annotation scheme that tags individual words in a phrasal as BEGIN and CONTINUATION INTERMEDIATE while words outside such phrases as OTHER . How do we encode the word-level factored Recall that words are initially represented as 1-hot vectors. We can encode each factor in the factored representation as a 1- hot vector. The concatenation of these vectors is then used as input to the word embedding. Note that mathematically this that each factor of the representation is mapped to a embedding and the word embedding is the sum of the factor Since the input to the neural machine translation system is still a sequence of word embed- we do not have to change anything in the architecture of the neural machine translation model. We just provide richer input representations and hope that the model is able to learn how to take advantage of it. Coming back to the debate about linguistics versus machine learning. All the linguistic an- notation proposed here can arguable be learned automatically as part of the word contextualized word in the hidden encoder This may or may not be true. But it does provide additional knowledge that comes from the tools that produce the an- notation and that is particularly relevant if there is not enough training data to automatically induce it. why make the job harder for the machine learning algorithm than In other why force the machine learning to discover features that can be readily 13.6. REFINEMENTS 79 Sentence the girl watched attentively the beautiful Syntax tree S NP VP the girl watched ADV NP attentively the beautiful S NP the girl VP watched ADV attentively NP the beautiful Figure of phrase structure grammar tree into a sequence of words watched and tags S NP these questions will be resolved empirically by demonstrating what actually works in data conditions. Linguistic annotation of the output What we have done for input words could be done also for output words. Instead of discussing the points about what adjustments need to made separate for each output let us take a look at another annotation scheme for the output that has been successfully applied to neural machine translation. Most syntax-based statistical machine translation models have focused on adding syntax to the output side. Traditional n-gram language models are good at promoting among neighboring they are not powerful enough to ensure overall of each output sentence. By designing models that also produce and evaluate the syntactic parse for each output syntax-based models give the means to promote grammatically correct output. The word-level annotation of phrase structure syntax suggested in Figure 13.38 is rather crude. The nature of language is and annotating nested phrases cannot be easily handled with a BEGIN CONT OTHER scheme. typically tree structures are used to represent syntax. See Figure 13.39 for an example. It shows the phrase structure syntactic parse tree for our example sentence The girl watched attentively the beautiful . Generating a tree structures is generally a quite different process than generating a sequence. It is typically built recursively bottom-up with algorithms such as chart parsing. we can the parse tree into a sequence of words and structural tokens that indicate the beginning NP and end closing parenthesis of syntactic phrases. forcing syntactic parse tree annotations into our sequence-to-sequence neural ma- chine translation model may be done by encoding the parse structure with additional output tokens. To be perfectly the idea is to produce as the output of the neural translation not just a sequence of but a sequence of a mix of output words and special tokens. 80 CHAPTER 13. NEURAL MACHINE TRANSLATION The hope is that forcing the neural machine translation model to produce syntactic structure in a encourages it to produce syntactically well-formed output. There is some evidence to support this despite the simplicity of the approach. Linguistically structured models The of syntactic parsing has not been left untouched by the recent wave of neural networks. The previous section suggests that syntactic parsing may be done as simply as framing it as a sequence to sequence with additional output tokens. the best-performing syntactic use model structures that take the recur- nature of language to heart. They are either inspired by networks and build parse trees or are neural versions of left-to-right push-down that main- a stack of opened phrases that any new word may extend or or be pushed down the stack to start a new phrase. There is some early work on integrating syntactic parsing and machine translation into a framework but no consensus on best practices has emerged yet. At the time of this is clearly still a challenge for future work. Further Readings Wu propose to use factored representations of words and part of with each factor encoded in a one-hot in the input to a recurrent neural network language model. and use such representations in the input and output of neural machine translation demonstrating better translation quality. 13.6.9 Multiple Language Pairs There are more than two languages in the world. And we also have training data for many language sometimes it is highly overlapping European Parliament proceedings in 24 sometimes it is unique Canadian in French and For some language a lot of training data is available But for most language there is only very including commercially interesting language pairs such as or There is a long history of moving beyond languages and encode meaning language- sometimes called In machine the idea is to map the input language into an and then map the into the output language. In such a we have to build just one mapping step into and one step out of the for each language. Then we can translate between it and all the other languages for which we have done the same. Researchers in deep learning often do not hesitate to claim that intermediate states in neural translation models encode semantics or meaning. can we train a neural machine translation system that accepts text in any language as input and translates it into any other 13.6. REFINEMENTS 81 Multiple Input Languages Let us we have two parallel one for and one for We can train a neural machine translation model on both corpora at the same time by simply concatenating them. The input vocabulary contains both German and French words. Any input sentence will be quickly recognized as being either German or due to the sentence words such as you in of in The combined model trained on both data sets has one advantage over two separate mod- It is exposed to both English sides of the parallel corpora and hence can learn a better language model. There may be also be general to having diversity in the leading to more robust models. Multiple Output Languages We can do the same trick for the output by a and a corpus. But given a French input sentence during how would the system know which output language to A crude but effective way to signal this to the model is by adding a tag like SPANISH as token of the input sentence. ENGLISH pas Is this not a case of double SPANISH pas verse con If we train a system on the three corpora mentioned and we can also use it translate a sentence from German to Spanish without having ever presented a sentence pair as training data to the system. SPANISH verse con For this to there has to be some representation of the meaning of the input sentence that is not tied to the input language and the output language. experiments show that this actually does somewhat. To achieve good some parallel data in the desired language pair is but much less than for a standalone model Figure 13.40 summarizes this idea. A single neural machine translation is trained on parallel corpora in resulting in a system that may translate between any seen input and output language. It is likely that increasingly deeper models Section may better serve as multi-language since their deeper layer compute more abstract rep- of language. The idea of marking the output language with a token such as SPANISH has been explored more widely in the context of systems for a single language pair. Such tokens may represent the domain of the input sentence or the required level of politeness of the output sentence 82 CHAPTER 13. NEURAL MACHINE TRANSLATION French German MT English Spanish Figure Multi-language machine translation system trained on one language pair at a rotating through many of them. After training on and it is even able to translate from German to Spanish. Sharing Components Instead of just throwing data at a generic neural machine translation we may want to more carefully consider which components may be shared among models. The idea is to train one model per language but some of the components are identical in these unique models. The encoder may be shared in models that have the same input language. The decoder may be shared in models that have the same output language. The attention mechanism may be shared in all models for all language pairs. Sharing components means is that the same parameter values are used in these separate models. Updates to them when training a model for one language pair then also changes them for in the model for the other language pairs. There is no need to mark the output since each model is trained for a language pair. The idea of shared training of components can also be pushed further to exploit mono- lingual data. The encoder may be trained on monolingual input language but we will need to add a training objective language model the decoder may be trained in isolation with monolingual language model data. since there are no context states these have to be blanked which may lead it to learn to ignore the input sentence and function only as a target side language model. Further Readings Johnson explore how well a single canonical neural translation model is able to learn from multiple to multiple by simultaneously training on on parallel corpora for several language pairs. They show small for several input languages with the same output mixed results for translating into multiple output languages by an additional input language The most interesting result is the ability for such a model to translate in language directions for which no parallel corpus is thus demonstrating that some meaning representation is although less well than using traditional pivot methods. support multi-language input and output by training encoders and decoders and a shared attention mechanism. 13.7. ALTERNATE ARCHITECTURES 83 Input Word K 2 Layer K 3 Layer L 3 Layer Figure Encoding a sentence with a neural network. By always using two the size of the convolutions differ K 2 and K 3 Decoding reverses this process. 13.7 Alternate Architectures Most of neural network research has focused on the use of recurrent neural networks with attention. But this is by no means the only architecture for neural networks. a vantage of using recurrent neural networks on the input side is that it requires a long sequential process that consumes each input word in one step. This also prohibits the ability to the processing of all words at thus limiting the use of the capabilities of There have been a few alternate suggestions for the architecture of neural machine models. We will present some of them in this section. It remains to be if they are a curiosity or conquer the 13.7.1 Neural Networks The end-to-end neural machine translation model of the modern era and was actually not based on recurrent neural but based on neural networks . These had been shown to be very successful in image thus looking for other applications was a natural next step. See Figure 13.41 for an illustration of a network that encodes an input sen- The basic building block of these networks is a convolution. It merges the representation of i input words into a single representation by using a matrix K i . Applying the convolution to every sequence of input words reduces the length of the sentence representation by i 1 . Repeating this process leads to a sentence representation in a single vector. The illustration shows an architecture with two K i followed by a L i layer that merges the sequence of phrasal representations into a single sentence The size of the kernels K i and L i depends on the length of the sentences. The example shows a 6-word sentence and a sequence of K 2 K 3 and L 3 layers. For longer bigger kernels are needed. The hierarchical process of building up a sentence representation bottom-up is well grounded in linguistic insight in the recursive nature of language. It is similar to chart except that we are not committing to a single hierarchical structure. On the other we are asking an 84 CHAPTER 13. NEURAL MACHINE TRANSLATION Input Word K 2 Encoding Layer K 2 Encoding Layer Transfer Layer K 3 Decoding Layer K 2 Decoding Layer Selected Word Output Word Embedding Figure of the neural network model. Convolutions do not result in a single sentence embedding but a sequence. The encoder is also informed by a recurrent neural network from output word to decoding layer. awful lot from the resulting sentence embedding to represents the meaning of an entire sen- of arbitrary length. Generating the output sentence translation reverses the bottom-up process. One problem for the decoder is to decide the length of the output sentence. One option to address this problem is to add a model that predicts output length from input length. This then leads to the selection of the size of the reverse convolution matrices. See Figure 13.42 for an illustration of a variation of this idea. The shown architecture always uses a K 2 and a K 3 resulting in a sequence of phrasal not a single sentence embedding. There is an explicit mapping step from phrasal representations of input words to phrasal representations of output called transfer layer. The decoder of the model includes a recurrent neural network on the output side. Sneaking in a recurrent neural network here does undermine a bit the argument about better the claim still holds true for encoding the and a sequential language model is just a too powerful tool to disregard. While the just-described neural machine translation model helped to set the scene for neural network approaches for machine it could not be demonstrated to achieve competitive results compared to traditional approaches. The compression of the sen- representation into a single vector is especially a problem for long sentences. the model was used successfully in candidate translations generated by traditional statistical machine translation systems. 13.7. ALTERNATE ARCHITECTURES 85 0 0 Input Word 0 0 0 0 Convolution Layer 1 Convolution Layer 2 Convolution Layer 3 Figure Encoder using stacked layers. Any number of layers may be used. 13.7.2 Neural Networks With Attention propose an architecture for neural networks that combines the ideas of neural networks and the attention mechanism. It is essentially the sequence-to- sequence attention that we described as the canonical neural machine translation but with the recurrent neural networks replaced by layers. We introduced convolutions in the previous section. The idea is to combine a short sequence of neighboring words into a single representation. To look at it in another a convolution encodes a word with its left and right in a limited window. Let us now describe in more detail what this means for the encoder and the decoder in the neural model. Encoder See Figure 13.43 for an illustration of the layers used in the encoder. For each input the state at each layer is informed by the corresponding state in the layer and its two neighbors. Note that these layers do not shorten the because we have a convolution centered around each using padding with zero for word positions that are out of bounds. we start with the input word Ex j and progress through a sequence of layer h at different depth d until a maximum depth D . h 0 E x j h f h d 1 k h d 1 k for d 0 d D The function f is a feed-forward with a residual connection from the corresponding previous layer state h d 1 . Note that even with a few the representation of a word h may only be informed by partial sentence context in contrast to the bi-directional recurrent neural networks in the canonical model. relevant context words in the input sentence that help with disambiguation may be outside this window. On the other there are computational advantages to this idea. All words at one depth can be processed in even combined into one massive tensor operation that can be on a GPU. 86 CHAPTER 13. NEURAL MACHINE TRANSLATION Input Context Output Word Predictions Decoder Convolution 2 Decoder Convolution 1 0 Output Word Embedding Selected Word Figure Decoder in neural network with attention. The decoder state is computed as a sequence of layers over the already predicted output words. Each state is also informed by the input context computed from the input sentence and attention. Decoder The decoder in the canonical model also has at its core a recurrent neural network. Recall its state progression in Equation 13.75 on page s i f s i 1 i 1 c i where s i is the encoder i 1 the embedding of the previous output and c i the input context. The version of this does not have recurrent decoder the does not depend on the previous state s i 1 but is conditioned on the sequence of the most recent previous words. s i f i i 1 c i these decoder convolutions may be just as the encoder layers. s 1 f i i 1 c i s f s d 1 1 s d 1 c i for d 0 d D See Figure 13.44 for an illustration of these equations. The main difference between the canonical neural machine translation model and this architecture is the conditioning of the states of the decoder. They are computed in a sequence of and also always the input context. Attention The attention mechanism is essentially unchanged from the canonical neural trans- model. Recall that is is based on an association a s i 1 h j between the word computed by the encoder h j and the previous state of the decoder s i 1 back to Equation 13.78 on page 13.7. ALTERNATE ARCHITECTURES 87 Since we still have such encoder and decoder states h and s 1 we use the same here. These association scores are normalized and used to compute a weighted sum of the input word the encoder states h A is that the encoder state h and the input word embedding x j is combined via addition when computing the context vector. This is the usual trick of using residual connections to assist training with deep neural networks. 13.7.3 Self-Attention The critique of the use of recurrent neural networks is that they require a lengthy word by of the entire input which is time-consuming and limits The previous sections replaced the recurrent neural networks in our canonical model with con- these have a limited context window to enrich representations of words. What we would like is some architectural component that allows us to use wide context and can be highly What could that In we already encountered the attention mechanism. It considers associations be- tween every input word and any output and uses it to build a vector representation of the entire input sequence. The idea behind self-attention is to extend this idea to the encoder. Instead of computing the association between an input and an output self-attention com- the association between any input word and any other input word. One way to view it is that this mechanism the representation of each input word by enriching it with context words that help to disambiguate it. Computing Self-Attention self attention for a sequence of vectors h j size h packed into a matrix H as self-attention H T h H Let us look at this equation in detail. The association between every word representation h j any other context word h k is done via the dot product between the packed matrix H and its transpose H T resulting in a vector of raw association values T . The values in this vector are scaled by the size of the word representation vectors h and then by the so that their values add up to 1. The resulting vector of normalized association values is then used to weigh the context words. Another way to put Equation 13.98 without the matrix H notation but using word vectors h j a 1 h j h T h exp a exp a j normalized association self-attention h j j h k weighted sum k h k raw association T 88 CHAPTER 13. NEURAL MACHINE TRANSLATION Self-Attention Layer The self-attention step described above is only one step in the self- attention layer used to encode the input sentence. There are four more steps that follow it. We combine self-attention with residual connections that pass the word representation through directly self-attention h j h j Next up is a layer normalization step in Section 13.2.6 on page h j layer-normalization self-attention h j h j A standard feed-forward step with activation function is applied. W h j b This is also augmented with residual connections and layer normalization. layer-normalization W h j b h j Taking a page from deep we now stack several such layers D 6 on top of each other. h 0 Ex j start with input word embedding h self-attention-layer h d 1 for d 0 d D The deep modeling is the reason behind the residual connections in the self-attention layer such residual connections help with training since they allow a shortcut to the input which may be utilized in early stages of before it can take advantage of the more complex that deep models enable. The layer normalization step is one standard train- trick that also helps especially with deep models. Attention in the Decoder Self-attention is also used in the now between output words. The decoder also has more traditional attention. In total there are 3 sub layers. Self Output words are initially encoded by word s i i . We per- form exactly the same self-attention computation as described in Equation 13.98. How- the association of a word s i is limited to words s k with k i just the previously produced output words. Let us denote the result of this sub layer for output word i as s i The attention mechanism in this model follows very closely self-attention. The only difference is we compute self attention between the hidden states H and themselves. we compute attention between the decoder states S and the encoder states H . T attention H H h SH 13.7. ALTERNATE ARCHITECTURES 89 Input Word Self Attention Layer 1 Self Attention Layer 1 Decoder Layer 1 Decoder Layer 2 Output Word Prediction Selected Output Word Output Word Embedding Figure Attention-based machine translation the input is encoded with several layers of self-attention. The decoder computes attention-based representations of the input in several initialized with the previous word Using the same more detailed exposition as above for s i h h exp a attention s i j h k weighted sum k This attention computation is augmented by adding in residual layer nor- and an additional just like the self-attention layer described above. It is worth noting the output of the attention computation is a weighted sum over input word representations k j h k . To we add the representation of the decoder state s i via a residual connection. This allows skipping over the deep thus speeding up training. Feed-forward This sub layer is identical to the W s s i b s Each of the sub-layers is followed by the add-and-norm step of using residual and then layer normalization noted in the description of the attention sub The entire model is shown in Figure a 1 T h k raw association SH T exp a normalized association 90 CHAPTER 13. NEURAL MACHINE TRANSLATION Further Readings and build a comprehensive machine translation model by encoding the source sentence with a neural and then generate the target sentence by reversing the process. A of this was proposed by who use multiple layers in the encoder and the decoder that do not reduce the length of the encoded sequence but incorporate wider context with each layer. replace the recurrent neural networks used in sequence-to-sequence models with multiple self-attention both for the encoder as well as the decoder. There are a number of additional of this so-called multi-head encoding of sentence positions of etc. 13.8 Current Challenges Neural machine translation has emerged as the most promising machine translation approach in recent showing superior performance on public benchmarks and rapid adoption in deployments Google and But there have also been reports of poor such as the systems built under low-resource conditions in the LORELEI pro- gram. 8 we examine a number of challenges to neural machine translation and give empirical results on how well the technology currently holds compared to traditional statistical ma- chine translation. We show despite its recent neural machine translation still has to overcome various most notably performance out-of-domain and under low resource conditions. What a lot of the problems have in common is that the neural translation models do not show robust behavior when confronted with conditions that differ from training conditions may it be due to limited exposure to training unusual input in case of out- of-domain test or unlikely initial word choices in beam search. The solution to these problems may hence lie in a more general approach of training that steps outside optimizing single word predictions given perfectly matching prior sequences. Another challenge that we do not examine neural machine translation are much less The answer to the question of why the training data leads these systems to decide on word choices during decoding is buried in large matrices of real-numbered values. There is a clear need to develop better for neural machine translation. We use common for neural machine translation and traditional phrase- based statistical machine translation with common data drawn from and OPUS. Unless noted we use default such as beam search and single model decoding. The training data is processed with byte-pair encoding into to a 50,000 word vocabulary limit. 8 evaluations 13.8. CURRENT CHALLENGES 91 Our statistical machine translation systems are trained using Moses 9 We build phrase-based systems using standard features that are commonly used in recent submissions to Ding While we consider here only phrase-based we note that there are other statistical machine translation such as hierarchical phrase-based models and syntax-based models that have been shown to give superior performance for language pairs such as and We carry out our experiments on and For these large training data sets are available. We use from the shared translation task organized alongside the Conference on Machine Translation 10 . For the domain we use the OPUS corpus 11 Except for the domain we use the test sets composed of news which are characterized by a broad range of formal relatively long sentences 30 words on and high standards for and style. 13.8.1 Domain Mismatch A known challenge in translation is that in different 12 words have different trans- and meaning is expressed in different styles. a crucial step in developing ma- chine translation systems targeted at a use case is domain adaptation. We expect that methods for domain adaptation will be developed for neural machine translation. A currently popular approach is to train a general domain followed by training on in-domain data for a few epochs and and large amounts of training data are only available out of but we still seek to have robust performance. To test how well neural machine translation and statistical machine translation hold we trained different systems using different corpora obtained from OPUS An additional system was trained on all the training data. Statistics about corpus sizes are shown in Table 13.3. Note that these domains are quite distant from each much more so TED News and Global Voices. We trained both statistical machine translation and neural machine translation systems for all domains. All systems were trained for with tuning and test sets sub- sampled from the data were not used in A common byte-pair encoding is used for all training runs. See Figure 13.46 for results. While the in-domain neural and statistical machine translation systems are similar machine translation is better for IT and statistical machine translation is better for and the out-of-domain performance for the machine translation systems is worse in almost all sometimes dramatically so. For 9 10 11 12 We use the customary of domain in machine a domain is by a corpus from a and may differ from other domains in level of etc. 92 CHAPTER 13. NEURAL MACHINE TRANSLATION Corpus Words Sentences Law 18,128,173 715,372 25.3 Medical 14,301,472 1,104,752 12.9 IT 3,041,677 337,817 9.0 Koran 9,848,539 480,421 20.5 Subtitles 114,371,754 13,873,398 8.2 Table Corpora used to train taken from the OPUS repository. IT corpora are and System Law Medical IT Koran Subtitles All Data 30.5 32.8 45.1 42.2 35.3 44.7 17.9 17.9 26.4 20.8 Law 31.1 34.4 12.1 18.2 3.5 6.9 1.3 2.2 2.8 6.0 Medical 3.9 10.2 39.4 43.5 2.0 8.5 0.6 2.0 1.4 5.8 IT 1.9 3.7 6.5 5.3 42.1 39.8 1.8 1.6 3.9 4.7 Koran 0.4 1.8 0.0 2.1 0.0 2.3 15.9 18.8 1.0 5.5 Subtitles 7.0 9.9 9.3 17.8 9.2 13.6 9.0 8.4 25.9 22.1 Figure Quality of systems when trained on one domain and tested on another domain neural machine translation systems show more degraded performance out of domain. 13.8. CURRENT CHALLENGES 93 Source um Reference Look around you. All Look around you. Look around you. Law In order to implement . Medical MB 049 01-EN-Final Work for 2002 by around . IT Switches to paused. To by itself . Koran Take heed of your own souls. And you see. Subtitles Look around you. Look around you . Figure Examples for the translation of a sentence from the Subtitles when translated with systems trained on different corpora. Performance out-of-domain is dramatically worse for neural machine translation. instance the Medical system leads to a score of 3.9 machine vs. 10.2 machine on the Law test set. Figure 13.47 displays an example. When translating the sentence um Look around you. from the Subtitles we see mostly and completely unrelated output from the neural machine translation system. For the translation from the IT system is Switches to paused. Note that the output of the neural machine translation system is often quite Take heed of your own souls. but completely unrelated to the while the statistical machine translation output betrays its with coping with the out-of-domain input by leaving some words untranslated by around. This is of particular concern when MT is used for information the user will be mislead by hallucinated content in the neural machine translation output. 13.8.2 Amount of Training Data A well-known property of statistical systems is that increasing amounts of training data lead to better results. In statistical machine translation we have previously observed that doubling the amount of training data gives a increase in scores. This holds true for both parallel and monolingual data Irvine and How do the data needs of statistical machine translation and neural machine translation Neural machine translation promises both to generalize better word sim- in and condition on larger context input and all prior output 94 CHAPTER 13. NEURAL MACHINE TRANSLATION Scores with Varying Amounts of Training Data 30 20 21 . 8 16 . 4 23 . 4 18 . 1 24 . 9 19 . 6 26 . 2 26 . 9 27 . 9 23 . 5 21 . 2 22 . 2 22 . 4 18 . 2 28 . 6 29 . 2 29 . 6 30 . 1 30 . 4 27 . 4 29 . 2 25 . 7 24 . 7 30 . 3 31 . 1 28 . 6 14 . 7 11 . 9 10 7 . 2 1 . 6 Phrase-Based with Big Phrase-Based Neural 0 10 6 10 7 10 8 Corpus Size Figure scores for English-Spanish systems trained on 0.4 million to 385.7 million words of parallel data. Quality for neural machine translation starts much outperforms statistical machine translation at about 15 million and even beats a statistical machine translation system with a big 2 billion word in-domain language model under high-resource conditions. 26 . 1 26 . 9 27 . 8 13.8. CURRENT CHALLENGES 95 Ratio Words A Republican strategy to counter the re-election of Obama 1 1 1 1 1 1 Figure Translations of the sentence of the test set using neural machine translation system trained on varying amounts of training data. Under low resource neural machine translation produces output unrelated to the input. We built English-Spanish systems on 13 about 385.7 million English words paired 1 1 machine the language model was trained on the Spanish part of each respectively. In addition to a neural and statistical machine translation system trained on each we also used all additionally provided monolingual data for a big language model in statistical machine translation systems. Results are shown in Figure 13.48. Neural machine translation exhibits a much steeper 1 outperforming statistical machine translation 25.7 vs. 24.7 with 16 of the data million and even beating the statistical machine translation system with a big language model with the full data set for neural machine 28.4 for statistical machine 30.4 for statistical with a big language The contrast between the neural and statistical machine translation learning curves is quite striking. While neural machine translation is able to exploit increasing amounts of training data more it is unable to get off the ground with training corpus sizes of a few million words or less. 1 1 1 for strategy or for election and starting with 64 1 the translations become respectable. 13.8.3 Noisy Data Statistical machine translation is fairly robust to noisy data . The quality of systems holds up fairly even if large parts of the training data are corrupted in various such as aligned content in wrong badly translated etc. Statistical ma- chine translation models are built on probability distributions estimated from many occur- of words and phrases. Any unsystematic noise in the training only affects the tail end of the distribution. 13 Spanish was last represented in we used data from 96 CHAPTER 13. NEURAL MACHINE TRANSLATION Ratio 32.7 32.7 32.6 32.0 35.4 34.8 32.1 30.1 Table Impact of noise in the training with parts of the training corpus to contain sentence pairs. Neural machine translation degrades while statistical machine translation holds up fairly well. Is this still the case for neural machine Chen considered one kind of misaligned sentence pairs in an experiments with a large parallel corpus. They the target side of part of the training so that these sentence pairs are Table 13.4 shows the result. Statistical machine translation systems hold up fairly well. Even with of the data the quality only drops from 32.7 to 32.0 about what is to be expected with half the valid training data. the neural machine translation system degrades from 35.4 to 30.1 a drop of 5.3 com- pared to the 0.7 point drop for statistical systems. A possible explanation for this poor behavior of neural machine translation models is that its prediction has to a good balance between language model and input context as the main driver. When training observes increasing ratios of training for which the input sen- is a meaningless it may generally learn to rely more on the output language model hence hallucinating by inadequate output. 13.8.4 Word Alignment The key contribution of the attention model in neural machine translation was the imposition of an alignment of the output words to the input words. This takes the shape of a probability distribution over the input words which is used to weigh them in a bag-of-words representation of the input sentence. this attention model does not functionally play the role of a word alignment between the source in the at least not in the same way as its analog in statistical machine translation. While in both alignment is a latent variable that is used to obtain probability distributions over words or arguably the attention model has a broader role. For when translating a attention may also be paid to its subject and object since these may disambiguate it. To further complicate the word representations are products of bidirectional gated recurrent neural networks that have the effect that each word representation is informed by the entire sentence context. But there is a clear need for an alignment mechanism between source and target words. For prior work used the alignments provided by the attention model to interpolate word translation decisions with traditional probabilistic dictionaries for the introduction of coverage and fertility models etc. 13.8. CURRENT CHALLENGES 97 the 47 17 die 56 16 relationship 81 89 between 72 72 26 Obama 87 Obama 96 and 93 79 95 98 42 11 38 has been 38 16 21 14 26 54 . 11 14 22 54 10 98 84 23 49 stretched for years . 11 77 38 33 12 90 19 32 17 Desired Alignment Mismatched Alignment Figure Word alignment for comparing the attention model states boxes with probability in percent if over with alignments obtained from fast-align But is the attention model in fact the proper To examine we compare the soft alignment matrix sequence of attention with word alignments obtained by traditional word alignment methods. We use incremental fast-align to align the input and output of the neural machine system. See Figure for an illustration. We compare the word attention states with the word alignments obtained with fast align For most these match up pretty well. Both attention states and fast-align alignment points are a bit fuzzy around the function words . the attention model may settle on alignments that do not correspond with our in- tuition or alignment points obtained with fast-align. See Figure for the reverse language All the alignment points appear to be off by one position. We are not aware of any intuitive explanation for this divergent behavior the translation quality is high for both systems. We measure how well the soft alignment of the neural machine system match the alignments of fast-align with two a match score that checks for each output if the aligned input word according to fast-align is indeed the input word that received the highest attention and a probability mass score that sums up the probability mass given to each alignment point obtained from fast-align. 98 CHAPTER 13. NEURAL MACHINE TRANSLATION Language Pair Match Prob. Table Scores indicating overlap between attention probabilities and alignments obtained with fast- align. In these we have to handle byte pair encoding and many-to-many alignments 14 In out we use the neural machine translation models provided by Edinburgh 15 We run fast-align on the same parallel data sets to obtain alignment models and used them to align the input and output of the neural machine translation system. Table 13.5 shows alignment scores for the systems. The results suggest while the divergence for is an outlier. We that we have seen such large a divergence also under different data conditions. Note that the attention model may produce better word alignments by guided alignment training where supervised word alignments as the ones produced by are provided to model training. 13.8.5 Beam Search The task of decoding is to the full sentence translation with the highest probability. In machine this problem has been addressed with heuristic search techniques that explore a subset of the space of possible translation. A common feature of these search techniques is a beam size parameter that limits the number of partial translations maintained per input word. There is typically a straightforward relationship between this beam size parameter and the model score of resulting translations and also their quality score While there are diminishing returns for increasing the beam typically improvements in these scores can be expected with larger beams. Decoding in neural translation models can be set up in similar fashion. When predicting the next output we may not only commit to the highest scoring word prediction but 14 neural machine translation operates on but fast-align is run on full words. If an input word is split into by byte pair then we add their attention scores. If an output word is split into then we take the average of their attention vectors. The match scores and probability mass scores are computed as average over output word-level scores. If an output word has no fast-align alignment it is ignored in this computation. If an output word is fast-aligned to multiple input then for the match count it as correct if the n aligned words among the top n highest scoring words according to attention and for the probability mass add up their attention scores. 15 13.8. CURRENT CHALLENGES 99 English-Czech 31 30 29 . 7 29 8 30 30 . 4 30 29 . 8 29 . 4 Normalized 28 . 5 30 . 3 29 . 9 24 23 22 22 21 20 24 24 . 2 23 . 2 1 23 . 6 8 23 . 5 Normalized 22 . 7 23 . 6 19 . 9 23 . 2 1 2 4 8 12 20 30 50 100 200 500 1 000 Beam Size 1 2 4 8 12 20 30 50 100 200 500 1 000 Beam Size 36 35 . 7 35 . 7 37 . 3 5 7 . 537 . 3 6 7 . 637 . 6 37 . 6 37 . 6 37 . 6 37 . 6 37 . 2 9 36 . 6 36 . 6 36 . 4 36 . 1 29 28 28 28 . 929 29 . 2 1 9 . 129 . 2 29 . 2 29 . 2 28 . 4 28 . 4 28 . 4 28 . 1 27 . 6 29 . 1 28 . 7 35 34 . 6 27 26 . 8 26 . 7 Normalized 1 2 4 8 12 20 30 50 100 200 500 1 000 Beam Size Normalized 1 2 4 8 12 20 30 50 100 200 500 1 000 Beam Size 17 16 15 . 8 15 16 . 1 9 6 . 916 . 9 6 16 . 4 16 16 . 1 4 6 . 416 . 1 4 6 . 416 . 3 Normalized 17 . 3 16 . 2 16 15 . 9 15 . 6 15 . 3 26 25 24 7 Normalized 25 . 6 25 . 6 24 . 7 24 1 2 4 8 12 20 30 50 100 200 500 1 000 Beam Size 1 2 4 8 12 20 30 50 100 200 500 1 000 Beam Size 27 7 8 27 . 5 26 . 9 26 . 9 26 . 6 26 . 6 26 . 4 27 . 1 23 22 21 . 8 7 6 4 22 . 1 22 22 . 1 21 . 9 21 . 8 26 25 . 5 25 . 9 25 . 5 25 . 9 21 . 4 21 . 3 25 24 . 8 21 20 . 7 Normalized 24 . 1 20 19 . 9 Normalized 1 2 4 8 12 20 30 50 100 200 500 1 000 Beam Size 1 2 4 8 12 20 30 50 100 200 500 1 000 Beam Size Figure Translation quality with varying beam sizes. For large quality especially when not normalizing scores by sentence length. 30 . 30 . 930 . 3 9 0 . 930 . 9 30 . 9 30 . 7 30 . . 6 5 30 . 4 30 . 3 23 . 9 23 . 24 . 124 . 124 23 . 8 24 23 . 9 37 36 . 9 36 . 36 . 836 . 7 36 . 3 27 . 9 28 . 6 28 . 528 . 2 5 8 . 5 16 . . 6 5 16 . 16 . 7 25 . 4 25 25 6 25 . 825 . 2 7 8 5 . 825 . 8 25 . 8 25 . . 6 5 25 . 6 25 . . 25 . 625 . 25 . 625 . 7 25 . 6 25 . 6 27 . 27 . 827 . 27 . 727 . 7 27 . 6 22 . 22 . 5 22 . 3 22 . 4 22 . . 2 22 2 . . 4 2 22 . 4 22 . 3 100 CHAPTER 13. NEURAL MACHINE TRANSLATION also maintain the next best scoring words in a list of partial translations. We record with each partial translation the word translation probabilities from the extend each partial translation with subsequent word predictions and accumulate these scores. Since the number of partial translation explodes exponentially with each new output we prune them down to a beam of highest scoring partial translations. As in traditional statistical machine translation increasing the beam size allows us to explore a larger set of the space of possible translation and hence translations with better model scores. as Figure 13.51 increasing the beam size does not consistently improve translation quality. In in almost all worse translations are found beyond an optimal beam size setting are using again Edinburghs 2016 The optimal beam size varies from 4 to around 30 Normalizing sentence level model scores by length of the output alleviates the problem somewhat and also leads to better optimal quality in most cases of the 8 language pairs Optimal beam sizes are in the range of in almost all but quality still drops with larger beams. The main cause of deteriorating quality are shorter translations under wider beams. 13.8.6 Further Readings Other studies have looked at the comparable performance of neural and statistical machine translation systems. considered different linguistic categories for German and and compared different broad aspects such as and reordering for nine language directions. 13.9 Additional Topics Especially early work on neural networks for machine translation was aimed at building neural compo- to be used in traditional statistical machine translation systems. Translation Models By including aligned source words in the conditioning enrich a feed-forward neural network language model with source context add a sentence embedding to the conditional context of this which are learned using a variant of neural networks and mapping them across languages. use a more complex neural network to encode the input sentence that uses gated layers and also incorporates information about the output context. Reordering Models reordering models struggle with sparse data problems when con- on rich context. Li show that a neural reordering model can be conditioned on current and previous phrase pair with a recursive neural network to make the same decisions for orientation type. 13.9. ADDITIONAL TOPICS 101 Instead of handing reordering within the decoding we may the input sentence into output word order. use an input dependency tree to learn a model that swaps children nodes and implement it using a feed-forward neural network. and formulate a top-down left-to-right walk through the dependency tree and make reorder- decisions at any node. They model this process with a recurrent neural network that includes past decisions in the conditioning context. N-Gram Translation Models An alternative view of the phrase based translation model is to break up phrase translations into minimal translation and employing a n-gram model over these units to condition each minimal translation units on the previous ones. treat each minimal translation unit as an atomic symbol and train a neural language model over it. represent the minimal translation units as bag of break them even further into single input single output or single input-output word and and use phrase leaned with an auto-encoder. 102 CHAPTER 13. NEURAL MACHINE TRANSLATION Bibliography Philip Graham and 2016. Incorporating discrete translation icons into neural machine translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing . Association for Computational pages 1567. and 2015. Neural machine translation by jointly learning to align and translate. In . Paul and Phil 2015. Pragmatic neural language in machine translation. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Lin- Human Language Technologies . Association for Computational pages Paul Phil and 2014. A neural language framework for machine translation. The Prague Bulletin of Mathematical Linguistics Pascal and Christian 2003. A neural probabilistic language model. Journal of Machine Learning Research Luisa Mauro and Federico. 2016. Neural versus phrase- based machine translation a case study. In Proceedings of the 2016 Conference on Empirical Meth- in Natural Language Processing . Association for Computational pages r Christian Yvette Barry Matthias Antonio Ne- Mariana Martin Matt Raphael Carolina Scar- Lucia Marco Karin and Marcos 2016. Findings of the 2016 conference on machine translation. In Proceedings of the First Conference on Ma- chine Translation . Association for Computational pages M. Francisco and Enrique Vidal. 1997. Machine translation using neural networks and models. pages Boxing Colin George and Samuel 2017. Cost weighting for machine translation domain adaptation. In Proceedings of the First Workshop on Machine Translation . Association for Computational pages Boxing Roland George Colin and Huang. Bilingual methods for 103 104 Bibliography adaptive training data selection for machine translation. In Annual Meeting of the Association for Ma- chine Translation in the Americas . and Peter. Guided alignment training for topic-aware neural machine translation. David 2007. Hierarchical phrase-based translation. Computational Linguistics Bart van and 2014. On the properties of neural machine approaches. In Proceedings of Eighth Workshop on Semantics and Structure in Statistical Translation . Association for Computational pages and 2017. An empirical comparison of domain adaptation methods for neural machine translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics Short . Association for Computational pages Trevor Cong Chris and 2016. Incorporating structural alignment biases into an neural translation model. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Human Language Technologies . Association for Computational San Cal- pages Maria Anabel Kathy Jean Patrice Joshua Catherine Jean Mar- Alexandra Thomas Natalia Cyril and Peter 2016. pure neural machine translation systems. Gonzalo and Bill 2015. Fast and accurate for using neural networks. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Human Language Technologies . Association for Computational pages Jacob Thomas Richard and John 2014. Fast and robust neural network joint models for statistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics Long . Association for Computational pages Kevin and Matt Post. 2016. The machine translation systems for 2016. In Proceedings of the First Conference on Ma- chine Translation . Association for Computational pages John and Singer. 2011. Adaptive methods for online learning and stochastic optimization. Journal of Machine Learning Research Chris Victor and Noah A. Smith. 2013. A and effective of model 2. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Human Language Technologies . Association for Computational pages Bibliography 105 M. Marco Nicola and Federico. 2017. Neural vs. phrase-based machine translation in a multi-domain scenario. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Vol- Short Papers . Association for Computational pages Nan Mu Ming and Kenny Q. 2016. Improving attention modeling with implicit distortion and fertility for machine translation. In Proceedings of the 26th International Conference on Computational Technical Papers . The 2016 Organizing pages Andrew Paul and 2012. a phrase-based machine system with recurrent neural network language models. In Proceedings of the 4th Named En- Workshop 2012 . Association for Computational pages and 2016. multilingual neural ma- chine translation with a shared attention mechanism. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Human Language Technologies . Association for Computational San pages L and P 1997. Recursive hetero-associative memories for translation. In Biological and From Neuroscience to Technology pages and 2016. Fast domain adaptation for neural machine translation. Technical report. Michel Jonathan Kevin Daniel Steve Wei and Ignacio 2006. inference and training of context-rich syntactic translation models. In Pro- of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics . Association for Computational pages Michel Mark Kevin and Daniel 2004. Whats in a translation In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics . Jonas Michael David Denis and N. Dauphin. 2017. sequence to sequence learning. Adam Greg Jimenez David and Timothy P. 2017. Generative temporal models with memory. Goldberg. 2017. Neural Network Methods for Natural Language Processing volume 37 of Synthesis Lectures on Human Language Technologies . Morgan San CA. Ian and Aaron 2016. Deep Learning . MIT Press. . Hang and Victor Li. 2016. Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association for Linguistics Long . Association for Computational pages 106 Bibliography Bowen and 2016. Pointing the unknown words. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics Long . Association for Computational pages Kelvin and 2015. On using monolingual corpora in neural machine trans- Michael and 2014. Minimum translation modeling with recurrent neural networks. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics . Association for Computational pages Ann Irvine and Chris 2013. Combining bilingual and comparable corpora for low resource machine translation. In Proceedings of the Eighth Workshop on Statistical Ma- chine Translation . Association for Computational pages Roland and On using very large target vocabulary for neural machine translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing Long . Association for Computational pages Roland and Mon- neural machine translation systems for C In Proceedings of the Tenth Workshop on Statistical Machine Translation . Association for Computational pages Melvin Mike V. Maxim B. Martin Greg and Jeffrey Dean. 2016. Googles multilingual neural machine translation Enabling zero-shot translation. and 2016. Is neural ma- chine translation ready for a case study on 30 translation directions. In Proceedings of the International Workshop on Spoken Language Translation . and Phil 2013. Recurrent continuous translation models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing . Association for Computational pages 1176. Shin and 2016. Neural reordering model considering phrase translation and word alignment for phrase-based translation. In Proceedings of the 3rd Workshop on Asian Translation . The 2016 Organizing pages 103. P. and Jimmy Ba. 2015. A method for stochastic optimization Catherine and Jean 2016. Domain control for neural machine translation. Technical report. Alexandra Chris Nicola Bibliography 107 Brooke Wade Christine Richard Christopher J. r Alexandra and Evan 2007. Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions . Association for Computational Czech pages J. Lei J. R. and G. E. Hinton. 2016. Layer Normalization. e-prints . Yang and 2014. A neural reordering model for phrase-based translation. In Proceedings of the 25th International Conference on Com- Technical Papers . Dublin City University and Association for Computational Lin- pages Andrew and 2016. Neural machine translation with supervised attention. In Proceedings of the 26th International Conference on Computational Technical Papers . The 2016 Organizing pages 3102. and 2014. Learning new semi-supervised deep auto-encoder for statistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics Long . Association for Computational pages and Christopher Manning. 2015. Stanford neural machine translation systems for spoken language domains. In Proceedings of the International Workshop on Spoken Language Translation . pages Michael and Christopher D. Manning. Deep neural language mod- for machine translation. In Proceedings of the Nineteenth Conference on Computational Language Learning . Association for Computational pages and Christopher D. Manning. Effective approaches to attention- based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing . Association for Computational pages and Addressing the rare word problem in neural machine translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing Long . Association for Computational pages Hang and 2016. Interactive attention for neural machine translation. In Proceedings of the 26th International Conference on Computational Lin- Technical Papers . The 2016 Organizing pages Hang and 2015. Encoding source language with neural network for machine translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing Long . Association for Computational pages and Abe 2016. Vocabulary manipulation for neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics Short . Association for Computational pages 108 Bibliography Antonio and Giuseppe 2015. dependency-based reordering with recurrent neural network for machine translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Nat- Language Processing Long . Association for Computational pages Antonio r Rico Barry and Alexandra Birch. 2017. Deep architectures for neural machine translation. In Proceedings of the Second Conference on Machine Volume Research Papers . Association for Computational Den- pages Tomas 2012. Statistical Language Models based on Neural Networks . Brno University of Technology. Tomas Wen-tau and Geoffrey 2013. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 Conference of the North American Chapter of the As- for Computational Human Language Technologies . Association for Computational pages Tomas and 2013. On the of training recurrent neural networks. In Proceedings of the 30th International Conference on Machine . pages 1318. Luis and Francisco 2017. Online learning for neural machine trans- post-editing. 2007. Continuous space language models. Computer Speech and Language 2010. Continuous-space language models for statistical machine translation. The Prague Bulletin of Mathematical Linguistics 2012. Continuous space translation models for phrase-based statistical machine trans- In Proceedings of Posters . The 2012 Organizing pages Marta Ruiz and Jose A. R. 2007. Smooth bilingual n gram translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Processing and Computational Natural Language Learning . pages Daniel and 2006. Continuous space language models for statistical machine translation. In Proceedings of the 2006 Main Confer- Poster Sessions . Association for Computational pages Anthony and 2012. pruned or continuous space language models on a for statistical machine translation. In Proceedings of the 2012 Will We Ever Really Replace the N-gram On the Future of Language Modeling for . Association for Computational pages Rico and Barry 2016. Linguistic input features improve neural machine translation. In Proceedings of the First Conference on Machine Translation . Association for Computational pages Bibliography 109 Rico Barry and Alexandra Birch. Controlling politeness in neural machine translation via side constraints. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Human Language Technologies . Association for Computational San pages Rico Barry and Alexandra Birch. Edinburgh neural ma- chine translation systems for 16. In Proceedings of the First Conference on Machine Translation . Association for Computational pages Rico Barry and Alexandra Birch. Improving neural machine translation mod- with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics Long . Association for Computational pages Rico Barry and Alexandra Birch. Neural machine translation of rare words with units. In Proceedings of the 54th Annual Meeting of the Association for Computational Lin- Long . Association for Computational pages Maria and Jean 2016. Domain a post-training domain adaptation for neural machine translation. Geoffrey Alex and 2014. A simple way to prevent neural networks from Journal of Machine Learning Research Martin Ben and 2013. Comparison of and recurrent network language models. In IEEE International Conference on and Signal Processing . pages and V. V Le. 2014. Sequence to sequence learning with neural networks. In Z. M. C. and Advances in Neural Information Processing Systems 27 pages Alex and 2015. Incremental adaptation strategies for neural network language models. In Proceedings of the 3rd Workshop on Continuous tor Space Models and their . Association for Computational pages 2012. Parallel tools and interfaces in opus. In Khalid U Do Joseph Jan and Proceedings of the Eighth International Conference on Resources and Evaluation . European Language Resources Association pages Anthology Antonio and M. 2017. A multifaceted evaluation of versus phrase-based machine translation for 9 language directions. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Volume Long Papers . Association for Computational pages 110 Bibliography Yang and Hang Li. Context gates for neural machine translation. Yang and Hang Li. 2017. Neural machine trans- with reconstruction. In Proceedings of the 31st Conference on Intelligence . Yang and Hang Li. Modeling coverage for neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics Long . Association for Computational pages Marco and 2008. Learning performance of a machine a statistical and computational analysis. In Proceedings of the Third Workshop on Machine Translation . Association for Computational pages N. and 2017. Attention is all you need. Victoria and David 2013. Decoding with large-scale neural language models improves translation. In Proceedings of the 2013 Conference on Empirical Meth- in Natural Language Processing . Association for Computational pages A. A. N. A. E. H. and J. 1991. A speech- to-speech translation system using and symbolic processing strategies. In Proceedings of the 1991 International Conference on Speech and Signal Processing . pages and Lu. 2013. Con- continuous-space language models into n-gram language models for statistical machine translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Pro- . Association for Computational pages and 2014. Neural network based bilingual language model growing for statistical machine translation. In Proceedings of the 2014 Con- on Empirical Methods in Natural Language Processing . Association for Computational pages Philip Rico Maria Matthias Barry and r 2016. Edinburghs statistical machine translation systems for In Proceedings of the First Conference on Machine Translation . Association for Computational pages Wei and Ting 2014. Improve statistical machine translation with context-sensitive bilingual tic embedding model. In Proceedings of the 2014 Conference on Empirical Methods in Natural Processing . Association for Computational pages Mike V. Mohammad Wolfgang Maxim Yuan Klaus Jeff Melvin Stephan Keith George Wei Cliff Jason Jason Alex Greg and Jeffrey Dean. 2016. Googles neural machine Bibliography 111 translation Bridging the gap between human and machine translation. and 2012. Factored recurrent neural network language model in TED lecture transcription. In Pro- of the seventh International Workshop on Spoken Language Translation . pages Di Tao and Ma. 2016. Dual learning for machine translation. and 2015. Recurrent neural network based rule sequence model for machine translation. In Proceedings of the 53rd Annual Meeting of the Association for Com- Linguistics and the 7th International Joint Conference on Natural Language Processing Short . Association for Computational pages Matthew D. 2012. an adaptive learning rate method. and 2015. Local translation prediction with global sentence rep- In Proceedings of the Twenty-Fourth International Joint Conference on Intelligence . pages 112 Bibliography Author Index 61 88 89 David 11 Philip 95 Giuseppe 99 45 Michael 99 Jimmy 23 94 Paul 45 75 94 Luisa 98 Nicola 89 Alexandra 96 98 Phil 88 r 89 75 Patrice 88 Bill 99 Chris 91 Yuan 88 Francisco 75 M. 5 Luis 75 Mauro 98 Victor 95 88 Boxing 94 96 5 88 Colin 94 David 89 94 75 Trevor 72 Alexandra 89 88 Greg 88 Marta Ruiz 99 Aaron 23 Brooke 89 81 Maria 88 91 75 N. 88 91 99 Jeffrey 88 Daniel 45 Steve 89 88 Jacob 98 89 Paul 45 99 113 114 Author Index 45 John 23 Kevin 89 88 Chris 95 Christopher J. 89 88 M. 75 98 Christian 88 72 Andrew 96 81 Jose A. R. 99 L 5 Victoria 45 George 94 Ben 45 89 Michel 89 99 99 45 Jonas 88 88 11 23 N. 88 Ian 23 45 Stephan 88 Jonathan 89 Yvette 88 David 88 62 61 Barry 96 72 98 5 23 Di 65 Wei 99 r 68 Evan 89 G. E. 23 Geoffrey 23 Cong 72 89 Mark 89 79 99 99 94 98 Matthias 89 88 11 Gonzalo 99 Ann 91 Abe 62 99 A. N. 5 Christian 45 61 98 Antonio 88 Joshua 88 Melvin 88 88 88 88 88 Shin 5 79 88 Michael 45 88 96 Author Index 115 88 89 88 88 P. 23 J. R. 23 88 Jeff 88 Kevin 89 88 Catherine 88 89 5 Maxim 88 Alex 23 88 Roland 94 George 88 75 Thomas 98 Samuel 75 61 V. 88 V. V 57 Lei J. 23 Hang 98 Mu 72 99 Victor 62 Timothy P. 11 63 96 98 72 65 Ting 99 88 95 Yang 99 88 Jean 88 45 5 79 98 89 61 65 Klaus 88 Wolfgang 88 John 98 Christopher 89 Christopher D. 57 Daniel 89 88 79 96 A. E. 5 Roland 61 98 62 Antonio 99 Tomas 45 11 88 Christine 89 Maria 89 95 61 P 5 88 Graham 95 88 Mariana 88 45 88 Mohammad 88 45 88 23 116 Author Index 88 75 96 57 88 Martin 88 Matt 89 Alexandra 88 Tao 65 Anabel 88 Jimenez 11 Thomas 88 Jason 88 Anthony 45 Raphael 88 Alex 88 H. 5 23 M. 98 Adam 11 Carolina 88 45 Mike 88 Richard 98 99 Natalia 88 Jean 88 Rico 96 88 88 57 88 Wade 89 23 Jason 88 Noah A. 95 Lucia 88 23 Keith 88 5 96 45 99 Martin 45 61 J. 5 Alex 75 Ignacio 89 81 89 Cyril 88 Antonio 98 95 Marco 91 88 96 van Bart 57 88 Karin 88 Enrique 5 B. 81 Pascal 45 88 72 A. 5 88 99 65 98 45 Wei 89 62 Martin 81 Greg 11 Philip 89 99 99 88 Author Index 117 79 65 5 Kelvin 63 79 88 Kathy 88 Nan 72 72 Denis 88 Wen-tau 35 Cliff 88 99 99 65 Marcos 88 61 98 Matthew D. 23 Richard 89 99 98 45 45 Bowen 61 88 Ming 72 Kenny Q. 72 99 Peter 88 Geoffrey 35'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limited_df['english_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d58900d1",
   "metadata": {
    "id": "d58900d1"
   },
   "outputs": [],
   "source": [
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51Xh5mpTM4nW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "51Xh5mpTM4nW",
    "outputId": "4a2b49e6-2d35-4100-f27d-b4d18323f5e0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Statistical Machine Translation\\n\\nDraft of Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Found in Translation:\\n\\nLearning Robust Joint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Available online at www.sciencedirect.com\\n\\nS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>J OURNAL OF I NFORMATION S CIENCE AND E NGINEE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The University of Sheeld\\n\\nT. E. Dunning\\n\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           articles\n",
       "0   0  Statistical Machine Translation\\n\\nDraft of Ch...\n",
       "1   1  Found in Translation:\\n\\nLearning Robust Joint...\n",
       "2   2  Available online at www.sciencedirect.com\\n\\nS...\n",
       "3   3  J OURNAL OF I NFORMATION S CIENCE AND E NGINEE...\n",
       "4   4  The University of Sheeld\\n\\nT. E. Dunning\\n\\n..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limited_df['clean_text'] = limited_df['english_text'].apply(lambda x: finalpreprocess(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec6fc8cf",
   "metadata": {
    "id": "ec6fc8cf"
   },
   "outputs": [],
   "source": [
    "articles = limited_df['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a9ad918",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "3a9ad918",
    "outputId": "b0d16b74-798b-4c4c-dc85-6f04b75e3caf"
   },
   "outputs": [],
   "source": [
    "raw_text = articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f9f261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ''\n",
    "\n",
    "for article in articles:\n",
    "    corpus += article\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "20d05e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'statistical machine translation draft chapter neural machine translation center speech language processing department computer science john hopkins university st public draft august nd public draft september content neural machine translation short history introduction neural network linear model multiple layer non linearity inference back propagation training computation graph neural network computation graph gradient computation deep learn framework neural language model fee forward neural language model word embed inference training recurrent neural language model long short term memory model gate recurrent unit deep model neural translation model encoder decoder approach add alignment model train beam search content ensemble decode large vocabulary use monolingual data deep model guide alignment training model coverage adaptation add linguistic annotation multiple language pair alternate architecture neural network neural network attention self attention current challenge domain mismatch amount training data noisy data word alignment beam search read additional topic bibliography author index index chapter neural machine translation major recent development statistical machine translation adoption neural net work neural network model promise well share statistical evidence similar word inclusion rich context chapter introduces several neural network model technique explains apply problem machine translation short history already last wave neural network research machine trans sight researcher explore method model propose strike similar current dominant neural machine translation approach none model train data size large enough produce reasonable result anything toy ex computational complexity involve far exceeded computational resource hence idea abandon almost two decade hibernation data driven approach phrase base statistical chine translation rise obscurity dominance make machine translation useful tool many information increase productivity professional translator modern resurrection neural method machine translation start neural language model traditional statistical machine translation system work showed large improvement public evaluation campaign ideas slowly mainly due computational concern use training also pose challenge many research group simply lack hardware experience exploit move beyond use language neural network method creep com traditional statistical machine provide additional score ex tend translation table lu reorder li chapter neural machine translation model joint translation language model since show large quality improvement top competitive statistical machine translation system ambitious effort aim pure neural machine abandon exist approach completely early step use model sequence sequence model able produce reasonable translation short fell apart crease sentence length addition attention mechanism yield competitive result jean byte pair encode back translation target side monolingual neural machine translation become new state art within year entire research machine translation go neural give indication speed share task machine translation organize conference machine translation one pure neural machine translation system submit outperformed traditional statistical system year neural machine translation system almost language pair almost submission neural machine translation system time neural machine translation research progress rapid pace many direction explore come range core machine learn improvement deep model linguistically inform model insight strength weaknesses neural machine translation gather inform future work extensive proliferation available neural machine translation system time number rather consolidate quite hard premature make recommendation promise marian implementation sockeye introduction neural network neural network machine learning technique take number input predict outputs many different machine learn method distinct strengths introduction neural network figure graphical illustration linear model feature value input arrow score output node linear model linear model core element statistical machine translation potential translation sentence represent set feature feature weighted parameter obtain overall score ignore exponential function use previously turn linear model log linear follow formula sum model score linear model illustrated feature value input arrow score output node figure use linear model combine different component machine translation language phrase translation reorder property length accumulate jump distance phrase translation train method assign weight value feature relate importance contribute score good translation higher machine call tune linear model allow u complex relationship feature let u say short sentence language model less important translation average phrase translation probability higher similarly reasonable value really terrible hypothetical example implies dependence feature second example implies non linear relationship feature value impact score linear model can not handle case commonly cite counter example use linear model operator truth table linear model two feature possible come weight give correct output case linear model assume represent point feature linearly separable case may case type feature use machine translation chapter neural machine translation figure neural network hide layer multiple layer neural network modify linear model two important way use multiple layer instead compute output value directly input hide layer introduce call observe input output train mechanism connects use concept hidden similar meaning hidden markov model see figure illustration network process two step linear combination weight input node compute produce hidden node value linear combination weight hidden node compute produce output node value let u introduce mathematical notation neural network literature neural network hide layer consist vector input node value vector hidden node value vector output node value matrix weight connect input node hide node matrix weight connect hidden node output node computation neural network hidden sketch note possibility multiple output node although far show one introduction neural network hyperbolic tangent logistic function linear unit cosh output range output range output range figure typical activation function neural network non linearity carefully think addition hide realize gain anything far model relationship easily away hidden layer multiply weight salient element neural network use non linear activation function compute linear combination weight feature value obtain value node apply function popular choice hyperbolic tangent logistic function see figure detail function good way think activation function segment range value linear combination segment node turn close transition segment node partly turn segment node turn close different popular choice activation function linear unit allow negative value alter value positive value simpler fast compute could view hidden node feature detector certain input node turn others turn advocate neural network claim chapter neural machine translation figure simple neural network bias node input hidden layer use hidden node obviates least drastically need feature engineer instead manually detect useful pattern input train hidden node discovers automatically stop single hidden layer currently fashionable name deep learn neural network stem fact often well performance achieve deeply stack together layer layer hide node inference let u walk neural network output value compute input concrete example consider neural network figure network one additional innovation present bias unit node always value bias unit give network something work case input value weight sum would matter weight let u use neural network process say value input node second input node value bias input node compute value hide node carry follow calculation calculation node summarize table output value node input expect binary would understand result value since threshold range possible output value output possible binary introduction neural network layer node summation activation hide hidden output table calculation input network figure input input hide hidden output neural network computes xor look hidden node notice act like boolean value high least two input value three otherwise low value hide node act like boolean high value input xor effectively implement subtraction hidden node note non linearity key since value node much high input oppose single input v distinct high value node case manage push output threshold would possible value input would simply sum linear model mention recently use name deep learn neural network become fashionable emphasizes often high performance achieve use network multiple hidden layer xor example hint power come single input output layer network possible mimic basic boolean operation since model linear xor express neural network example implement boolean operation subtraction second layer function require intricate operation may chain hence neural network architecture hidden layer may need may possible train build neural network computer number hidden layer match depth computation line research banner neural turing machine explores kind architecture need implement basic neural network two hidden layer implement algorithm sort bit number back propagation training train neural network require optimization weight value network correct output set training example repeatedly fee input chapter neural machine translation optimum gradient comb current point figure gradient descent compute gradient regard every dimension case gradient respect weight small gradient respect weight move leave arrows point negative gradient point training example compare compute output network correct output train update weight several pass train data carry pas data call epoch common training method neural network call back propagation since update weight output propagate back error information earlier layer whenever train example node error term compute basis update value incoming weight formula use compute updated value weight follow principle gradient descent training error node understood function incoming weight reduce error give compute gradient error function respect move gradient reduce error move alongside gradient good consider optimize multiple dimension time look low point area look water ground fall steep west also slightly south would go direction mainly west slightly south go alongside gradient see figure illustration follow two derive formula update weight example network less interested skip section continue read summarize update formulae page weight output node let u review extend notation output node compute linear combination weight hidden node value introduction neural network sum pass activation function compute output value compare compute output value target output value train example various way compute error value value let u use norm state goal compute gradient error respect weight direction move weight value weight separately break computation gradient three essentially unfolding equation let u work three step since error term output value compute component follow derivative output value respect linear combination weight hidden node depend activation function case keep treatment general possible commit activation use shorthand note give train example give differentiable activation value always compute compute derivative respect weight turn quite simply value hidden node equation compute three step need compute gradient error function give unfolded laid equation put chapter neural machine translation factor learn rate give u follow update formula weight note also remove minus since move gradient towards minimum useful introduce concept error term note term associate weight update concern weight error term compute use incoming weight reduces update formula weight hide node computation gradient hence update formula hide node quite anal linear combination input value hide value weight weight weight lead computation value hide node follow principle gradient need compute derivative error respect weight decompose derivative dz error term output value value hide node idea behind back propagation track error cause hidden node contribute error next layer apply chain rule give dz computation complex case output since introduction neural network already encounter two term previously third term equation compute straightforward put equation equation equation solve give rise quite intuitive interpretation error matter hide node depend error term subsequent node weight impact hidden node output node let u tie remain loose end miss piece equation second term dz dz third term dz put equation equation equation together give u gradient dz dz error term hidden node analogous output node analogous update formula chapter neural machine translation summary train neural network process train one update weight time drive weight updates gradient towards small error weight update compute base error term associate non input node network output error term compute actual output node current target output node hidden error term compute via back propagate error term subsequent node connect weight compute require derivative activation weight sum incoming value pass give error weight proceed node temper learn rate weight next training example process typically pass training call epochs example give neural network figure let u see train example pro let u start calculation error term output node inference table page compute linear combination weight hidden node value node value target value compute weight weight since hidden node lead one output node calculation error term computationally complex table summarizes update weight introduction neural network node error term weight update table weight update learn rate neural network figure training example present local optimum global optimum high learn rate bad initialization local optimum figure problem gradient descent training motivate detail section high learn rate may lead drastic parameter overshoot bad initialization may require many update escape existence local optimum trap train chapter neural machine translation error validation minimum validation train train progress figure train progress time error training set continuously decrease validation set use point error increase train stop validation minimum set conclude introduction neural network basic motivate consider figure gradient descent training may run practical problem set learn rate high lead update overshoot optimum con low learn rate lead slow convergence bad initialization weight may lead long path many update step reach optimum especially problem activation function like short interval change existence local optima lead search get trap miss global optimum validation set neural network train proceeds several full iteration train data track training see error training set continuously decrease point set train data memorize generalized check additional set call validation set use train see figure illustration measure error validation set point see point error increase stop minimum validation set reach introduction neural network weight initialization training weight initialize random value value uniform distribution prefer initial weight lead node value transition area activation low high shallow slope would take long time push towards change activation feeding value range activation function lead activation value range activation commonly use formula weight layer network size previous layer hidden choose weight range size previous size next layer momentum term consider case weight value far optimum even train exam push weight value may still take small update accumulate weight reach optimum common trick use momentum term speed train momentum term get updated time step train combine previous value momentum term current raw weight update value use result momentum term value update weight decay rate update formula change adapt learn rate per parameter common training strategy reduce learn rate time begin parameter far away optimal value change later train stage concern large learn rate may cause parameter bounce around optimum different parameter may different stage path optimal different learn rate parameter may helpful one call record gradient compute parameter accumulates square value us sum adjust learn rate chapter neural machine translation update formula base sum gradient error respect weight time step divide learn rate weight accumulate sum big change parameter value big gradient lead reduction learn rate weight parameter combine idea momentum term adjust parameter update change inspiration adam another method transform raw gradient parameter update idea compute equation idea square gradient adjust learn rate since raw accumulation run risk become large hence permanently depress learn adam us exponential like momentum term hyper parameter set typically close also mean early training value close initialization value adjust correct bias increase training time step correction go piece hand rate momentum accumulate change weight update per adam compute common value hyper parameter various adaptation scheme active area research second order gradient give useful information rate change often expensive shortcut take introduction neural network dropout parameter space back propagation learning variant operate lit local optimum hill climb algorithm may climb mole hill stick instead move towards climb high mountain various method propose get train local optimum one currently popular method neural machine translation call drop sound bit simplistic wacky node neural network ignore value set associated parameter update drop node choose may account much even nod training resume number iteration without different set drop node select drop node play useful role model train point ignore node pick slack end result robust model several node share similar role layer normalization layer normalization address problem arises especially deep neural network use neural machine compute proceeds large sequence layer train average value one layer may become feed follow also produce large output especially problem activation function limit output narrow linear unit train examples average value layer may small cause problem train recall equation gradient update strongly effect node value large node value lead explode gradient small node value lead diminish gradient remedy idea normalize value per layer basis do add additional computational step neural network recall feed forward layer consist matrix multiplication weight matrix node value previous layer result weight sum follow activation function compute mean variance value weight sum vector chapter neural machine translation use normalize vector use two additional bias vector element wise multiplication difference subtracts scalar average vector element formula normalize value shift average hence ensure average afterwards result vector divide variance additional bias vector give may share across multiple layer multiple time step recurrent neural network introduce section page mini batch train example yield set weight update may process train example afterwards apply updates neural network advantage immediately learn train example training method update model train example call online learn online learn variant gradient descent training call stochastic gradient descent online learn generally take few pass train set epochs con since train constantly change hard may want process train data accumulate weight apply collectively small set train example call mini batch distinguish approach batch training entire training set consider one batch variation organize process training typically motivate restriction parallel process process training data mini computation weight update value synchronize summation application weight want distribute training number computationally convenient break training data equally size perform online learning part use small mini average weight break training often lead good result straightforward linear processing scheme call run several train thread immediately update even though thread still use weight value compute gradient clearly violate safe guard typically take parallel hurt practical experience vector matrix operation express calculation need handle neural network vector matrix operation computation graph forward activation error propagation error weight execute operation computationally expensive layer matrix operation require multiplication matrix also common another highly use area computer graphic process render image geometric property dimensional object process generate color value dimensional image screen since high demand fast graphic instance use realistic look computer specialize hardware become graphic process unit processor massive number core nvidia gpu provide thread rather lightweight instruction set provide instruction apply many data point exactly need vector space computation list program support various become essential part develop large scale neural network application general term matrix tensor tensor may also sequence matrix pack dimensional tensor large object actually frequently use today neural network reading good introduction modern neural network research textbook also book neural network method apply natural language process general number key technique recently develop entered standard neural machine translation research training make robust method drop training interval number node randomly mask avoid explode vanish gradient back propagation several gradient clip layer normalization ba similar ensure node value within reasonable bound active topic research optimization method adjust learn rate gradient descent training popular method currently adam computation graph example neural network section painstakingly work gradient computation need gradient descent train hard may chapter neural machine translation prod sum prod sum figure two layer fee forward neural network computation consist input value weight parameter computation node right parameter value show leave input computation show input process graph come surprise likely never do even arbitrarily complex neural network architecture number allow network take care rest take close look work neural network computation graph take different look network build previously represent neural network graph consist node connection figure page mathematical equation equation describe fee forward neural network use run example represent math form computation graph see figure illustration computation graph network graph contain nodes parameter model weight matrix bias vector input mathematical operation carry next show value computation graphs neural view computation arbitrary connect operation input number parameter operation may little inspiration neuron stretch term neural network quite bite graph nice tree structure may direct graph anything go long straightforward process direction cycle another way view graph fancy way visualize sequence function call take argument previously compute combination recursion loop process input neural network require place input value node carry computation show input vector result number look familiar since previously work example section move let u take stock computation node graph accomplish consist function execute computation operation link input node process computed value add two item node follow section gradient computation show computation graph use process input value examine use vastly simply model train model training require error function computation gradient derive update rule parameter quite straightforward compute need add another computation end computation graph computation take compute put value give correct output value train data produce error value typical error function norm view result execution computation graph error value part update rule parameter look computation model update originate error value propagate back model parameter call computation need compute date value also backward pas oppose forward pas compute output error calculus refresher chain rule formula compute derivative two function chain rule ex press derivative composition function map term derivative product function write explicitly term variable let chapter neural machine translation consider chain operation con weight matrix error com sum prod value hide layer result early computation compute update rule pa matrix view error function parameter take derivative respect case break step use chain rule sum prod sum prod note purpose compute update rule treat computation target value bias vector hidden node value constant break derivative error respect parameter chain derivative along line node computation graph gradient computation come derivative node computation graph example sum prod want compute gradient update parameter compute value backward start error term see figure illustration give detail computation gradient backward start bottom recall compute computation graph prod sum prod sum figure computation graph gradient compute backward pas training example gradient compute respect input node two input also two gradient see text detail computation value chapter neural machine translation use formula give target output value give train data compute forward pas gradient norm note use value compute forward pas gradient computation low use formula recall formula plug value compute forward pass formula give u chain rule require u multiply value compute give u low sum simply copy previous since sum note two gradient associate sum node one respect output prod one parameter derivative value gradient case low prod use formula prod deal scalar value encounter vector value hide node chain rule require u multiply previously compute scalar sum two input hence two gradient gradient respect output upper node prod similarly compute computation graph gradient read relevant value weight date gradient associate trainable parameter weight second gradient prod node new value time step prod remain computation carry similar since form simply another layer feed forward neural network example include one special output computation may use multiple time subsequent step computation graph multiple output node fee back gradient back propagation pas add step factor add impact let u take second look node computation graph function compute value link input node obtain argument process example forward compute value function execute gradient computation link child node obtain downstream gradient process example forward compute gradient object orient program node computation graph provide forward backward function value gradient computation instantiate computation connect input also aware dimension variable value gradient forward backward variable deep learn framework next encounter various network architecture need vector matrix well computation obtain weight update formula would quite tedious write almost identical code deal variant number framework emerge sup port develop neural network method choose problem time prominent one python library generates compiles code build torch machine learn library script language base program python variant chapter neural machine translation implementation natural language processing researcher use library recent entry genre framework less geared towards ready use neural network provide implementation vector space operation computation seamless support example section implement line python show use example framework quite execute follow command python command line interface instal pip install import import import map input layer hide layer use weight matrix bias vector map function consist linear combination activation function note matrix allow u process several training example sequence good way think term functional programming language symbolically operation actually function method function use example call compute value hide node number table page map hidden layer output layer fashion callable function test full network predict neural language model model train require cost function use formulate need variable correct output overall cost compute average training examples cost gradient descent training require computation derivative cost function respect model parameter value weight matrix bias vector great use computes derivative follow also example function multiple input multiple output need train function update model parameter return current prediction cost us learn rate train let u train data train function return prediction cost update call train function prediction cost change well would loop train function convergence discuss may also break train data mini batch train one mini batch time neural language model neural network powerful method model conditional probability distribution multiple input robust unseen data point unobserved training data use traditional statistical estimation may address sparse data problem back require insight prob part condition context drop arbitrary choice many chapter neural machine translation word word word word word figure sketch neural language predict word base precede word gram language model reduce probability sentence product word probability context previous word model prime example conditional probability distribution rich conditioning context often lack data point would like cluster information statistical language complex discounting back scheme use balance rich evidence low order model model sparse estimate high order model turn neural network help fee forward neural language model figure give basic sketch gram neural network language model network node represent context word connection hide connects put layer predict word represent word immediately face rep resent node neural network carry real numbered word discrete item large vocabulary can not simply use token since neural network assume token similar token practice completely arbitrary argument applies idea use bit encode token id word similar may nothing idea use bit vector occasionally appear consider next represent word high dimensional one dimension per word value dimension match rest type vector call one hot vector dog cat eat neural language model word word word word word figure full architecture feed forward neural network language model context word represent one hot project continuous space word weight matrix predict word compute one hot vector via hide layer large continue wrestle impact choice represent word one stopgap limit vocabulary pool word token could also use word class automatic cluster linguistically motivate class part speech reduce vector revisit problem large vocabulary later pool evidence introduce another layer input layer hidden layer context word individually project low space use weight matrix context thus generate continuous space representation independent position condition context representation commonly refer word embedding word occur similar context similar word training data language model frequently contain gram cute dog jump cute cat jump child hug cat tightly child hugged dog tightly like watch cat video like watch dog videos language model would knowledge dog cat occur similar context hence somewhat interchangeable like predict context dog occur see context word cat would still like treat positive evidence word enable generalizing word hence robust prediction unseen contexts chapter neural machine translation neural network architecture see figure visualization architecture fully fee forward neural network language consist context word one hot vector input word embed hidden layer predict output word layer context word encode one hot vector pass embed matrix result vector point word embed embed vector typically order node note use embed matrix context word also note mathematically much go since input multiplication matrix one hot input value matrix multiplication zero select one column matrix correspond input word id use activation function embed matrix lookup table word indexed word id map hidden layer model require concatenation context word em input typical feed forward use activation function output layer interpret probability distribution word linear combination weight hidden node value compute node ensure indeed proper probability use activation function ensure value add one describe close neural probabilistic language model propose model one add direct connection context word output word equation replace paper report direct connection context word output word speed although ultimately improve performance en counter idea short cut hidden layer bite late discus deep model hide layer also call residual connection skip connection even highway connection neural language model train train parameter neural language model embed weight bias process gram train corpus feed context word network match network output one hot vector correct word predict weight update use back propagation go detail next language model commonly evaluate related probability give proper english text language model like proper english good language model train objective language model increase likelihood train data give context correct value hot vector train example likelihood log note one value others really come probability give correct word likelihood way allow u update also one lead wrong output word word embed move worth role word neural machine trans many natural language processing task introduce compact encode word relatively high dimensional say point natural language time word acquire reputation almost magical quality consider role play neural language language describe represent context word enable prediction next word sequence recall part earlier cute dog jump cute cat jump since dog cat occur similar predicting word jump similar different word dress unlikely trigger completion jump idea word occur similar context semantically similar powerful idea lexical semantics point researcher love cite john rupert shall know word company keep ludwig wittgenstein put bit chapter neural machine translation figure word project semantically similar word occur close meaning word use meaning semantics quite concept largely unresolved idea distributional lexical semantics word mean distributional prop context occur word occur similar context dog cat similar representation vector space word use similarity measure distance cosine distance angle vector project high dimensional word two visualize word show figure word similar festival cluster together stop would like semantic representation carry semantic inference queen king queen queen indeed evidence word embed allow good stop note word crucial tool neural machine translation neural language model inference train train neural language model computationally expensive billion word even use train take several day modern compute cluster even use neural language model score component statistical machine translation decode require lot computation could restrict use rank best list consider method inference training cache inference actually possible use neural language model within decoder word actually need carry map ping one hot vector word store beforehand computation hide layer also partly carry note word occur one slot condition context gram language matrix multiplication word embed vector correspond weight run sum hidden layer apply activation function compute value output node insanely since many output node vocabulary item interested score give word produce translation model compute node score use last point require long discussion compute node value word want score language miss important step obtain proper need normalize require computation value node could simply ignore problem use score face value likely word give context get high score less likely main objective since place constraint may work model contexts give high score many contexts give preference would node value layer already normalized probability method enforce train let u discuss train move method section noise estimation discuss earlier problem compute tie neural language model expensive due need normalize output node value use function require compute value output even interested score particular gram overcome need chapter neural machine translation explicit normalization would like train model already value normalized one way include constraint normalization factor close objective function instead simple likelihood may include norm log factor note log log log another way train self normalize model call noise estimation main idea optimize model separate correct training example create noise example method need less computation since require computation output node value try learn model distribution give noise case language model model good choice generate set noise example addition correct train example set size probability give example predict correct train example correct objective noise estimation maximize correct correct train example minimize noise example use log objective function log correct log correct return original goal self normalize note noise normalize model distribution encourage produce value would generally overshoot would also give high value noise example generally undershoot would give low value correct translation example train since need compute output node value give train noise example need compute since function give train objective complete computation graph implement use standard deep learning do via gradient descent training may immediately obvious optimize towards classify correct noise examples give rise model also predict correct probability grams variant method common statistical machine translation tune phase mira infused relaxation pro rank follow principle compute gradient parameter use parameter update neural language model word word word word copy value copy value word word figure recurrent neural language predict word context follow word use hidden layer correct word predict word hide layer prediction use prediction word recurrent neural language model fee forward neural language model describe able use longer con texts traditional statistical back since mean deal unknown context use word make use similar robust handling unseen word context position possible condition much large contexts traditional statistical model large gram report use instead use context word recurrent neural network may condition context sequence length trick use hidden layer predict word additional input predict word see figure illustration model look different feed forward neural language model discuss far inputs network word sentence second set neuron point indicate start sentence word embed start sentence neuron map hidden layer use predict output word model use architecture word represent one hot word hide layer real value neuron use activation function hide layer function output layer thing get interesting move predict third word sequence one input directly precede word neuron network use represent start sentence value hide layer previous prediction word neuron encode chapter neural machine translation word word word word word word figure back propagation unfold recurrent neural network number prediction step derive update formula base train objective predicting output word back propagation error via gradient descent previous sentence context enrich step information new input word hence condition full history sentence even last word sentence condition part word sentence model less weights gram feed forward neural language model train model arbitrarily long one initial stage second word architecture hence training procedure fee forward neural network assess error output layer propagate update back input layer could process every training example way essentially treat hide layer previous train example input current example never provide feedback representation prior history hidden layer back propagation time train procedure figure unfold current neural network number go back word note despite limit unfolding time network still able learn dependency longer distance back propagation time either applied training example call time computationally quite expensive time computation carry several step compute apply weight update mini batch section process large number training example entire update weight give modern compute fully unfolding recurrent neural network become common recurrent neural network theory arbitrary give train size actually know fully construct com graph give train error sum word prediction carry back propagation entire sentence require quickly build computation graph call dynamic computation graph currently support good others neural language model long short term memory model consider follow step word prediction sequential language much economic progress country directly precede word country informative prediction word previous word much less relevant importance word decay distance hidden state recurrent neural network always update recent memory older word likely diminish time distant word much follow example country make much economic progress year still verb depends subject country separate long subordinate clause recurrent neural network allow model arbitrarily long sequences architecture simple simplicity cause number problem hide layer play double duty memory network continuous space representation use predict output word may sometimes want pay attention directly previous sometimes pay attention longer clear mechanism con train model long update need back propagate beginning sentence propagate many step raise concern impact recent information step drown old information rather confusingly name long short term memory neural network address issue design quite although use practice core distinction basic building block call cell contain explicit memory state memory state cell motivate digital memory cell ordinary computer digital memory cell offer operation reset digital memory cell may store single cell store real number operation cell regulate real call gate figure note correspond explode gradient long distance gradient value become large typically suppress clip limit maximum value set hyper parameter chapter neural machine translation layer time precede layer forget gate next layer layer time figure cell neural network recurrent neural receives input layer hide layer value previous time step memory state update input state previous time value memory state various gate channel information cell towards output value input gate parameter regulates much new input change memory state forget gate parameter regulate much prior memory state retain output gate parameter regulate strongly memory state pass next layer mark output value time step information within cell follow memory gate input input gate forget memory output gate output memory hidden node value pass next layer application activation function output value output layer consist vector traditional layer consist vector node input layer compute way input recurrent neural network node give node value prior layer value hide layer previous time step input value typical combination matrix multiplication weight activation function input neural language model gate parameter actually play fairly important role par would like give preference recent input input rather past memory forget pay less attention cell current point time output decision inform broad view context compute value complex condition treat like node neural network gate input forget output matrix ha compute gate parameter value multiplication weight node value previous layer hide layer previous time memory state previous time step memory follow activation function gate ha memory train way recurrent neural use back propagation time fully unrolling network operation within cell complex recurrent neural operation still base matrix differentiable activation function compute gradient objective function respect parameter model compute update function gate recurrent unit cell add large number additional parameter gate multiple weight matrix add parameter lead long train time risk simpler gate recurrent unit propose used neural translation model time cell seem make comeback neural machine still commonly used see figure illustration cell separate memory hidden state serf purpose two gate gate predict input previous state update update input update state bias update reset reset input reset state bias reset gate use combination input previous state combination identical traditional recurrent neural except previous state impact scale reset gate since gate value may give preference current input combination input reset state update gate use interpolation previous state compute combination do weighted update gate balance two chapter neural machine translation layer time reset gate precede layer next layer layer time figure gate recurrent unit long short term memory cell state update state update combination bias one extreme update gate previous state pass directly another extreme update gate new state mainly determine much impact previous state reset gate allows may seem bit redundant two operation gate combine prior state input play different role operation yield combination classic recurrent neural network component allow complex computation combination input output second operation yield new hidden state output unit allow bypass enable long distant memory simply pass information back pass thus enable long distance dependency deep model currently fashionable name deep learning late wave neural network research real motivation large gain see task vision speech recognition due stack multiple hidden layer together layer allow complex sequence traditional computation component allow complex computation multiplication number generally recognize long modern hardware enable train deep neural network real world problem neural language model input hide layer output input hidden layer hide layer hide layer output input hidden layer hide layer hide layer output shallow deep stack deep transition figure deep recurrent neural network input pass hidden layer output prediction make deep stacked hidden layer also connect layer value time step depend value time step well previous layer time step deep transitional layer time step sequentially connect hidden layer also inform last layer time step learn experiment vision speech even dozen layer give increasingly good quality idea deep neural network apply sequence prediction task common several option figure give two example shallow neural input pass single hidden output predict sequence hidden layer use hidden layer may deeply stacked layer act like hidden layer shallow recurrent neural network state condition value previous time step value previous layer sequence layer prediction last layer hide layer may directly connect deep transitional hidden layer inform last hidden layer previous time step hidden layer connect value previous time step layer prediction last layer chapter neural machine translation function may layer multiplication plus activation cell cell experiment use neural language model traditional statistical machine show hidden layer modern hardware allow train deep stretch computational resource practical limit computation neural convergence training typically slow add skip connection input directly output hidden sometimes speed still talk several time longer train time shallow network reading vanguard neural network research tackle language model prominent reference neural language model implement gram model feed forward neural network history word input predict word output introduce language model machine translation call space language use similar earlier work speech recognition propose number speed ups make implementation avail able open source toolkit also support train graphical processing unit cluster word class encode word pair class word class reduce computational complexity allow integration neural network language model decoder another way reduce computational complexity enable decoder integration use noise estimation roughly self normalizes output score model hence remove need compute value possible output word compare two technique class base word encode normalized score vs estimation without normalized score show letter give good performance much high speed another way allow straightforward decoder wang convert space language model short list word traditional gram language model format wang present method merge continuous space language model traditional gram language take advantage well estimate word short list full coverage traditional model finch use recurrent neural network language model best list system compare fee forward long short term neural network language variant recurrent neural network language show good performance latter speech recognition rank task report improvement best list machine translation system recurrent neural network language model neural language model deep learning model sense use lot hidden layer show hidden layer improve typical layer language model neural machine traditional statistical machine translation model straightforward mechanism integrate additional knowledge large domain language model hard end end neural machine translation add language model train additional monolingual data form recurrently neural translation model house big give word embed hidden state predict word house big figure sequence sequence encoder decoder extend language con english input sentence house big german output sentence dark green box process end sentence token contain embed entire input sentence neural network run parallel compare use language model rank deep integration gate unit regulates relative contribution language model translation model predict word neural translation model prepared look actual translation model already do since commonly use architecture neural machine translation straightforward extension neural language model one alignment model encoder decoder approach stab neural translation model straightforward extension language model recall idea recurrent neural network model language sequential process give previous model predicts next word reach end proceed predict translation one word time see figure illustration train simply concatenate input output sentence use method train language model feed input go prediction model predict end sentence token network processing reach end input sentence predict end sentence marker hidden state encodes mean vector hold value node hide layer input sentence embed encoder phase model hidden state use produce translation decoder phase chapter neural machine translation ask lot hidden state recurrent neural network encoder need incorporate information input sentence can not forget word towards end sentence decoder need enough information predict next also need account part input sentence already still need cover proposed model work reasonable well short sentence fail long sentence minor model use sentence embed state input hidden state decoder phase model make decoder structurally different encoder reduces load hidden state since need remember anymore input another idea reverse order output last word input sentence close last word output sentence follow embark improvement explicitly alignment output word input word add alignment model time state art neural machine translation sequence sequence encoder decoder model attention essentially model describe previous explicitly alignment mechanism deep learning alignment call attention use word alignment attention interchangeable since attention mechanism add bit complexity slowly build take look attention mechanism encoder task encoder provide representation input sentence input sentence sequence consult embed matrix basic language model describe process word recurrent neural network result hide state encode word leave precede word also get right also build recurrent neural network run right end sentence begin figure illustrate model two recurrent neural network run two direction call bidirectional recurrent neural network encoder consist embed lookup input word map step hidden state neural translation model input word leave right recurrent right leave recurrent figure neural machine translation part input encoder consist two recurrent neural run right leave leave right recurrent neural encoder state combination two hidden state recurrent neural network equation use generic function cell recurrent neural net work function may typical fee forward neural network layer ax complex gate recurrent unit long short term memory cell original paper propose approach use lately become popular note could train model add step predicts next word actually train context full machine translation model limit description output sequence word representation concatenate two hidden state decoder decoder also recurrent neural network take representation input con text next section attention previous hidden state output word generate new hidden decoder state new output word prediction see figure illustration start recurrent neural network maintains sequence hidden state compute previous hidden state embed previous output word input context still several choice function combine input generate next hidden linear transforms activation etc choice match encoder use also use decoder hidden state predict output word prediction take form probability distribution entire output vocabulary vocabulary prediction dimensional element correspond probability predict one word vocabulary chapter neural machine translation context state prediction word select word embed figure neural machine translation part output decoder give context input embed previously select new decoder state word compute prediction vector condition decoder hidden state embed previous output word input context cc note repeat conditioning since use hidden state separate encoder state progression prediction output word use convert raw vector probability sum value high value vector indicate output word token word embed informs next time step recurrent neural network correct output word training proceeds word training objective give much probability mass possible correct output word cost function drive train hence negative log probability give correct word translation cost log want give correct word probability would mean negative log probability typically low hence high cost note cost function tie individual overall sentence cost sum word cost inference new test typically chose word high value use embed next step also explore beam search next likely word select create different condition context next word later neural translation model encoder state attention input context hidden state output word figure neural machine translation part attention model association compute last hidden state decoder word representation use compute weight sum encoder state attention mechanism currently two loose end decoder give u sequence word representation decoder expect context step describe attention mechanism tie end together attention hard visualize use typical neural network figure give least idea input output relation attention mechanism inform input word representation previous hidden state decoder produce context state motivation want compute association decoder state contains information output sentence input word base strong association word relevant particular input word produce next output want weight impact word compute association layer weight vector bias value output computation scalar indicate important input word produce output word normalize attention attention value across input word add use exp exp use normalized attention value weigh contribution input word representation context vector do chapter neural machine translation simply add word representation vector may seem odd simplistic thing common practice deep learn natural language processing researcher qualms use sentence simply sum word scheme train complete model take close look train one challenge number step decoder number step encoder varies train example sentence pair consist sentence different can not computation graph train example instead dynamically create computation graph technique call unrolling recurrent neural already discuss regard language model section fully unrolled computation graph short sentence pair show figure note couple thing error compute one sentence pair sum error compute word proceed next word use correct word condition context decoder hidden state word prediction train objective base probability mass give correct give perfect context attempt use different training yet show superior practical training neural machine translation model require well suit high degree parallelism inherent deep learning model think many matrix increase parallelism even process several sentence pair implies increase state tensor give example represent input word sentence pair vector since already sequence input line matrix process batch sentence line matrix dimensional tensor give another decoder hidden state vector output word since process batch line hide state matrix note case helpful line state output since state compute sequentially recall computation attention mechanism pas computation gpu matrix encoder state dimensional tensor input result matrix attention value sentence one dimension input due massive use value well inherent parallelism show true power may feel create glare contradiction argue process one training example since sentence pair typically different neural translation model house big input word leave right recurrent right leave recurrent attention input context hidden state output word prediction error give output word output word embed figure fully unrolled computation graph train example input token house big output token cost function compute output word sum across sentence walk correct previous output word use condition context chapter neural machine translation figure make good use parallelism process batch training example time convert batch training example set mini batch similar length waste less computation word hence computation graph different size argue sentence pair together well exploit parallelism indeed goal see figure batch training example consider maxi mum size input output sentence batch unroll computation graph maximum sizes shorter remain gap non word keep track valid data mask ensure attention give word beyond length input error gradient update compute output word beyond length output sentence avoid waste computation nice trick sort sentence pair batch length break mini batch similar length training consist follow step train corpus avoid undue bias due temporal topical break corpus maxi batch break maxi batch mini batch process gather gradient apply gradient maxi batch update parameter train neural machine translation model take epochs entire training common stop criterion check progress model val set part training halt error validation set improve training longer would lead improvement may even degrade performance due beam search translate neural translation model proceed one step time predict one output word compute probability distribution word bite confusion technical term entire training corpus call batch use contrast batch update online update small batch subset call mini batch section page use term batch maxi batch mini batch subset subset neural translation model context cat state prediction word fish select word dog embed figure elementary decoding model predict word prediction probability distribution select likely word embed part condition context next word prediction decoder pick likely word move next prediction step since model condition previous output word equation use word embed condition context next step see figure illustration time obtain probability distribution word distribution often quite word maybe even one word amass almost probability word receive high pick output word real example neural machine translation model translate german sentence english show figure model tend give almost mass top sentence translation also indicate word choice believe v think different v various also ambiguity grammatical sentence start discourse connective subject process suggest perform best greedy search make u vulnerable call garden path problem sometimes follow sequence word realize late make mistake early best sequence consist less probable word initially redeem subsequent word context full output consider case produce idiomatic phrase word phrase may really odd word choice piece cake easy full phrase choice redeem note face problem traditional statistical machine translation model arguable even since rely sparse context make next word decode algorithm model keep list best candidate hypothesis expand keep best expanded hypothesis neural translation model chapter neural machine translation input sentence er clever um seine art output word prediction best alternative however yet also also think believe believe think feel clever smart enough around keep maintain hold make statement statement testimony message comment vague ambiguous enough may could might interpret get interpret differently different various several way way way manner figure word predictions neural machine translation model mass give top semantically related word may rank believe v think unit explain section page neural translation model cat fish dog cat cat cat dog cat figure beam search neural machine translation commit short list output word beam new word prediction make differ since commit output word part condition context make prediction predict word output keep beam top likely word choice score probability use word beam condition context next word due make different word prediction multiply score partial translation point probability probability word prediction select high score word pair next beam see figure illustration process continue time accumulate word translation give u score hypothesis sentence translation end sentence token produce remove complete hypothesis beam reduce beam size search hypotheses leave beam search produce graph show figure start start sentence symbol path terminate end sentence symbol give compete result translation obtain follow back pointer complete hypothesis one end high score point best translation choose among best score product word prediction probability get good result normalize score output length divide number word carry normalization search complete translation beam normalization would make difference note traditional statistical machine able combine share condition context future feature function possible anymore recurrent neural network since condition entire output word sequence begin search graph generally less diverse search chapter neural machine translation figure search graph beam search decode neural translation model time best partial translation select output sentence complete end sentence token predict reduce beam terminate full sentence translation complete follow back pointer end sentence token allow u read empty box represent hypothesis part complete path graph statistical machine translation model really search tree number complete path size beam reading attention model root sequence sequence model use recurrent neural network approach use short term network reverse order source sentence decode seminal work add alignment model call link generate output word source include condition hidden state produce precede target word source word represent two hidden state recurrent neural network process source sentence leave right right leave propose variant attention mechanism call attention also hard constraint attention model attention restrict gaussian distribution around input word explicitly model trade source context input target context already produce target tu introduce interpolation weight scale impact source context state previous hidden state last word predict next hidden state decoder tu augment attention model reconstruction step generate output translate back input language training objective extend include likelihood target sentence also likelihood reconstruct input sentence previous section give comprehensive description currently commonly use basic neural translation model architecture perform fairly well box many refinement checkpoint ensemble multi run ensemble figure two method generate alternative system checkpoint use model dump various stage train multi run start dent training run different initial weight order training data language pair since number propose describe section fairly target particular use case data give one best performing system recent evaluation campaign use ensemble decode byte pair encode address large add synthetic data derive monolingual target side data use deep model ensemble decode common technique machine learn build one system multiple one combine call ensemble system successful strategy various method propose systematically build instance use different feature different subset data neural one straightforward way use different stop different point train process intuitive argument system make different mistake two system likely rather make mistake one also see general principle play human set committee make decision democratic voting election apply ensemble method case neural machine address two generate alternate combining output generate alternative system see figure illustration two method generate alter native system train neural translation iterate training data stop criterion meet typically lack improvement cost function apply validation set translation performance validation set dump model interval every iteration batch training look back performance chapter neural machine translation model model model model model average cat fish dog figure combine prediction ensemble model independently predict probability distribution output average combined distribution model different stage pick model best performance translation quality measure call checkpoint since select model different checkpoint train process multi run require build system completely different training run mention accomplished use different random initialization lead train seek different local optimum also randomly train use different random order also lead different training outcome multi run usually work good deal also computationally much expensive note multi run also build checkpoint instead combine end point apply checkpoint combine ensemble combine system output neural translation model allow combination several system fairly deeply recall model predict probability distribution possible output com one word combine different train model model predict probability distribution combine prediction combination do simple average distribution average distribution basis select output word see figure illustration may weigh different system although way generate similar typically do refinement right leave decode one tweak idea instead build multiple system different random also build one set system second set system reverse order output sentence second set system call right leave although arguably good name since make sense language arabic hebrew normal write order right leave deep integration describe work anymore combination leave right right leave since produce output different order resort involve several use ensemble leave right system generate best list candidate input sentence score candidate translation individual leave right right leave combine score different model select candidate best score input sentence score give candidate translation right leave system require require forced decode special mode run inference input predict give output sentence mode actually much closer training also output translation regular inference large vocabulary law tell u word language unevenly distribution always large tail rare word new word come language time wake also deal large inventory include company names microsoft neural method well equip deal large ideal representation neural network continuous space vector convert discrete object word word ultimately discrete nature word show input need train embed matrix map word embed output side predict probability distribution output word latter generally big since amount computation involve linear size make large matrix operation neural translation model typically restrict vocabulary word initial work neural machine frequent word others represent unknown tag translation rare word handle back dictionary chapter neural machine translation obama receive relationship obama exactly friendly two want talk implementation international agreement activity middle east meeting also plan cover palestinian dispute two state solution relation obama year washington continuous building settlement israel use lack initiative peace process relationship two deteriorate deal obama negotiate iran atomic march invitation an make controversial speech u congress partly see obama speech agree obama reject meeting reference election time pending israel figure byte pair encode applied english use word split indicate note data also true case common approach today break rare word unit may seem bit crude actually similar standard approach statistical machine translation handle compound website web site morphology follow convolution convolution even decent approach problem transliteration name traditionally handle sub modular letter translation component popular method create inventory unit legitimate word byte pair encode method train parallel corpus word corpus split character original space special space frequent pair character merge may step repeat give number time step increase vocabulary beyond original inventory single character example mirror quite well behavior algorithm real world data set start group together frequent letter combination join frequent word end frequent word emerge single rare word consist still see figure unit indicate two symbol byte pair encode vast majority word rarer word break split seem motivate pending mostly note also decomposition relatively rare name reading limitation neural machine translation model burden support large vocabulary avoid typically vocabulary reduce refinement shortlist remain token replace unknown word ken translate unknown jean resort separate dictionary arthur argue neural translation model bad rare word interpolate traditional probabilistic bilingual dictionary prediction neural machine translation model use attention mechanism link target word distribution source word weigh word translation accordingly source word name number may also directly copy target use call switching network predict either traditional translation operation copying operation aid layer source sentence training data change target word word position copy source word augment word prediction step neural translation model either translate word copy source word observe attention mechanism mostly drive semantics language model case word location case copy speed mi use traditional statistical machine translation word phrase translation model target vocabulary mini batch split word sub word use character gram model segmentation base byte pair encode compression algorithm use monolingual data key feature statistical machine translation system language train large monolingual data set large language high translation quality language model train trillion word crawl general web use surprise basic neural translation model use additional monolingual language model aspect condition previous hidden decoder state previous train jointly translation model aspect input two main idea propose improve neural translation model data one transform additional monolingual translation parallel data syn miss half integrate language model component neural network architecture back translation language model improve output use large amount monolingual data target language give machine evidence common sequence word can not use monolingual target side data neural translation model since miss source side one idea synthesize data back translation see figure illustration step involve train reverse system translates intend target language source language typically use neural machine translation setup source target may use even traditional phrase base system chapter neural machine translation reverse system system figure create synthetic parallel data target side monolingual train system reverse use translate target side monolingual data source combine generate synthetic parallel data true parallel data system building use reverse system translate target side monolingual create synthetic parallel corpus combine generate synthetic parallel data true parallel data building system open question much synthetic parallel data use relation amount exist true parallel data magnitude monolingual data also want drown actual real data successful idea use equal amount synthetic true data may also generate much synthetic parallel ensure train process equal amount sample true parallel data add language model idea train language model separate component neural translation model train large language model recurrent neural net work available include target side parallel corpus add language model neural translation model since language model translation model predict output natural point connect two model join output prediction node network concatenate condition contexts expand equation add hidden state neural language model hidden state neural translation model tm source context previous english word tm training combine leave parameter large neural language model update parameter translation model layer concern otherwise output side parallel corpus would overwrite refinement mt mt figure round trip addition train two model do traditionally parallel also optimize model convert sentence restore back use monolingual data may add correspond round trip start memory large monolingual corpus language model would parallel train data less general one question much weight give translation model much weight give language equation considers instance way may output word translation model relevant translation content word distinct output word language model relevant introduction relevant function word balance translation model language model achieve type gate unit encounter discussion long short term memory neural network architecture gate unit may predict solely model state use factor multiply language model state use prediction equation gate gate tm round trip training look idea strict machine learn see two learn objective objective learn transformation give parallel do traditionally goal learn convert output sentence input back output language objective match traditional sentence good machine translation model able preserve meaning output language sentence map input language back see figure illustration two machine translation model one translate sentence language direction opposite direction two system may train traditional use parallel corpus also round trip sentence back two objective model train chapter neural machine translation translation give monolingual sentence valid sentence language measure language model reconstruction translation back original language measure translation model mt two objective use update model parameter translation model mt mt typical model update drive correct prediction word round trip translation compute usual training model mt give sentence pair make good use training best list translation compute model update compute also update model mt monolingual data language scaling date language model cost forward translation cost mt translation best list use monolingual data language training do reverse round trip direction detail refer reading back translate monolingual data input use obtain synthetic parallel corpus additional training data use monolingual data dual learning setup machine translation engine train addition regular model training parallel monolingual data translate round trip evaluate language model language reconstruction match back cost function drive gradient descent update model deep model learn lesson research vision speech recent work machine translation also look deep model simply involve add intermediate layer baseline architecture core component neural machine translation encoder take input word convert sequence contextualized representation decoder gen output sequence word recurrent neural network recall already discuss build deep recurrent neural network back section page extend idea recurrent neural network encoder decoder recurrent neural network common process input sequence output time step information new input combine hidden state previous time step predict new hidden state hidden state additional prediction may make word case next word sequence case language hidden state use otherwise attention mechanism case refinement context decoder stack transition decoder stack transition decoder stack transition decoder stack transition figure deep instead single recurrent neural network layer decoder deep consist several layer illustration show combination deep transition stack omits word word selection output word embed step identical original show figure page decoder see figure part decoder neural machine use par deep architecture see instead single hidden state give time step sequence hidden state give time step various option hide state may connect sec present two idea stack recurrent neural network hide state condition hidden state previous layer hidden state depth previous time step deep transition recurrent neural net hidden state condition last hidden state previous time step hidden layer condition previous previous layer figure combine two idea layer stack previous time step previous layer others deep transition previous layer break stack layer deep transition layer either chapter neural machine translation input word embed encoder layer encoder layer encoder layer encoder layer figure deep alternating combination idea bidirectional recurrent neural network previously propose neural machine translation figure page stack recurrent neural network figure page architecture may extend idea deep show decoder figure function compute sequence function call function may implement feed forward neural network layer multiplication plus activation long short term memory cell gate recurrent unit either function set trainable model parameter encoder deep recurrent neural network encoder may draw idea one baseline neural translation use bidirectional recurrent neural network condition leave right context want deep version encoder figure show one idea could call alternate recurrent neural network look basically like stack recurrent neural one hidden state layer alternately condition hidden state previous time step next time step formulate even number hidden state condition leave context odd number hidden state condition right context extend idea deep transition note deep model typically augment direct connection input output case may mean direct connection embed encoder connection layer pas input directly output refinement die obama figure alignment v alignment point traditional word align method show attention state shade box depend alignment value generally match note instance prediction output auxiliary verb pay attention entire verb group strain residual connection help train early deep architecture skip basic function model deep architecture exploit enrich typically see residual connection early train stage initial reduction model le improvement converge model reading recent work show good result stack deep transition encoder well alternate network encoder large number variation use skip choice v number layer still need explored empirical various data condition guide alignment training attention mechanism neural machine translation model motivate need align output word input word figure show example attention weight give english input word german output word translation sentence attention value typically match pretty well word alignment use statistical machine obtain tool fast align implement variant ibm model several good us word alignments beyond intrinsic value improve quality translation instance next look use attention chapter neural machine translation mechanism explicitly track coverage input may also want override preference neural machine translation model translation certain terminology expression measurement well handle rule base require know neural model translate source word also end user may interest alignment translator use machine translation computer aid translation tool may want check output word originate instead trust attention mechanism implicitly acquire role word may enforce role idea provide parallel corpus train also word alignment use traditional mean additional information may even train model converge faster overcome data sparsity low resource condition straightforward way add give word alignment training process change model modify train objective goal train neural machine translation model generate correct output word add goal also match give word alignment assume access alignment matrix alignment point input word output word way output word alignment score add model estimate attention score also add output equation page mismatch give alignment score compute attention score measure several cross entropy cost ce log mean square error cost cost add train objective may weight reading chen add supervised word alignment information traditional statistical word alignment training augment objective function also optimize match attention mechanism give alignment model coverage one impressive aspect neural machine translation model well able trans late entire input even lot reorder involved aspect occasionally model translates input word multiple sometimes miss translate refinement um problem figure example generation input tokens around social housing attend lead hallucinate output word company end sentence fresh start attend untranslated see figure example translation two related attention begin phrase alliance receives much result faulty translation hallucinate company society social education end input phrase fresh start receive attention hence untranslated output obvious idea strictly model coverage give attention reasonable way coverage add attention state complete sentence roughly expect input word receive similar amount attention input word never receive attention much attention signal problem translation enforce coverage inference may restrict enforce proper coverage decoder consider multiple hypothesis beam age one pay much attention input word hypotheses penalize pay little attention input various way come score function generation generation chapter neural machine translation coverage generation max generation min coverage coverage use multiple score function decoder common practice traditional machine translation neural machine translation challenge give proper weight different score function two three optimize grid search possible value may borrow method mira statistical machine translation coverage model vector accumulate coverage input word may directly use inform attention model attention give input word condition previous state decoder representation input word also add condition context accumulated attention give word equation page coverage coverage tracking may also integrate train objective take page guide alignment train previous section augment training function coverage penalty weight log coverage note problematic add additional function learn ob since distract main goal produce good translation fertility describe coverage need cover input word roughly evenly even early statistical machine translation model consider fertility number output word generate input word consider english language require equivalent negate verb word translate multiple output word german may translate course thus generate output word may augment model coverage add fertility component predict number output word input word one example model predict fertility input use normalize coverage statistic coverage refinement fertility predict neural network layer condition input word representation use activation function result value scale maximum fertility feature engineering versus machine learn work model coverage neural machine translation model nice example contrast engineering approach belief generic machine learn technique engineering good way improve system analyze weak point consider change overcome notice generation generation respect add component model overcome problem proper coverage one feature good translation machine learn able get training data able may need deep robust estimation way adjustment give right amount power need problem hard carry analysis need make generic machine learn give complexity task like machine translation argument deep learning require feature add coverage model remain see neural machine translation evolve next move engineering machine learn direction reading well model tu add coverage state input word either sum attention scale fertility value predict input word learning coverage update function feed forward neural network layer coverage state add additional conditioning context prediction attention state condition prediction attention state also previous context state also introduce coverage state sum input source aim subtract cover word step separate hide state keep track source coverage hide state keep track produce output add number bias model alignment inspired traditional statistical machine translation model condition prediction attention state absolute word attention state previous output word limit coverage attention state limit window also add fertility model add coverage train objective adaptation text may differ degree common problem practical development machine translation system available train data different data relevant choose use case goal translate chat room realize little translated chat room data available massive quantity publication international random translation crawl maybe somewhat relevant movie subtitle translation chapter neural machine translation general training data initial train general system domain train data adaptation adapt system figure online train neural machine translation model allow straightforward domain adaptation general domain translation system train general purpose handful additional training epoch domain data allow domain adapted system problem generally frame problem domain adaptation simple one set data relevant use case domain data another set less relevant domain data traditional statistical machine vast number method domain propose model may may back domain domain may sample domain data train sub sample domain etc neural machine fairly straightforward method currently pop figure method divide train two stage train model available data convergence run iteration train domain data stop training performance domain validate set peak model training still specialize domain data practical experience method show second domain training stage may converge quickly amount domain data typically relatively handful train epoch need less commonly use method draw idea ensemble decode train separate model different set may combine ensemble decode want choose weight although choose weight trivial task domain domain may simply do line search possible value let u look special case arise practical use domain data large collection common problem amount available domain data train even secondary adaptation risk good performance see data poor everything else refinements large random collection parallel text often contain data closely match domain data may want extract domain data large collection mainly domain data general idea behind variety method build two one domain detector train domain one domain detector train domain data score sentence pair domain data detector select sentence pair prefer judged relatively domain detector classic detector language model train source target side domain domain result total language source side domain model target side domain model source side domain model target side domain model give sentence pair domain data score base relevance may use traditional gram language model neural recurrent language model work suggest replace open class word part speech tag word cluster sophisticated model consider domain relevance noisiness training data misalign mistranslate may even use domain domain neural translation model score sentence pair stead source target side sentence isolation data may use several way may train data build system use secondary adaptation stage outline monolingual domain data parallel data domain use two main idea explore may still use monolingual may source target language parallel data large pile general outline another idea use exist parallel data train domain back translate domain data section generate synthetic domain use data adapt initial model traditional statistical machine much adaptation success achieve interpolating language idea neural translation equivalent multiple domains multiple collection data clearly domain typically category information etc use technique describe build specialized translation model domains give test select appropriate model know domain test build allow u automatically chapter neural machine translation make determination may base method domain detector describe give decision select appropriate model commit single domain may instead provide distribution relevance domain model domain domain domain use weight ensemble model domain may do base whole document instead individual brings context make robust decision hard give conclusive advice handle adaptation since broad topic style text may relevant content data may differ narrowly publication united nation vs european dramatically chat room v publish amount domain domain data differs data may cleanly separate domain come massive disorganize pile data may higher translation quality may pollute noise even generate machine translation system reading often domain mismatch bulk even train data translation test data deployment rich literature traditional statistical machine translation topic common approach neural model train available training run iteration domain data already pioneer neural language model adaption demonstrate effectiveness adaptation method small domain set consist little sentence pair argue give small amount domain data lead suggest mix domain domain data adaption identify problem suggest use ensemble baseline model adapt model avoid consider alternative training method adaptation phase consistently well result traditional gradient descent training inspire domain adaptation work statistical machine translation sub sample sentence chen build domain v domain sentence pair training use prediction score reduce learn rate sentence pair domain show traditional statistical machine translation outperform neural chine translation train general purpose machine translation system collection test niche domain adaptation technique allow neural machine translation catch multi domain model may train informed run time domain input sentence apply idea initially propose augment input sentence register politeness feature token domain adaptation problem add domain token training test sentence chen report well result token approach adapt topic encode give topic membership sentence additional input vector condition context word prediction layer refinement add linguistic annotation one big debate machine translation research question key progress develop relatively machine learn method implicitly learn important feature use linguistic insight augment data model recent work statistical machine translation demonstrate motivated model best statistical machine translation system major evaluation campaign language pair syntax base translate also build syntactic structure output sentence serious effort move towards deeply semantics machine translation turn towards neural machine translation hard swing back towards well machine learn ignore much linguistic insight neural machine translation view translation generic sequence sequence happen involve sequence word different language method byte pair encode character base model even put value concept word basic unit doubt recently also attempt add linguistic annotation neural translation step towards linguistically motivate model take look successful effort integrate linguistic annotation input linguistic annotation output build linguistically structured model linguistic annotation input one great neural network cope rich context neural machine translation model word prediction condition entire input sentence previously generate put word even typically input sequence partially generate output sequence never observe neural model able generalize training data draw relevant knowledge traditional statistical mod require carefully choose independence assumption back scheme add information condition context neural translation model accommodate rather straightforwardly information would like typical linguistic treasure chest contain part speech morphological property syntactic phrase syntactic maybe even semantic annotation format annotation individual input word require bit syntactic semantic annotation span multiple word see figure example walk linguistic annotation word girl part speech noun lemma girl surface form lemma differs watch watch morphology singular chapter neural machine translation word girl watch attentively beautiful part speech adv lemma girl watch attentive beautiful morphology sing past plural noun phrase begin cont begin cont cont verb phrase begin cont cont cont cont dependency girl watch watch watched depend relation subj adv adj obj semantic role actor manner mod patient semantic type human view animate figure linguistic annotation format word level factor representation word continuation cont noun phrase start word part verb phrase syntactic head watch dependency relationship head subject subj semantic role actor many scheme semantic type instance girl could man note phrasal annotation handle noun phrase girl common use annotation scheme tag individual word phrasal begin continuation intermediate word outside phrase encode word level factor recall word initially represent hot vector encode factor factored representation hot vector concatenation vector use input word embed note mathematically factor representation map embed word embed sum factor since input neural machine translation system still sequence word embed change anything architecture neural machine translation model provide rich input representation hope model able learn take advantage come back debate linguistics versus machine learning linguistic notation propose arguable learn automatically part word contextualized word hide encoder may may true provide additional knowledge come tool produce notation particularly relevant enough training data automatically induce make job hard machine learn algorithm force machine learn discover feature readily refinement sentence girl watch attentively beautiful syntax tree np vp girl watch adv np attentively beautiful np girl vp watch adv attentively np beautiful figure phrase structure grammar tree sequence word watch tag np question resolve empirically demonstrate actually work data condition linguistic annotation output do input word could do also output word instead discuss point adjustment need make separate output let u take look another annotation scheme output successfully applied neural machine translation syntax base statistical machine translation model focus add syntax output side traditional gram language model good promoting among neighbor powerful enough ensure overall output sentence design model also produce evaluate syntactic parse output syntax base model give mean promote grammatically correct output word level annotation phrase structure syntax suggest figure rather crude nature language annotate nested phrase can not easily handle begin cont scheme typically tree structure use represent syntax see figure example show phrase structure syntactic parse tree example sentence girl watch attentively beautiful generate tree structure generally quite different process generate sequence typically build recursively bottom algorithms chart parse parse tree sequence word structural token indicate begin np end closing parenthesis syntactic phrase force syntactic parse tree annotation sequence sequence neural chine translation model may do encode parse structure additional output token perfectly idea produce output neural translation sequence sequence mix output word special token chapter neural machine translation hope force neural machine translation model produce syntactic structure encourage produce syntactically well form output evidence support despite simplicity approach linguistically structured model syntactic parsing leave untouched recent wave neural network previous section suggest syntactic parsing may do simply frame sequence sequence additional output token best perform syntactic use model structure take recur nature language heart either inspired network build parse tree neural version leave right push main stack open phrase new word may extend push stack start new phrase early work integrate syntactic parsing machine translation framework consensus best practice emerge yet time clearly still challenge future work reading wu propose use factor representation word part factor encode one hot input recurrent neural network language model use representation input output neural machine translation demonstrate good translation quality multiple language pair two language world also train data many language sometimes highly overlap european parliament proceeding sometimes unique canadian french language lot train data available language include commercially interesting language pair long history move beyond language encode mean language sometimes call machine idea map input language map output language build one map step one step language translate languages do researcher deep learn often hesitate claim intermediate state neural translation model encode semantics mean train neural machine translation system accept text language input translate refinement multiple input language let u two parallel one one train neural machine translation model corpora time simply concatenate input vocabulary contains german french word input sentence quickly recognize either german due sentence word combine model train data set one advantage two separate mod expose english side parallel corpora hence learn well language model may also general diversity lead robust model multiple output language trick output corpus give french input sentence would system know output language crude effective way signal model add tag like spanish token input sentence english pa case double spanish pa verse con train system three corpus mention also use translate sentence german spanish without ever present sentence pair train data system spanish verse con representation mean input sentence tie input language output language experiment show actually somewhat achieve good parallel data desire language pair much less standalone model figure summarize idea single neural machine translation train parallel corpus result system may translate see input output language likely increasingly deep model section may better serve multi language since deep layer compute abstract rep language idea mark output language token spanish explore widely context system single language pair token may represent domain input sentence require level politeness output sentence chapter neural machine translation french german mt english spanish figure multi language machine translation system train one language pair rotate many train even able translate german spanish share component instead throw data generic neural machine translation may want carefully consider component may share among model idea train one model per language component identical unique model encoder may share model input language decoder may share model output language attention mechanism may share model language pair share component mean parameter value use separate model update train model one language pair also change model language pair need mark output since model train language pair idea share training component also push exploit mono lingual data encoder may train monolingual input language need add train objective language model decoder may train isolation monolingual language model data since context state blank may lead learn ignore input sentence function target side language model reading johnson explore well single canonical neural translation model able learn multiple multiple simultaneously train parallel corpora several language pair show small several input language output mixed result translate multiple output language additional input language interesting result ability model translate language direction parallel corpus thus demonstrate mean representation although less well use traditional pivot method support multi language input output train encoders decoder share attention mechanism alternate architecture input word layer layer layer figure encode sentence neural network always use two size convolution differ decode revers process alternate architectures neural network research focus use recurrent neural network attention mean architecture neural network vantage use recurrent neural network input side require long sequential process consumes input word one step also prohibit ability processing word thus limit use capability alternate suggestion architecture neural machine model present section remain curiosity conquer neural network end end neural machine translation model modern era actually base recurrent neural base neural network show successful image thus look application natural next step see figure illustration network encodes input sen basic building block network convolution merges representation input word single representation use matrix apply convolution every sequence input word reduce length sentence representation repeat process lead sentence representation single vector illustration show architecture two follow layer merges sequence phrasal representation single sentence size kernel depend length sentence example show word sentence sequence layer longer big kernel need hierarchical process build sentence representation bottom well ground linguistic insight recursive nature language similar chart except commit single hierarchical structure ask chapter neural machine translation input word encode layer encode layer transfer layer decode layer decode layer select word output word embed figure neural network model convolution result single sentence embed sequence encoder also inform recurrent neural network output word decode layer awful lot result sentence embed represent mean entire sen arbitrary length generate output sentence translation reverse bottom process one problem decoder decide length output sentence one option address problem add model predicts output length input length lead selection size reverse convolution matrix see figure illustration variation idea show architecture always use result sequence phrasal single sentence embed explicit mapping step phrasal representation input word phrasal representation output call transfer layer decoder model include recurrent neural network output side sneak recurrent neural network undermine bit argument good claim still hold true encode sequential language model powerful tool disregard describe neural machine translation model help set scene neural network approach machine could demonstrate achieve competitive result compare traditional approach compression sen representation single vector especially problem long sentence model use successfully candidate translation generate traditional statistical machine translation system alternate architecture input word convolution layer convolution layer convolution layer figure encoder use stacked layer number layer may use neural network attention propose architecture neural network combine idea neural network attention mechanism essentially sequence sequence attention describe canonical neural machine translation recurrent neural network replace layer introduce convolution previous section idea combine short sequence neighboring word single representation look another convolution encode word leave right limit window let u describe detail mean encoder decoder neural model encoder see figure illustration layer use encoder input state layer inform correspond state layer two neighbor note layer shorten convolution center around use pad zero word position bound start input word ex progress sequence layer different depth maximum depth function feed forward residual connection correspond previous layer state note even representation word may inform partial sentence context contrast bi directional recurrent neural network canonical model relevant context word input sentence help disambiguation may outside window computational advantage idea word one depth process even combine one massive tensor operation gpu chapter neural machine translation input context output word prediction decoder convolution decoder convolution output word embed select word figure decoder neural network attention decoder state compute sequence layer already predict output word state also inform input context compute input sentence attention decoder decoder canonical model also core recurrent neural network recall state progression equation page encoder embed previous output input context version recurrent decoder depend previous state condition sequence recent previous word decoder convolution may encoder layer see figure illustration equation main difference canonical neural machine translation model architecture condition state decoder compute sequence also always input context attention attention mechanism essentially unchanged canonical neural trans model recall base association word compute encoder previous state decoder back equation page alternate architecture since still encoder decoder state use association score normalize use compute weight sum input word encoder state encoder state input word embed combine via addition compute context vector usual trick use residual connection assist train deep neural network self attention critique use recurrent neural network require lengthy word entire input time consume limit previous section replace recurrent neural network canonical model con limit context window enrich representation word would like architectural component allow u use wide context highly could already encounter attention mechanism considers association tween every input word output use build vector representation entire input sequence idea behind self attention extend idea encoder instead compute association input output self attention com association input word input word one way view mechanism representation input word enrich context word help disambiguate compute self attention self attention sequence vector size pack matrix self attention let u look equation detail association every word representation context word do via dot product pack matrix transpose result vector raw association value value vector scaled size word representation vector value add result vector normalize association value use weigh context word another way put equation without matrix notation use word vector exp exp normalize association self attention weight sum raw association chapter neural machine translation self attention layer self attention step describe one step self attention layer use encode input sentence four step follow combine self attention residual connection pass word representation directly self attention next layer normalization step section page layer normalization self attention standard feed forward step activation function apply also augment residual connection layer normalization layer normalization take page deep stack several layer top ex start input word embed self attention layer deep modeling reason behind residual connection self attention layer residual connection help train since allow shortcut input may utilize early stage take advantage complex deep model enable layer normalization step one standard train trick also help especially deep model attention decoder self attention also use output word decoder also traditional attention total sub layer self output word initially encode word per form exactly self attention computation describe equation association word limit word previously produce output word let u denote result sub layer output word attention mechanism model follow closely self attention difference compute self attention hide state compute attention decoder state encoder state attention sh alternate architecture input word self attention layer self attention layer decoder layer decoder layer output word prediction select output word output word embed figure attention base machine translation input encode several layer self attention decoder compute attention base representation input several initialize previous word use detailed exposition exp attention weight sum attention computation augment add residual layer additional like self attention layer describe worth note output attention computation weight sum input word representation add representation decoder state via residual connection allow skip deep thus speed train feed forward sub layer identical sub layer follow add norm step use residual layer normalization note description attention sub entire model show figure raw association sh exp normalize association chapter neural machine translation reading build comprehensive machine translation model encode source sentence neural generate target sentence reverse process propose use multiple layer encoder decoder reduce length encode sequence incorporate wider context layer replace recurrent neural network use sequence sequence model multiple self attention encoder well decoder number additional call multi head encode sentence position etc current challenge neural machine translation emerge promising machine translation approach recent show superior performance public benchmark rapid adoption deployment google also report poor system build low resource condition lorelei pro gram examine number challenge neural machine translation give empirical result well technology currently hold compare traditional statistical chine translation show despite recent neural machine translation still overcome various notably performance domain low resource condition lot problem common neural translation model show robust behavior confront condition differ train condition may due limited exposure train unusual input case domain test unlikely initial word choice beam search solution problem may hence lie general approach training step outside optimize single word prediction give perfectly match prior sequence another challenge examine neural machine translation much less answer question training data lead system decide word choice decode bury large matrix real numbered value clear need develop well neural machine translation use common neural machine translation traditional phrase base statistical machine translation common data drawn opus unless note use default beam search single model decode train data process byte pair encode word vocabulary limit evaluation current challenge statistical machine translation system train use moses build phrase base system use standard feature commonly use recent submission ding consider phrase base note statistical machine translation hierarchical phrase base model syntax base model show give superior performance language pair carry experiment large train data set available use share translation task organize alongside conference machine translation domain use opus corpus except domain use test set compose news characterize broad range formal relatively long sentence word high standard style domain mismatch know challenge translation different word different trans mean express different style crucial step develop chine translation system target use case domain adaptation expect method domain adaptation developed neural machine translation currently popular approach train general domain follow train domain data epochs large amount train data available still seek robust performance test well neural machine translation statistical machine translation hold trained different system use different corpus obtain opus additional system train train data statistic corpus size show table note domains quite distant much ted news global voice train statistical machine translation neural machine translation system domains system train tune test set sub sample data use common byte pair encode use training run see figure result domain neural statistical machine translation system similar machine translation well statistical machine translation well domain performance machine translation system bad almost sometimes dramatically use customary domain machine domain corpus may differ domain level etc chapter neural machine translation corpus word sentence law medical koran subtitle table corpus use train take opus repository corpus system law medical koran subtitle data law medical koran subtitle figure quality system train one domain test another domain neural machine translation system show degraded performance domain current challenge source um reference look around look around look around law order implement medical mb en final work around switch pause koran take heed soul see subtitle look around look around figure examples translation sentence subtitle translate system train different corpus performance domain dramatically bad neural machine translation instance medical system lead score machine v machine law test set figure display example translate sentence um look around subtitle see mostly completely unrelated output neural machine translation system translation system switch pause note output neural machine translation system often quite take heed soul completely unrelated statistical machine translation output betrays cop domain input leave word untranslated around particular concern mt use information user mislead hallucinate content neural machine translation output amount training data well know property statistical system increase amount train data lead well result statistical machine translation previously observe double amount training data give increase score hold true parallel monolingual data irvine data need statistical machine translation neural machine translation neural machine translation promise generalize good word sim condition large context input prior output chapter neural machine translation score vary amount train data phrase base big phrase base neural corpus size figure score english spanish system train million million word parallel data quality neural machine translation start much outperforms statistical machine translation million even beat statistical machine translation system big billion word domain language model high resource condition current challenge ratio word republican strategy counter election obama figure translation sentence test set use neural machine translation system train vary amount train data low resource neural machine translation produce output unrelated input build english spanish system million english word pair machine language model train spanish part respectively addition neural statistical machine translation system train also use additionally provide monolingual data big language model statistical machine translation system result show figure neural machine translation exhibit much steep outperform statistical machine translation v data million even beat statistical machine translation system big language model full data set neural machine statistical machine statistical big language contrast neural statistical machine translation learn curve quite striking neural machine translation able exploit increase amount train data unable get ground training corpus size million word less strategy election start translation become respectable noisy data statistical machine translation fairly robust noisy data quality system hold fairly even large part train data corrupt various align content wrong badly translate etc statistical chine translation model build probability distribution estimate many occur word phrase unsystematic noise training affect tail end distribution spanish last represent use data chapter neural machine translation ratio table impact noise training part train corpus contain sentence pairs neural machine translation degrade statistical machine translation hold fairly well still case neural machine chen consider one kind misalign sentence pair experiment large parallel corpus target side part train sentence pair table show result statistical machine translation system hold fairly well even data quality drop expect half valid training data neural machine translation system degrade drop com par point drop statistical system possible explanation poor behavior neural machine translation model prediction good balance language model input context main driver training observe increase ratio train input sen meaningless may generally learn rely output language model hence hallucinate inadequate output word alignment key contribution attention model neural machine translation imposition alignment output word input word take shape probability distribution input word use weigh bag word representation input sentence attention model functionally play role word alignment source least way analog statistical machine translation alignment latent variable use obtain probability distribution word arguably attention model broad role translate attention may also pay subject object since may disambiguate complicate word representation product bidirectional gated recurrent neural network effect word representation inform entire sentence context clear need alignment mechanism source target word prior work use alignment provide attention model interpolate word translation decision traditional probabilistic dictionary introduction coverage fertility model etc current challenge die relationship obama obama stretch year desire alignment mismatch alignment figure word alignment compare attention model state box probability percent alignment obtain fast align attention model fact proper examine compare soft alignment matrix sequence attention word alignment obtain traditional word alignment method use incremental fast align align input output neural machine system see figure illustration compare word attention state word alignment obtain fast align match pretty well attention state fast align alignment point bite fuzzy around function word attention model may settle alignment correspond tuition alignment point obtain fast align see figure reverse language alignment point appear one position aware intuitive explanation divergent behavior translation quality high system measure well soft alignment neural machine system match alignment fast align two match score check output align input word accord fast align indeed input word receive high attention probability mass score sum probability mass give alignment point obtain fast align chapter neural machine translation language pair match prob table score indicate overlap attention probability alignment obtain fast align handle byte pair encode many many alignment use neural machine translation model provide edinburgh run fast align parallel data set obtain alignment model use align input output neural machine translation system table show alignment score system result suggest divergence outlier see large divergence also different data condition note attention model may produce good word alignment guide alignment training supervise word alignment one produce provided model train beam search task decode full sentence translation high probability machine problem address heuristic search technique explore subset space possible translation common feature search technique beam size parameter limit number partial translation maintain per input word typically straightforward relationship beam size parameter model score result translation also quality score diminish return increase beam typically improvements score expect large beam decode neural translation model set similar fashion predict next output may commit high score word prediction neural machine translation operate fast align run full word input word split byte pair add attention score output word split take average attention vector match score probability mass score compute average output word level score output word fast align alignment ignore computation output word fast align multiple input match count correct align word among top high scoring word accord attention probability mass add attention score current challenge english czech normalize normalized beam size beam size normalized beam size normalized beam size normalized normalized beam size beam size normalized normalized beam size beam size figure translation quality vary beam size large quality especially normalizing score sentence length chapter neural machine translation also maintain next best scoring word list partial translation record partial translation word translation probability extend partial translation subsequent word prediction accumulate score since number partial translation explodes exponentially new output prune beam high score partial translation traditional statistical machine translation increase beam size allow u explore large set space possible translation hence translation well model score figure increase beam size consistently improve translation quality almost bad translation find beyond optimal beam size set use edinburgh optimal beam size varies around normalize sentence level model score length output alleviates problem somewhat also lead well optimal quality case language pair optimal beam size range almost quality still drop large beam main cause deteriorate quality short translation wider beam reading study look comparable performance neural statistical machine translation system consider different linguistic category german compare different broad aspect reorder nine language direction additional topic especially early work neural network machine translation aim build neural compo use traditional statistical machine translation system translation model include align source word condition enrich feed forward neural network language model source context add sentence embed conditional context learn use variant neural network map across language use complex neural network encode input sentence use gate layer also incorporate information output context reorder model reorder model struggle sparse data problem con rich context li show neural reorder model condition current previous phrase pair recursive neural network make decision orientation type additional topic instead hand reorder within decode may input sentence output word order use input dependency tree learn model swap child node implement use feed forward neural network formulate top leave right walk dependency tree make reorder decision node model process recurrent neural network include past decision condition context gram translation model alternative view phrase base translation model break phrase translation minimal translation employ gram model unit condition minimal translation unit previous one treat minimal translation unit atomic symbol train neural language model represent minimal translation unit bag break even single input single output single input output word use phrase lean auto encoder chapter neural machine translation bibliography philip graham incorporate discrete translation icon neural machine translation proceeding conference empirical method natural language processing association computational page neural machine translation jointly learn align translate paul phil pragmatic neural language machine translation proceeding conference north american chapter association computational lin human language technology association computational page paul phil neural language framework machine translation prague bulletin mathematical linguistics pascal christian neural probabilistic language model journal machine learn research luisa mauro federico neural versus phrase base machine translation case study proceeding conference empirical meth natural language processing association computational page christian yvette barry matthias antonio ne mariana martin matt raphael carolina scar lucia marco karin marcos finding conference machine translation proceeding first conference chine translation association computational page francisco enrique vidal machine translation use neural network model page box colin george samuel cost weight machine translation domain adaptation proceeding first workshop machine translation association computational page box roland george colin huang bilingual method bibliography adaptive training data selection machine translation annual meeting association chine translation america peter guide alignment training topic aware neural machine translation david hierarchical phrase base translation computational linguistics bart van property neural machine approach proceeding eighth workshop semantics structure statistical translation association computational page empirical comparison domain adaptation method neural machine translation proceeding th annual meeting association computational linguistics short association computational page trevor cong chris incorporate structural alignment bias neural translation model proceeding conference north american chapter association computational human language technology association computational san cal page maria anabel kathy jean patrice joshua catherine jean mar alexandra thomas natalia cyril peter pure neural machine translation system gonzalo bill fast accurate use neural network proceeding conference north american chapter association computational human language technology association computational page jacob thomas richard john fast robust neural network joint model statistical machine translation proceeding nd annual meeting association computational linguistics long association computational page kevin matt post machine translation system proceeding first conference chine translation association computational page john singer adaptive method online learn stochastic optimization journal machine learn research chris victor noah smith effective model proceeding conference north american chapter association computational human language technology association computational page bibliography marco nicola federico neural v phrase base machine translation multi domain scenario proceeding th conference european chapter association computational vol short paper association computational page nan mu ming kenny improve attention model implicit distortion fertility machine translation proceeding th international conference computational technical paper organize page andrew paul phrase base machine system recurrent neural network language model proceeding th name en workshop association computational page multilingual neural chine translation share attention mechanism proceeding conference north american chapter association computational human language technology association computational san page recursive hetero associative memory translation biological neuroscience technology page fast domain adaptation neural machine translation technical report michel jonathan kevin daniel steve wei ignacio inference training context rich syntactic translation model pro st international conference computational linguistics th annual meeting association computational linguistics association computational page michel mark kevin daniel whats translation proceeding joint conference human language technology annual meeting north american chapter association computational linguistics jonas michael david denis dauphin sequence sequence learn adam greg jimenez david timothy generative temporal model memory goldberg neural network method natural language processing volume synthesis lecture human language technology morgan san ca ian aaron deep learning mit press hang victor li incorporate copy mechanism sequence sequence learn proceeding th annual meeting association linguistics long association computational page bibliography bowen point unknown word proceeding th annual meeting association computational linguistics long association computational page kelvin use monolingual corpora neural machine trans michael minimum translation model recurrent neural network proceeding th conference european chapter association computational linguistics association computational page ann irvine chris combine bilingual comparable corpus low resource machine translation proceeding eighth workshop statistical chine translation association computational page roland use large target vocabulary neural machine translation proceeding rd annual meeting association computational linguistics th international joint conference natural language processing long association computational page roland mon neural machine translation system proceeding tenth workshop statistical machine translation association computational page melvin mike maxim martin greg jeffrey dean google multilingual neural machine translation enable zero shot translation neural chine translation ready case study translation direction proceed international workshop speak language translation phil recurrent continuous translation model proceeding conference empirical method natural language processing association computational page shin neural reorder model consider phrase translation word alignment phrase base translation proceeding rd workshop asian translation organize page jimmy ba method stochastic optimization catherine jean domain control neural machine translation technical report alexandra chris nicola bibliography brooke wade christine richard christopher alexandra evan open source toolkit statistical machine translation proceeding th annual meeting association computational linguistics companion volume proceeding demo poster session association computational czech page lei hinton layer normalization print yang neural reorder model phrase base translation proceeding th international conference com technical paper dublin city university association computational lin page andrew neural machine translation supervise attention proceeding th international conference computational technical paper organize page learn new semi supervise deep auto encoder statistical machine translation proceeding nd annual meeting association computational linguistics long association computational page christopher man stanford neural machine translation system speak language domains proceeding international workshop speak language translation page michael christopher man deep neural language mod machine translation proceeding nineteenth conference computational language learn association computational page christopher man effective approach attention base neural machine translation proceeding conference empirical method natural language processing association computational page address rare word problem neural machine translation proceeding rd annual meeting association computational linguistics th international joint conference natural language processing long association computational page hang interactive attention neural machine translation proceeding th international conference computational lin technical paper organize page hang encode source language neural network machine translation proceeding rd annual meeting association computational linguistics th international joint conference natural language processing long association computational page abe vocabulary manipulation neural machine translation proceeding th annual meeting association computational linguistics short association computational page bibliography antonio giuseppe dependency base reorder recurrent neural network machine translation proceeding rd annual meeting association computational linguistics th international joint conference nat language processing long association computational page antonio rico barry alexandra birch deep architectures neural machine translation proceeding second conference machine volume research paper association computational den page tomas statistical language model base neural network brno university technology tomas wen tau geoffrey linguistic regularity continuous space word representation proceeding conference north american chapter computational human language technology association computational page tomas train recurrent neural network proceeding th international conference machine page luis francisco online learn neural machine trans post edit continuous space language model computer speech language continuous space language model statistical machine translation prague bulletin mathematical linguistics continuous space translation model phrase base statistical machine trans proceed poster organize page marta ruiz jose smooth bilingual gram translation proceeding joint conference empirical method natural process computational natural language learning page daniel continuous space language model statistical machine translation proceeding main confer poster session association computational page anthony prune continuous space language model statistical machine translation proceeding ever really replace gram future language model association computational page rico barry linguistic input feature improve neural machine translation proceeding first conference machine translation association computational page bibliography rico barry alexandra birch control politeness neural machine translation via side constraint proceeding conference north american chapter association computational human language technology association computational san page rico barry alexandra birch edinburgh neural chine translation system proceeding first conference machine translation association computational page rico barry alexandra birch improve neural machine translation mod monolingual data proceeding th annual meeting association computational linguistics long association computational page rico barry alexandra birch neural machine translation rare word unit proceeding th annual meeting association computational lin long association computational page maria jean domain post training domain adaptation neural machine translation geoffrey alex simple way prevent neural network journal machine learn research martin ben comparison recurrent network language model ieee international conference signal processing page le sequence sequence learn neural network advance neural information process system page alex incremental adaptation strategy neural network language model proceeding rd workshop continuous tor space model association computational page parallel tool interface opus khalid joseph jan proceeding eighth international conference resource evaluation european language resource association page anthology antonio multifaceted evaluation versus phrase base machine translation language direction proceeding th conference european chapter association computational volume long paper association computational page bibliography yang hang li context gate neural machine translation yang hang li neural machine trans reconstruction proceeding st conference intelligence yang hang li model coverage neural machine translation proceeding th annual meeting association computational linguistics long association computational page marco learn performance machine statistical computational analysis proceeding third workshop machine translation association computational page attention need victoria david decode large scale neural language model improve translation proceeding conference empirical meth natural language processing association computational page speech speech translation system use symbolic processing strategy proceed international conference speech signal processing page lu con continuous space language model gram language model statistical machine translation proceeding conference empirical method natural language pro association computational page neural network base bilingual language model grow statistical machine translation proceeding con empirical method natural language processing association computational page philip rico maria matthias barry edinburghs statistical machine translation system proceeding first conference machine translation association computational page wei ting improve statistical machine translation context sensitive bilingual tic embed model proceeding conference empirical method natural process association computational page mike mohammad wolfgang maxim yuan klaus jeff melvin stephan keith george wei cliff jason jason alex greg jeffrey dean google neural machine bibliography translation bridge gap human machine translation factor recurrent neural network language model ted lecture transcription pro seventh international workshop speak language translation page di tao dual learning machine translation recurrent neural network base rule sequence model machine translation proceeding rd annual meeting association com linguistics th international joint conference natural language process short association computational page matthew adaptive learning rate method local translation prediction global sentence rep proceeding twenty fourth international joint conference intelligence page bibliography author index david philip giuseppe michael jimmy paul luisa nicola alexandra phil patrice bill chris yuan francisco luis mauro victor box colin david trevor alexandra greg marta ruiz aaron brooke maria jeffrey daniel steve jacob paul author index john kevin chris christopher christian andrew jose victoria george ben michel jonas ian stephan jonathan yvette david barry di wei evan geoffrey cong mark matthias gonzalo ann abe christian antonio joshua melvin shin michael author index jeff kevin catherine maxim alex roland george thomas samuel lei hang mu victor timothy ting yang jean klaus wolfgang john christopher christopher daniel roland antonio tomas christine maria graham mariana mohammad author index martin matt alexandra tao anabel jimenez thomas jason anthony raphael alex adam carolina mike richard natalia jean rico wade jason noah lucia keith martin alex ignacio cyril antonio marco van bart karin enrique pascal wei martin greg philip author index kelvin kathy nan denis wen tau cliff marcos matthew richard bowen ming kenny peter geoffrey'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus{topic, : }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a687b034",
   "metadata": {
    "id": "a687b034"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Char to int {' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "Int to char {0: ' ', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# creates mapping of unique characters to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "print(chars)\n",
    "print(\"Char to int\" , char_to_int)\n",
    "print(\"Int to char\", int_to_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dedbc5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of total characters are 138822\n",
      "\n",
      "The character vocab size is 27\n"
     ]
    }
   ],
   "source": [
    "# Prints the total characters and character vocab size\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "\n",
    "print(\"The number of total characters are\", n_chars)\n",
    "print(\"\\nThe character vocab size is\", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "291b12e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data X [19, 20, 1, 20, 9, 19, 20, 9, 3, 1, 12, 0, 13, 1, 3, 8, 9, 14, 5, 0, 20, 18, 1, 14, 19, 12, 1, 20, 9, 15, 14, 0, 4, 18, 1, 6, 20, 0, 3, 8, 1, 16, 20, 5, 18, 0, 14, 5, 21, 18, 1, 12, 0, 13, 1, 3, 8, 9, 14, 5, 0, 20, 18, 1, 14, 19, 12, 1, 20, 9, 15, 14, 0, 3, 5, 14, 20, 5, 18, 0, 19, 16, 5, 5, 3, 8, 0, 12, 1, 14, 7, 21, 1, 7, 5, 0, 16, 18, 15, 3]\n",
      " data Y 5\n",
      "Total Patterns:  138722\n"
     ]
    }
   ],
   "source": [
    "#Prepares dataset where the input is sequence of 100 characters and target is next character.\n",
    "seq_length = 100\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "          seq_in = raw_text[i:i + seq_length]\n",
    "          seq_out = raw_text[i + seq_length]\n",
    "          dataX.append([char_to_int[char] for char in seq_in])\n",
    "          dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b32842e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138722, 100, 1)\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# reshapes X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "# one hot encodes the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59e01b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim =100\n",
    "max_length =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13517ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b278fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 100)          2700      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 256)               365568    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 27)                6939      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 375,207\n",
      "Trainable params: 375,207\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab, embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb9a4477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 17:29:21.629920: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 110977600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1084/1084 [==============================] - 451s 414ms/step - loss: 1.9136\n",
      "Epoch 2/20\n",
      "1084/1084 [==============================] - 435s 401ms/step - loss: 1.2530\n",
      "Epoch 3/20\n",
      "1084/1084 [==============================] - 456s 421ms/step - loss: 1.0920\n",
      "Epoch 4/20\n",
      "1084/1084 [==============================] - 673s 621ms/step - loss: 1.0119\n",
      "Epoch 5/20\n",
      "1084/1084 [==============================] - 719s 663ms/step - loss: 0.9586\n",
      "Epoch 6/20\n",
      "1084/1084 [==============================] - 722s 666ms/step - loss: 0.9177\n",
      "Epoch 7/20\n",
      "1084/1084 [==============================] - 705s 650ms/step - loss: 0.8832\n",
      "Epoch 8/20\n",
      "1084/1084 [==============================] - 722s 666ms/step - loss: 0.8544\n",
      "Epoch 9/20\n",
      "1084/1084 [==============================] - 764s 705ms/step - loss: 0.8280\n",
      "Epoch 10/20\n",
      "1084/1084 [==============================] - 738s 681ms/step - loss: 0.8044\n",
      "Epoch 11/20\n",
      "1084/1084 [==============================] - 724s 668ms/step - loss: 0.7805\n",
      "Epoch 12/20\n",
      "1084/1084 [==============================] - 693s 640ms/step - loss: 0.7596\n",
      "Epoch 13/20\n",
      "1084/1084 [==============================] - 682s 629ms/step - loss: 0.7412\n",
      "Epoch 14/20\n",
      "1084/1084 [==============================] - 620s 572ms/step - loss: 0.7224\n",
      "Epoch 15/20\n",
      "1084/1084 [==============================] - 674s 621ms/step - loss: 0.7041\n",
      "Epoch 16/20\n",
      "1084/1084 [==============================] - 699s 644ms/step - loss: 0.6890\n",
      "Epoch 17/20\n",
      "1084/1084 [==============================] - 739s 682ms/step - loss: 0.6733\n",
      "Epoch 18/20\n",
      "1084/1084 [==============================] - 830s 766ms/step - loss: 0.6614\n",
      "Epoch 19/20\n",
      "1084/1084 [==============================] - 540s 497ms/step - loss: 0.6480\n",
      "Epoch 20/20\n",
      "1084/1084 [==============================] - 495s 457ms/step - loss: 0.6395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5e2422ec70>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs = 20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8beb28e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the sequence similar to above methods. Gets the generated string using the model.\n",
    "def predict_next_n_chars(pattern, n):\n",
    "    for i in range(n):\n",
    "      x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "      prediction = model.predict(x, verbose=0)\n",
    "      print (int_to_char[np.argmax(prediction)], end = '')   #get next char index.\n",
    "      seq_in = [int_to_char[value] for value in pattern]\n",
    "      pattern.append(np.argmax(prediction))\n",
    "      pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a46d8f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed -\n",
      "\n",
      "rrent right leave recurrent figure neural machine translation part input encoder consist two recurre\n",
      "\n",
      "Generated string -\n",
      "\n",
      "nt neural network also increase state compute gradient descent training data adaptation statistical machine translation system train example correct output word see figure illustration stack recurrent"
     ]
    }
   ],
   "source": [
    "#picks a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "input_str = ''.join([int_to_char[value] for value in pattern])\n",
    "print (\"Seed -\",  input_str, sep = '\\n\\n')\n",
    "print (\"\\nGenerated string -\\n\")\n",
    "\n",
    "predict_next_n_chars(pattern, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f05cd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed -\n",
      "\n",
      "many modern service systems rely on a network of hub facilities to help concentrate flows of freight or passengers\n",
      "\n",
      "Generated string -\n",
      "\n",
      "er compute computationally concern implement use standard train example set train example self attention layer decoder state compute gradient descent training data adaptation statistical machine trans"
     ]
    }
   ],
   "source": [
    "input_str = \"Many modern service systems rely on a network of hub facilities to help concentrate flows of freight or passengers\"\n",
    "\n",
    "#Uses the first 100 characters from given input_str as input to generate next 200 characters. \n",
    "input_str = input_str.lower()\n",
    "input_string = ''\n",
    "for each in input_str:\n",
    "    if each in chars:\n",
    "           if (len (input_string) < 100):\n",
    "                input_string += each\n",
    "\n",
    "pattern = []\n",
    "pattern.append([char_to_int[char] for char in input_string])\n",
    "\n",
    "print (\"Seed -\",  input_str, sep = '\\n\\n')\n",
    "print (\"\\nGenerated string -\\n\")\n",
    "predict_next_n_chars(pattern[0], 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f0173f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
