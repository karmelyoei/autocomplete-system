{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "339eafae",
   "metadata": {
    "id": "339eafae"
   },
   "source": [
    "# Final Project NLP Smart Scientific Content Auto-completion System\n",
    "\n",
    "\n",
    "a text auto-complete system that supports scientists in writing articles in:\n",
    "- completing some typed words/letters.\n",
    "- Identifying words/phrases that are very unlikely to occur in some text contexts and suggesting proper replacements.\n",
    "\n",
    "\n",
    "Karmel Salah & Fatimah Najwan\n",
    "15/4/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FZVlVeATK31U",
   "metadata": {
    "id": "FZVlVeATK31U"
   },
   "source": [
    "# We will use Colab for a Free GPU so if you are using Colab frist connect to your drive to get the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ntkBblK8IURV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntkBblK8IURV",
    "outputId": "582dff94-4d99-4f0d-e2c5-f80a6b29def6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "J-f70JR2IvsP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J-f70JR2IvsP",
    "outputId": "82d9f6ff-762d-46da-b5c7-bda6367d0397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./drive/MyDrive/dataset.zip\n",
      "   creating: dataset/\n",
      "  inflating: dataset/1012.2609.txt   \n",
      "  inflating: dataset/1207.1847v1.txt  \n",
      "  inflating: dataset/1209.3126v1.txt  \n",
      "  inflating: dataset/1210.0852.txt   \n",
      "  inflating: dataset/1305.6143.txt   \n",
      "  inflating: dataset/1306.6802v2.txt  \n",
      "  inflating: dataset/1308.0850v5.txt  \n",
      "  inflating: dataset/1309.4168v1.txt  \n",
      "  inflating: dataset/1312.3005v3.txt  \n",
      "  inflating: dataset/1403.6652v2.txt  \n",
      "  inflating: dataset/1404.3377v1.txt  \n",
      "  inflating: dataset/1404.4326v1.txt  \n",
      "  inflating: dataset/1405.4053v2.txt  \n",
      "  inflating: dataset/1405.4273v1.txt  \n",
      "  inflating: dataset/1406.1078v3.txt  \n",
      "  inflating: dataset/1408.5882v2.txt  \n",
      "  inflating: dataset/1409.0473v7.txt  \n",
      "  inflating: dataset/1409.2329v5.txt  \n",
      "  inflating: dataset/1409.3215v3.txt  \n",
      "  inflating: dataset/1410.5329v4.txt  \n",
      "  inflating: dataset/1410.8206v4.txt  \n",
      "  inflating: dataset/1411.4555v2.txt  \n",
      "  inflating: dataset/1412.1058v2.txt  \n",
      "  inflating: dataset/1412.2007v2.txt  \n",
      "  inflating: dataset/1412.5335v7.txt  \n",
      "  inflating: dataset/1412.6622v4.txt  \n",
      "  inflating: dataset/1412.7753v2.txt  \n",
      "  inflating: dataset/1502.01710v5.txt  \n",
      "  inflating: dataset/1503.00075v3.txt  \n",
      "  inflating: dataset/1503.06733v2.txt  \n",
      "  inflating: dataset/1503.08895v5.txt  \n",
      "  inflating: dataset/1505.00641v3.txt  \n",
      "  inflating: dataset/1505.07818v4.txt  \n",
      "  inflating: dataset/1506.00019v4.txt  \n",
      "  inflating: dataset/1506.03140v2.txt  \n",
      "  inflating: dataset/1506.07285v5.txt  \n",
      "  inflating: dataset/1507.07998v1.txt  \n",
      "  inflating: dataset/1508.01211v2.txt  \n",
      "  inflating: dataset/1508.01745v2.txt  \n",
      "  inflating: dataset/1508.02096v2.txt  \n",
      "  inflating: dataset/1508.04025v5.txt  \n",
      "  inflating: dataset/1508.04395v2.txt  \n",
      "  inflating: dataset/1508.06034v1.txt  \n",
      "  inflating: dataset/1508.06615v4.txt  \n",
      "  inflating: dataset/1508.07909v5.txt  \n",
      "  inflating: dataset/1509.01626v3.txt  \n",
      "  inflating: dataset/1509.01626v3_18-29-38.txt  \n",
      "  inflating: dataset/1509.01626v3_18-29-55.txt  \n",
      "  inflating: dataset/1510.01784v1.txt  \n",
      "  inflating: dataset/1510.03820v4.txt  \n",
      "  inflating: dataset/1511.01432v1.txt  \n",
      "  inflating: dataset/1511.02301v4.txt  \n",
      "  inflating: dataset/1511.03745v4.txt  \n",
      "  inflating: dataset/1511.04590v5.txt  \n",
      "  inflating: dataset/1511.06349v4.txt  \n",
      "  inflating: dataset/1511.06391v4.txt  \n",
      "  inflating: dataset/1511.06456v4.txt  \n",
      "  inflating: dataset/1511.06732v7.txt  \n",
      "  inflating: dataset/1511.06909v7.txt  \n",
      "  inflating: dataset/1511.06939v4.txt  \n",
      "  inflating: dataset/1511.08630v2.txt  \n",
      "  inflating: dataset/1512.01337v4.txt  \n",
      "  inflating: dataset/1512.04906v1.txt  \n",
      "  inflating: dataset/1512.05287v5.txt  \n",
      "  inflating: dataset/1512.07982v1.txt  \n",
      "  inflating: dataset/1512.08183v5.txt  \n",
      "  inflating: dataset/1601.01892v2.txt  \n",
      "  inflating: dataset/1601.03313v2.txt  \n",
      "  inflating: dataset/1601.04811v6.txt  \n",
      "  inflating: dataset/1601.06733v7.txt  \n",
      "  inflating: dataset/1602.02410v2.txt  \n",
      "  inflating: dataset/1602.05568v1.txt  \n",
      "  inflating: dataset/1602.06023v5.txt  \n",
      "  inflating: dataset/1602.07776v4.txt  \n",
      "  inflating: dataset/1602.07783v2.txt  \n",
      "  inflating: dataset/1603.01913v2.txt  \n",
      "  inflating: dataset/1603.03116v3.txt  \n",
      "  inflating: dataset/1603.03827v1.txt  \n",
      "  inflating: dataset/1603.04466v1.txt  \n",
      "  inflating: dataset/1603.06075v3.txt  \n",
      "  inflating: dataset/1603.06147v4.txt  \n",
      "  inflating: dataset/1603.06318v5.txt  \n",
      "  inflating: dataset/1603.06393v3.txt  \n",
      "  inflating: dataset/1603.06744v2.txt  \n",
      "  inflating: dataset/1603.06807v2.txt  \n",
      "  inflating: dataset/1603.07771v3.txt  \n",
      "  inflating: dataset/1603.08861v2.txt  \n",
      "  inflating: dataset/1603.08884v1.txt  \n",
      "  inflating: dataset/1603.09025v5.txt  \n",
      "  inflating: dataset/1603.09457v1.txt  \n",
      "  inflating: dataset/1603.09727v1.txt  \n",
      "  inflating: dataset/1604.00837v1.txt  \n",
      "  inflating: dataset/1604.02201v1.txt  \n",
      "  inflating: dataset/1604.02748v2.txt  \n",
      "  inflating: dataset/1604.03390v2.txt  \n",
      "  inflating: dataset/1604.03489v2.txt  \n",
      "  inflating: dataset/1605.00937v2.txt  \n",
      "  inflating: dataset/1605.04469v3.txt  \n",
      "  inflating: dataset/1605.04655v2.txt  \n",
      "  inflating: dataset/1605.05101v1.txt  \n",
      "  inflating: dataset/1605.07422v3.txt  \n",
      "  inflating: dataset/1605.07722v3.txt  \n",
      "  inflating: dataset/1605.07725v3.txt  \n",
      "  inflating: dataset/1605.09186v4.txt  \n",
      "  inflating: dataset/1605.09507v3.txt  \n",
      "  inflating: dataset/1606.00253v1.txt  \n",
      "  inflating: dataset/1606.00499v2.txt  \n",
      "  inflating: dataset/1606.00776v2.txt  \n",
      "  inflating: dataset/1606.00931v3.txt  \n",
      "  inflating: dataset/1606.01305v4.txt  \n",
      "  inflating: dataset/1606.01781v2.txt  \n",
      "  inflating: dataset/1606.02006v2.txt  \n",
      "  inflating: dataset/1606.02892v2.txt  \n",
      "  inflating: dataset/1606.02960v2.txt  \n",
      "  inflating: dataset/1606.03821v2.txt  \n",
      "  inflating: dataset/1606.04080v2.txt  \n",
      "  inflating: dataset/1606.04582v6.txt  \n",
      "  inflating: dataset/1606.04621v3.txt  \n",
      "  inflating: dataset/1606.04838v3.txt  \n",
      "  inflating: dataset/1606.05819v1.txt  \n",
      "  inflating: dataset/1606.06259.txt  \n",
      "  inflating: dataset/1606.07659v3.txt  \n",
      "  inflating: dataset/1606.07947v4.txt  \n",
      "  inflating: dataset/1607.01759v3.txt  \n",
      "  inflating: dataset/1607.02501v2.txt  \n",
      "  inflating: dataset/1607.03474v5.txt  \n",
      "  inflating: dataset/1607.04228v1.txt  \n",
      "  inflating: dataset/1607.04315v3.txt  \n",
      "  inflating: dataset/1607.07086v3.txt  \n",
      "  inflating: dataset/1608.00272v3.txt  \n",
      "  inflating: dataset/1608.02021v1.txt  \n",
      "  inflating: dataset/1608.04738v2.txt  \n",
      "  inflating: dataset/1608.06043v3.txt  \n",
      "  inflating: dataset/1608.07076v1.txt  \n",
      "  inflating: dataset/1609.03976v1.txt  \n",
      "  inflating: dataset/1609.05244v2.txt  \n",
      "  inflating: dataset/1609.05473v6.txt  \n",
      "  inflating: dataset/1609.06686v1.txt  \n",
      "  inflating: dataset/1609.08144v2.txt  \n",
      "  inflating: dataset/1609.08264v1.txt  \n",
      "  inflating: dataset/1609.08359v2.txt  \n",
      "  inflating: dataset/1609.09552v1.txt  \n",
      "  inflating: dataset/1610.01588v3.txt  \n",
      "  inflating: dataset/1610.03017v3.txt  \n",
      "  inflating: dataset/1610.05838v3.txt  \n",
      "  inflating: dataset/1610.08613v2.txt  \n",
      "  inflating: dataset/1610.09225v1.txt  \n",
      "  inflating: dataset/1610.09565v1.txt  \n",
      "  inflating: dataset/1610.10099v2.txt  \n",
      "  inflating: dataset/1611.00144v1.txt  \n",
      "  inflating: dataset/1611.00472v1.txt  \n",
      "  inflating: dataset/1611.01368v1.txt  \n",
      "  inflating: dataset/1611.01462v3.txt  \n",
      "  inflating: dataset/1611.01576v2.txt  \n",
      "  inflating: dataset/1611.01578v2.txt  \n",
      "  inflating: dataset/1611.01874v2.txt  \n",
      "  inflating: dataset/1611.01884v3.txt  \n",
      "  inflating: dataset/1611.02344v3.txt  \n",
      "  inflating: dataset/1611.02639v2.txt  \n",
      "  inflating: dataset/1611.04358v2.txt  \n",
      "  inflating: dataset/1611.07804v1.txt  \n",
      "  inflating: dataset/1611.08307v1.txt  \n",
      "  inflating: dataset/1611.08480v2.txt  \n",
      "  inflating: dataset/1611.09414v2.txt  \n",
      "  inflating: dataset/1612.00385v2.txt  \n",
      "  inflating: dataset/1612.01887v2.txt  \n",
      "  inflating: dataset/1612.03651v1.txt  \n",
      "  inflating: dataset/1612.04426v1.txt  \n",
      "  inflating: dataset/1612.06935v6.txt  \n",
      "  inflating: dataset/1612.07640v1.txt  \n",
      "  inflating: dataset/1612.08083v3.txt  \n",
      "  inflating: dataset/1701.02810v2.txt  \n",
      "  inflating: dataset/1701.02870v3.txt  \n",
      "  inflating: dataset/1701.04831v2.txt  \n",
      "  inflating: dataset/1701.06511v3.txt  \n",
      "  inflating: dataset/1701.07274v6.txt  \n",
      "  inflating: dataset/1702.00887v3.txt  \n",
      "  inflating: dataset/1702.01417v2.txt  \n",
      "  inflating: dataset/1702.01824v2.txt  \n",
      "  inflating: dataset/1702.02390v1.txt  \n",
      "  inflating: dataset/1702.08400v3.txt  \n",
      "  inflating: dataset/1702.08431v4.txt  \n",
      "  inflating: dataset/1703.00535v3.txt  \n",
      "  inflating: dataset/1703.01442v1.txt  \n",
      "  inflating: dataset/1703.02504v1.txt  \n",
      "  inflating: dataset/1703.02620v1.txt  \n",
      "  inflating: dataset/1703.03130v1.txt  \n",
      "  inflating: dataset/1703.03906v2.txt  \n",
      "  inflating: dataset/1703.04247v1.txt  \n",
      "  inflating: dataset/1703.04357v1.txt  \n",
      "  inflating: dataset/1703.06103v4.txt  \n",
      "  inflating: dataset/1703.09570.txt  \n",
      "  inflating: dataset/1703.10152v1.txt  \n",
      "  inflating: dataset/1703.10722v3.txt  \n",
      "  inflating: dataset/1703.10931v2.txt  \n",
      "  inflating: dataset/1703.10960v3.txt  \n",
      "  inflating: dataset/1704.00784v2.txt  \n",
      "  inflating: dataset/1704.01087v1.txt  \n",
      "  inflating: dataset/1704.01444v2.txt  \n",
      "  inflating: dataset/1704.02798v4.txt  \n",
      "  inflating: dataset/1704.04368v2.txt  \n",
      "  inflating: dataset/1704.06125v1.txt  \n",
      "  inflating: dataset/1704.06803v1.txt  \n",
      "  inflating: dataset/1704.06960v5.txt  \n",
      "  inflating: dataset/1704.07138v2.txt  \n",
      "  inflating: dataset/1704.07156v1.txt  \n",
      "  inflating: dataset/1704.08803v2.txt  \n",
      "  inflating: dataset/1705.00823v1.txt  \n",
      "  inflating: dataset/1705.03122v3.txt  \n",
      "  inflating: dataset/1705.05311v2.txt  \n",
      "  inflating: dataset/1705.05414v2.txt  \n",
      "  inflating: dataset/1705.06463v1.txt  \n",
      "  inflating: dataset/1705.07393v2.txt  \n",
      "  inflating: dataset/1705.07704v3.txt  \n",
      "  inflating: dataset/1705.07830v3.txt  \n",
      "  inflating: dataset/1705.09037v3.txt  \n",
      "  inflating: dataset/1705.09655v2.txt  \n",
      "  inflating: dataset/1705.10513v2.txt  \n",
      "  inflating: dataset/1706.00043v2.txt  \n",
      "  inflating: dataset/1706.00061v1.txt  \n",
      "  inflating: dataset/1706.00188v1.txt  \n",
      "  inflating: dataset/1706.00457v1.txt  \n",
      "  inflating: dataset/1706.01399v3.txt  \n",
      "  inflating: dataset/1706.01449v3.txt  \n",
      "  inflating: dataset/1706.02263v2.txt  \n",
      "  inflating: dataset/1706.02459v1.txt  \n",
      "  inflating: dataset/1706.03059v2.txt  \n",
      "  inflating: dataset/1706.03196v1.txt  \n",
      "  inflating: dataset/1706.03471v2.txt  \n",
      "  inflating: dataset/1706.03762v5.txt  \n",
      "  inflating: dataset/1706.05565v8.txt  \n",
      "  inflating: dataset/1706.06363v1.txt  \n",
      "  inflating: dataset/1706.06415v1.txt  \n",
      "  inflating: dataset/1706.07206v2.txt  \n",
      "  inflating: dataset/1706.09799v1.txt  \n",
      "  inflating: dataset/1707.01166v3.txt  \n",
      "  inflating: dataset/1707.01780v3.txt  \n",
      "  inflating: dataset/1707.02275v1.txt  \n",
      "  inflating: dataset/1707.02377v1.txt  \n",
      "  inflating: dataset/1707.02485v1.txt  \n",
      "  inflating: dataset/1707.02786v4.txt  \n",
      "  inflating: dataset/1707.02968v2.txt  \n",
      "  inflating: dataset/1707.04412v1.txt  \n",
      "  inflating: dataset/1707.06885v1.txt  \n",
      "  inflating: dataset/1707.07402v4.txt  \n",
      "  inflating: dataset/1707.07847v3.txt  \n",
      "  inflating: dataset/1707.08052v1.txt  \n",
      "  inflating: dataset/1707.09569v1.txt  \n",
      "  inflating: dataset/1708.00077v1.txt  \n",
      "  inflating: dataset/1708.00107v2.txt  \n",
      "  inflating: dataset/1708.00524v2.txt  \n",
      "  inflating: dataset/1708.02182v1.txt  \n",
      "  inflating: dataset/1708.02657v2.txt  \n",
      "  inflating: dataset/1708.02702v4.txt  \n",
      "  inflating: dataset/1708.03629v3.txt  \n",
      "  inflating: dataset/1708.04439v2.txt  \n",
      "  inflating: dataset/1708.04729v3.txt  \n",
      "  inflating: dataset/1708.05031.txt  \n",
      "  inflating: dataset/1708.05045v2.txt  \n",
      "  inflating: dataset/1708.05891v1.txt  \n",
      "  inflating: dataset/1709.01584v2.txt  \n",
      "  inflating: dataset/1709.02755v5.txt  \n",
      "  inflating: dataset/1709.02984.txt  \n",
      "  inflating: dataset/1709.03082v8.txt  \n",
      "  inflating: dataset/1709.03714v1.txt  \n",
      "  inflating: dataset/1709.03856v5.txt  \n",
      "  inflating: dataset/1709.04054v3.txt  \n",
      "  inflating: dataset/1709.04396v2.txt  \n",
      "  inflating: dataset/1709.05074v1.txt  \n",
      "  inflating: dataset/1709.06671v1.txt  \n",
      "  inflating: dataset/1709.07417v2.txt  \n",
      "  inflating: dataset/1709.07432v2.txt  \n",
      "  inflating: dataset/1709.07809v1.txt  \n",
      "  inflating: dataset/1709.08267v2.txt  \n",
      "  inflating: dataset/1709.08624v2.txt  \n",
      "  inflating: dataset/1709.08878v2.txt  \n",
      "  inflating: dataset/1709.09500v1.txt  \n",
      "  inflating: dataset/1710.00482v1.txt  \n",
      "  inflating: dataset/1710.02318v1.txt  \n",
      "  inflating: dataset/1710.04087v3.txt  \n",
      "  inflating: dataset/1710.05649.txt  \n",
      "  inflating: dataset/1710.06071v1.txt  \n",
      "  inflating: dataset/1710.09537v1.txt  \n",
      "  inflating: dataset/1710.10296.txt  \n",
      "  inflating: dataset/1710.11035v2.txt  \n",
      "  inflating: dataset/1710.11342v2.txt  \n",
      "  inflating: dataset/1711.00043v2.txt  \n",
      "  inflating: dataset/1711.00066v4.txt  \n",
      "  inflating: dataset/1711.00350v3.txt  \n",
      "  inflating: dataset/1711.01068v2.txt  \n",
      "  inflating: dataset/1711.01731v3.txt  \n",
      "  inflating: dataset/1711.02013v2.txt  \n",
      "  inflating: dataset/1711.02132v1.txt  \n",
      "  inflating: dataset/1711.02281v2.txt  \n",
      "  inflating: dataset/1711.03953v4.txt  \n",
      "  inflating: dataset/1711.04956v5.txt  \n",
      "  inflating: dataset/1711.06104v4.txt  \n",
      "  inflating: dataset/1711.07601v1.txt  \n",
      "  inflating: dataset/1711.09151v1.txt  \n",
      "  inflating: dataset/1711.09357v1.txt  \n",
      "  inflating: dataset/1711.09645v2.txt  \n",
      "  inflating: dataset/1711.09724v1.txt  \n",
      "  inflating: dataset/1712.02616v3.txt  \n",
      "  inflating: dataset/1712.05690v2.txt  \n",
      "  inflating: dataset/1712.05846v2.txt  \n",
      "  inflating: dataset/1712.05972v2.txt  \n",
      "  inflating: dataset/1712.06751v2.txt  \n",
      "  inflating: dataset/1712.07040v1.txt  \n",
      "  inflating: dataset/1712.07525.txt  \n",
      "  inflating: dataset/1712.09948v1.txt  \n",
      "  inflating: dataset/1801.00209v3.txt  \n",
      "  inflating: dataset/1801.00632v2.txt  \n",
      "  inflating: dataset/1801.01315v1.txt  \n",
      "  inflating: dataset/1801.02203v1.txt  \n",
      "  inflating: dataset/1801.02294v5.txt  \n",
      "  inflating: dataset/1801.04354v5.txt  \n",
      "  inflating: dataset/1801.06146v5.txt  \n",
      "  inflating: dataset/1801.06261v2.txt  \n",
      "  inflating: dataset/1801.07736v3.txt  \n",
      "  inflating: dataset/1801.08284v2.txt  \n",
      "  inflating: dataset/1801.08831v1.txt  \n",
      "  inflating: dataset/1801.09251v2.txt  \n",
      "  inflating: dataset/1801.09797v1.txt  \n",
      "  inflating: dataset/1801.10308v1.txt  \n",
      "  inflating: dataset/1802.00889v1.txt  \n",
      "  inflating: dataset/1802.00923v1.txt  \n",
      "  inflating: dataset/1802.00924v1.txt  \n",
      "  inflating: dataset/1802.01345v3.txt  \n",
      "  inflating: dataset/1802.02550v7.txt  \n",
      "  inflating: dataset/1802.03238v2.txt  \n",
      "  inflating: dataset/1802.03268v2.txt  \n",
      "  inflating: dataset/1802.03594v2.txt  \n",
      "  inflating: dataset/1802.03938v1.txt  \n",
      "  inflating: dataset/1802.04051v4.txt  \n",
      "  inflating: dataset/1802.04591v2.txt  \n",
      "  inflating: dataset/1802.05335v3.txt  \n",
      "  inflating: dataset/1802.05365v2.txt  \n",
      "  inflating: dataset/1802.05694v1.txt  \n",
      "  inflating: dataset/1802.05814v1.txt  \n",
      "  inflating: dataset/1802.06182v1.txt  \n",
      "  inflating: dataset/1802.06901v3.txt  \n",
      "  inflating: dataset/1802.08452v1.txt  \n",
      "  inflating: dataset/1802.09957v1.txt  \n",
      "  inflating: dataset/1803.00114v3.txt  \n",
      "  inflating: dataset/1803.00188v1.txt  \n",
      "  inflating: dataset/1803.01271v2.txt  \n",
      "  inflating: dataset/1803.02155v2.txt  \n",
      "  inflating: dataset/1803.02218v1.txt  \n",
      "  inflating: dataset/1803.02781v3.txt  \n",
      "  inflating: dataset/1803.02879v2.txt  \n",
      "  inflating: dataset/1803.03467v4.txt  \n",
      "  inflating: dataset/1803.03816v2.txt  \n",
      "  inflating: dataset/1803.05030v1.txt  \n",
      "  inflating: dataset/1803.07416v1.txt  \n",
      "  inflating: dataset/1803.08240v1.txt  \n",
      "  inflating: dataset/1803.09065v3.txt  \n",
      "  inflating: dataset/1803.10109v1.txt  \n",
      "  inflating: dataset/1803.11175v2.txt  \n",
      "  inflating: dataset/1803.11175v2_18-30-23.txt  \n",
      "  inflating: dataset/1804.00247v2.txt  \n",
      "  inflating: dataset/1804.00538v4.txt  \n",
      "  inflating: dataset/1804.00823v4.txt  \n",
      "  inflating: dataset/1804.02063v1.txt  \n",
      "  inflating: dataset/1804.04095v1.txt  \n",
      "  inflating: dataset/1804.04950v2.txt  \n",
      "  inflating: dataset/1804.06087v1.txt  \n",
      "  inflating: dataset/1804.06323v2.txt  \n",
      "  inflating: dataset/1804.07755v2.txt  \n",
      "  inflating: dataset/1804.07827v2.txt  \n",
      "  inflating: dataset/1804.07998v2.txt  \n",
      "  inflating: dataset/1804.08069v1.txt  \n",
      "  inflating: dataset/1804.08166v1.txt  \n",
      "  inflating: dataset/1804.08771v2.txt  \n",
      "  inflating: dataset/1804.08875v1.txt  \n",
      "  inflating: dataset/1804.09541v1.txt  \n",
      "  inflating: dataset/1804.09779v2.txt  \n",
      "  inflating: dataset/1804.09849v2.txt  \n",
      "  inflating: dataset/1804.10862.txt  \n",
      "  inflating: dataset/1804.10959v1.txt  \n",
      "  inflating: dataset/1804.11019v1.txt  \n",
      "  inflating: dataset/1804.11258v3.txt  \n",
      "  inflating: dataset/1805.01070v2.txt  \n",
      "  inflating: dataset/1805.02473v3.txt  \n",
      "  inflating: dataset/1805.02474v1.txt  \n",
      "  inflating: dataset/1805.03294v1.txt  \n",
      "  inflating: dataset/1805.03352v2.txt  \n",
      "  inflating: dataset/1805.04174v1.txt  \n",
      "  inflating: dataset/1805.04437v1.txt  \n",
      "  inflating: dataset/1805.04601v1.txt  \n",
      "  inflating: dataset/1805.06064v1.txt  \n",
      "  inflating: dataset/1805.06201v1.txt  \n",
      "  inflating: dataset/1805.07030v1.txt  \n",
      "  inflating: dataset/1805.07043v1.txt  \n",
      "  inflating: dataset/1805.07513v1.txt  \n",
      "  inflating: dataset/1805.08159v2.txt  \n",
      "  inflating: dataset/1805.08297v1.txt  \n",
      "  inflating: dataset/1805.08705v2.txt  \n",
      "  inflating: dataset/1805.09016v1.txt  \n",
      "  inflating: dataset/1805.09461v4.txt  \n",
      "  inflating: dataset/1805.10212v1.txt  \n",
      "  inflating: dataset/1805.10387v2.txt  \n",
      "  inflating: dataset/1805.11462v1.txt  \n",
      "  inflating: dataset/1806.00187v3.txt  \n",
      "  inflating: dataset/1806.01501v1.txt  \n",
      "  inflating: dataset/1806.01822v2.txt  \n",
      "  inflating: dataset/1806.02557v2.txt  \n",
      "  inflating: dataset/1806.02960v1.txt  \n",
      "  inflating: dataset/1806.03529v2.txt  \n",
      "  inflating: dataset/1806.04381v2.txt  \n",
      "  inflating: dataset/1806.05219v2.txt  \n",
      "  inflating: dataset/1806.05507v1.txt  \n",
      "  inflating: dataset/1806.05516v1.txt  \n",
      "  inflating: dataset/1806.05559v2.txt  \n",
      "  inflating: dataset/1806.06219v3.txt  \n",
      "  inflating: dataset/1806.06228v1.txt  \n",
      "  inflating: dataset/1806.06259v1.txt  \n",
      "  inflating: dataset/1806.08462v2.txt  \n",
      "  inflating: dataset/1806.08730v1.txt  \n",
      "  inflating: dataset/1806.09055v2.txt  \n",
      "  inflating: dataset/1806.09828v1.txt  \n",
      "  inflating: dataset/1806.09835v1.txt  \n",
      "  inflating: dataset/1806.10478v2.txt  \n",
      "  inflating: dataset/1807.00311v1.txt  \n",
      "  inflating: dataset/1807.02291.txt  \n",
      "  inflating: dataset/1807.02478v1.txt  \n",
      "  inflating: dataset/1807.03491v1.txt  \n",
      "  inflating: dataset/1807.03819v3.txt  \n",
      "  inflating: dataset/1807.04271v3.txt  \n",
      "  inflating: dataset/1807.04990v1.txt  \n",
      "  inflating: dataset/1807.06786v1.txt  \n",
      "  inflating: dataset/1807.07741v1.txt  \n",
      "  inflating: dataset/1808.00076v3.txt  \n",
      "  inflating: dataset/1808.00720v2.txt  \n",
      "  inflating: dataset/1808.01371v2.txt  \n",
      "  inflating: dataset/1808.02733v1.txt  \n",
      "  inflating: dataset/1808.03867v3.txt  \n",
      "  inflating: dataset/1808.03908v1.txt  \n",
      "  inflating: dataset/1808.04189v1.txt  \n",
      "  inflating: dataset/1808.04469v2.txt  \n",
      "  inflating: dataset/1808.04819v1.txt  \n",
      "  inflating: dataset/1808.05326v1.txt  \n",
      "  inflating: dataset/1808.05505v3.txt  \n",
      "  inflating: dataset/1808.05784v2.txt  \n",
      "  inflating: dataset/1808.06161v1.txt  \n",
      "  inflating: dataset/1808.06226v1.txt  \n",
      "  inflating: dataset/1808.07233v5.txt  \n",
      "  inflating: dataset/1808.07325.txt  \n",
      "  inflating: dataset/1808.07733v1.txt  \n",
      "  inflating: dataset/1808.08703v2.txt  \n",
      "  inflating: dataset/1808.08795v1.txt  \n",
      "  inflating: dataset/1808.08931v2.txt  \n",
      "  inflating: dataset/1808.09031v1.txt  \n",
      "  inflating: dataset/1808.09160v1.txt  \n",
      "  inflating: dataset/1808.09644v1.txt  \n",
      "  inflating: dataset/1808.09744v1.txt  \n",
      "  inflating: dataset/1808.09781v1.txt  \n",
      "  inflating: dataset/1808.10000v1.txt  \n",
      "  inflating: dataset/1808.10122v3.txt  \n",
      "  inflating: dataset/1808.10245v1.txt  \n",
      "  inflating: dataset/1808.10805v2.txt  \n",
      "  inflating: dataset/1809.00366v1.txt  \n",
      "  inflating: dataset/1809.00530v1.txt  \n",
      "  inflating: dataset/1809.00582v2.txt  \n",
      "  inflating: dataset/1809.00717v1.txt  \n",
      "  inflating: dataset/1809.00794v2.txt  \n",
      "  inflating: dataset/1809.01272v1.txt  \n",
      "  inflating: dataset/1809.01478v2.txt  \n",
      "  inflating: dataset/1809.01576v2.txt  \n",
      "  inflating: dataset/1809.01694v2.txt  \n",
      "  inflating: dataset/1809.01797v2.txt  \n",
      "  inflating: dataset/1809.01829v4.txt  \n",
      "  inflating: dataset/1809.02836v1.txt  \n",
      "  inflating: dataset/1809.03999v1.txt  \n",
      "  inflating: dataset/1809.05053v1.txt  \n",
      "  inflating: dataset/1809.05255v2.txt  \n",
      "  inflating: dataset/1809.05679v3.txt  \n",
      "  inflating: dataset/1809.06858v1.txt  \n",
      "  inflating: dataset/1809.06963v3.txt  \n",
      "  inflating: dataset/1809.07428v1.txt  \n",
      "  inflating: dataset/1809.08037v2.txt  \n",
      "  inflating: dataset/1809.08353v2.txt  \n",
      "  inflating: dataset/1809.08370v1.txt  \n",
      "  inflating: dataset/1809.10324v2.txt  \n",
      "  inflating: dataset/1809.10853v3.txt  \n",
      "  inflating: dataset/1810.00494v1.txt  \n",
      "  inflating: dataset/1810.00952v1.txt  \n",
      "  inflating: dataset/1810.01170v1.txt  \n",
      "  inflating: dataset/1810.01861v2.txt  \n",
      "  inflating: dataset/1810.03167v1.txt  \n",
      "  inflating: dataset/1810.03552v3.txt  \n",
      "  inflating: dataset/1810.03660v1.txt  \n",
      "  inflating: dataset/1810.04805v2.txt  \n",
      "  inflating: dataset/1810.05334v5.txt  \n",
      "  inflating: dataset/1810.06682v2.txt  \n",
      "  inflating: dataset/1810.06683v3.txt  \n",
      "  inflating: dataset/1810.06825v1.txt  \n",
      "  inflating: dataset/1810.06860v1.txt  \n",
      "  inflating: dataset/1810.07091v1.txt  \n",
      "  inflating: dataset/1810.07150v3.txt  \n",
      "  inflating: dataset/1810.09177v3.txt  \n",
      "  inflating: dataset/1810.09305v1.txt  \n",
      "  inflating: dataset/1810.09311v1.txt  \n",
      "  inflating: dataset/1810.09536v6.txt  \n",
      "  inflating: dataset/1810.09995v1.txt  \n",
      "  inflating: dataset/1810.10181v1.txt  \n",
      "  inflating: dataset/1810.10804v3.txt  \n",
      "  inflating: dataset/1810.11921v2.txt  \n",
      "  inflating: dataset/1810.12836v4.txt  \n",
      "  inflating: dataset/1810.13338v1.txt  \n",
      "  inflating: dataset/1811.00606v2.txt  \n",
      "  inflating: dataset/1811.00854v1.txt  \n",
      "  inflating: dataset/1811.01088v2.txt  \n",
      "  inflating: dataset/1811.01136v2.txt  \n",
      "  inflating: dataset/1811.01713v1.txt  \n",
      "  inflating: dataset/1811.01908v1.txt  \n",
      "  inflating: dataset/1811.01910v2.txt  \n",
      "  inflating: dataset/1811.02084v1.txt  \n",
      "  inflating: dataset/1811.02549v5.txt  \n",
      "  inflating: dataset/1811.04540v1.txt  \n",
      "  inflating: dataset/1811.05082v2.txt  \n",
      "  inflating: dataset/1811.05475.txt  \n",
      "  inflating: dataset/1811.05544v1.txt  \n",
      "  inflating: dataset/1811.05949v1.txt  \n",
      "  inflating: dataset/1811.06965v5.txt  \n",
      "  inflating: dataset/1811.08129v1.txt  \n",
      "  inflating: dataset/1811.08883v1.txt  \n",
      "  inflating: dataset/1811.09362v2.txt  \n",
      "  inflating: dataset/1811.09386v1.txt  \n",
      "  inflating: dataset/1811.10792v3.txt  \n",
      "  inflating: dataset/1811.10804v1.txt  \n",
      "  inflating: dataset/1811.11431v3.txt  \n",
      "  inflating: dataset/1812.00176v1.txt  \n",
      "  inflating: dataset/1812.01187v2.txt  \n",
      "  inflating: dataset/1812.01207v1.txt  \n",
      "  inflating: dataset/1812.01353v5.txt  \n",
      "  inflating: dataset/1812.01504v4.txt  \n",
      "  inflating: dataset/1812.02303v2.txt  \n",
      "  inflating: dataset/1812.02971.txt  \n",
      "  inflating: dataset/1812.03825v1.txt  \n",
      "  inflating: dataset/1812.04412v1.txt  \n",
      "  inflating: dataset/1812.04616v3.txt  \n",
      "  inflating: dataset/1812.05271v1.txt  \n",
      "  inflating: dataset/1812.06705v1.txt  \n",
      "  inflating: dataset/1812.07617v2.txt  \n",
      "  inflating: dataset/1812.07809v1.txt  \n",
      "  inflating: dataset/1812.08928v1.txt  \n",
      "  inflating: dataset/1901.00158v2.txt  \n",
      "  inflating: dataset/1901.00603v2.txt  \n",
      "  inflating: dataset/1901.01703v4.txt  \n",
      "  inflating: dataset/1901.02262v2.txt  \n",
      "  inflating: dataset/1901.02860v3.txt  \n",
      "  inflating: dataset/1901.02985v2.txt  \n",
      "  inflating: dataset/1901.04112v1.txt  \n",
      "  inflating: dataset/1901.04555v2.txt  \n",
      "  inflating: dataset/1901.05534v2.txt  \n",
      "  inflating: dataset/1901.05816v1.txt  \n",
      "  inflating: dataset/1901.07291v1.txt  \n",
      "  inflating: dataset/1901.07696v2.txt  \n",
      "  inflating: dataset/1901.07786v1.txt  \n",
      "  inflating: dataset/1901.08149v2.txt  \n",
      "  inflating: dataset/1901.08634v2.txt  \n",
      "  inflating: dataset/1901.08746v3.txt  \n",
      "  inflating: dataset/1901.08907v1.txt  \n",
      "  inflating: dataset/1901.10430v2.txt  \n",
      "  inflating: dataset/1901.10444v1.txt  \n",
      "  inflating: dataset/1901.10548v4.txt  \n",
      "  inflating: dataset/1901.11117v4.txt  \n",
      "  inflating: dataset/1901.11196v2.txt  \n",
      "  inflating: dataset/1901.11459v2.txt  \n",
      "  inflating: dataset/1901.11504v2.txt  \n",
      "  inflating: dataset/1902.00087v4.txt  \n",
      "  inflating: dataset/1902.00438v2.txt  \n",
      "  inflating: dataset/1902.00863v2.txt  \n",
      "  inflating: dataset/1902.01382v3.txt  \n",
      "  inflating: dataset/1902.05196v1.txt  \n",
      "  inflating: dataset/1902.06022v1.txt  \n",
      "  inflating: dataset/1902.06188v2.txt  \n",
      "  inflating: dataset/1902.06236v1.txt  \n",
      "  inflating: dataset/1902.07153v2.txt  \n",
      "  inflating: dataset/1902.07153v2_18-31-32.txt  \n",
      "  inflating: dataset/1902.07816v2.txt  \n",
      "  inflating: dataset/1902.08009v1.txt  \n",
      "  inflating: dataset/1902.08850v3.txt  \n",
      "  inflating: dataset/1902.08955v1.txt  \n",
      "  inflating: dataset/1902.09113v2.txt  \n",
      "  inflating: dataset/1902.09243v2.txt  \n",
      "  inflating: dataset/1902.09314v2.txt  \n",
      "  inflating: dataset/1902.09362v2.txt  \n",
      "  inflating: dataset/1902.09757v1.txt  \n",
      "  inflating: dataset/1902.10339v4.txt  \n",
      "  inflating: dataset/1902.10461v3.txt  \n",
      "  inflating: dataset/1902.10547v3.txt  \n",
      "  inflating: dataset/1902.10623v2.txt  \n",
      "  inflating: dataset/1902.11049v2.txt  \n",
      "  inflating: dataset/1903.00138.txt  \n",
      "  inflating: dataset/1903.00138v3.txt  \n",
      "  inflating: dataset/1903.00142v1.txt  \n",
      "  inflating: dataset/1903.00241v1.txt  \n",
      "  inflating: dataset/1903.00905v2.txt  \n",
      "  inflating: dataset/1903.02188v3.txt  \n",
      "  inflating: dataset/1903.02831v1.txt  \n",
      "  inflating: dataset/1903.03714v1.txt  \n",
      "  inflating: dataset/1903.04561v2.txt  \n",
      "  inflating: dataset/1903.05734v1.txt  \n",
      "  inflating: dataset/1903.06620v2.txt  \n",
      "  inflating: dataset/1903.07926v2.txt  \n",
      "  inflating: dataset/1903.08289v2.txt  \n",
      "  inflating: dataset/1903.10145v3.txt  \n",
      "  inflating: dataset/1903.10433v1.txt  \n",
      "  inflating: dataset/1903.10520v1.txt  \n",
      "  inflating: dataset/1903.10676v3.txt  \n",
      "  inflating: dataset/1903.11410v2.txt  \n",
      "  inflating: dataset/1903.12090v1.txt  \n",
      "  inflating: dataset/1903.12457v3.txt  \n",
      "  inflating: dataset/1903.12626v1.txt  \n",
      "  inflating: dataset/1904.00132v2.txt  \n",
      "  inflating: dataset/1904.00648v1.txt  \n",
      "  inflating: dataset/1904.01038v1.txt  \n",
      "  inflating: dataset/1904.01301v2.txt  \n",
      "  inflating: dataset/1904.01608v1.txt  \n",
      "  inflating: dataset/1904.02020v2.txt  \n",
      "  inflating: dataset/1904.02399v4.txt  \n",
      "  inflating: dataset/1904.02514v3.txt  \n",
      "  inflating: dataset/1904.02682v1.txt  \n",
      "  inflating: dataset/1904.02792v1.txt  \n",
      "  inflating: dataset/1904.02839v1.txt  \n",
      "  inflating: dataset/1904.02954v1.txt  \n",
      "  inflating: dataset/1904.03279v2.txt  \n",
      "  inflating: dataset/1904.03288v3.txt  \n",
      "  inflating: dataset/1904.03396v2.txt  \n",
      "  inflating: dataset/1904.03651v2.txt  \n",
      "  inflating: dataset/1904.03746v6.txt  \n",
      "  inflating: dataset/1904.03889v1.txt  \n",
      "  inflating: dataset/1904.04365v4.txt  \n",
      "  inflating: dataset/1904.04447v1.txt  \n",
      "  inflating: dataset/1904.04514v1.txt  \n",
      "  inflating: dataset/1904.04547v1.txt  \n",
      "  inflating: dataset/1904.07418v1.txt  \n",
      "  inflating: dataset/1904.08067v4.txt  \n",
      "  inflating: dataset/1904.08128v1.txt  \n",
      "  inflating: dataset/1904.08378v1.txt  \n",
      "  inflating: dataset/1904.08398v3.txt  \n",
      "  inflating: dataset/1904.08779v2.txt  \n",
      "  inflating: dataset/1904.09223v1.txt  \n",
      "  inflating: dataset/1904.09675v1.txt  \n",
      "  inflating: dataset/1904.09816v1.txt  \n",
      "  inflating: dataset/1904.10273v1.txt  \n",
      "  inflating: dataset/1904.10322v1.txt  \n",
      "  inflating: dataset/1904.10367v1.txt  \n",
      "  inflating: dataset/1904.11475v1.txt  \n",
      "  inflating: dataset/1904.11544v2.txt  \n",
      "  inflating: dataset/1904.11829v3.txt  \n",
      "  inflating: dataset/1904.12575v1.txt  \n",
      "  inflating: dataset/1904.12683v2.txt  \n",
      "  inflating: dataset/1904.12796v3.txt  \n",
      "  inflating: dataset/1904.12848.txt  \n",
      "  inflating: dataset/1905.00453v1.txt  \n",
      "  inflating: dataset/1905.00616v2.txt  \n",
      "  inflating: dataset/1905.01263v1.txt  \n",
      "  inflating: dataset/1905.01338v1.txt  \n",
      "  inflating: dataset/1905.01395v1.txt  \n",
      "  inflating: dataset/1905.01976v1.txt  \n",
      "  inflating: dataset/1905.02244v4.txt  \n",
      "  inflating: dataset/1905.02423v3.txt  \n",
      "  inflating: dataset/1905.02450v5.txt  \n",
      "  inflating: dataset/1905.03072v3.txt  \n",
      "  inflating: dataset/1905.03704v1.txt  \n",
      "  inflating: dataset/1905.03752v1.txt  \n",
      "  inflating: dataset/1905.04363v2.txt  \n",
      "  inflating: dataset/1905.04413v3.txt  \n",
      "  inflating: dataset/1905.04564.txt  \n",
      "  inflating: dataset/1905.04847v1.txt  \n",
      "  inflating: dataset/1905.04877v1.txt  \n",
      "  inflating: dataset/1905.05583v2.txt  \n",
      "  inflating: dataset/1905.06316v1.txt  \n",
      "  inflating: dataset/1905.06482v1.txt  \n",
      "  inflating: dataset/1905.07129v3.txt  \n",
      "  inflating: dataset/1905.07504v2.txt  \n",
      "  inflating: dataset/1905.07799v2.txt  \n",
      "  inflating: dataset/1905.07854v2.txt  \n",
      "  inflating: dataset/1905.07870v4.txt  \n",
      "  inflating: dataset/1905.08108v1.txt  \n",
      "  inflating: dataset/1905.08836v1.txt  \n",
      "  inflating: dataset/1905.08880v1.txt  \n",
      "  inflating: dataset/1905.09205v2.txt  \n",
      "  inflating: dataset/1905.09217v1.txt  \n",
      "  inflating: dataset/1905.09248v3.txt  \n",
      "  inflating: dataset/1905.10072v1.txt  \n",
      "  inflating: dataset/1905.10536v1.txt  \n",
      "  inflating: dataset/1905.10630v1.txt  \n",
      "  inflating: dataset/1905.10752v1.txt  \n",
      "  inflating: dataset/1905.11096v2.txt  \n",
      "  inflating: dataset/1905.11596v1.txt  \n",
      "  inflating: dataset/1905.12616v1.txt  \n",
      "  inflating: dataset/1905.12897v2.txt  \n",
      "  inflating: dataset/1905.13127v1.txt  \n",
      "  inflating: dataset/1905.13129v1.txt  \n",
      "  inflating: dataset/1905.13132v1.txt  \n",
      "  inflating: dataset/1905.13656v3.txt  \n",
      "  inflating: dataset/1906.00091v1.txt  \n",
      "  inflating: dataset/1906.00346v1.txt  \n",
      "  inflating: dataset/1906.01213v3.txt  \n",
      "  inflating: dataset/1906.01684v2.txt  \n",
      "  inflating: dataset/1906.01910v1.txt  \n",
      "  inflating: dataset/1906.02192v1.txt  \n",
      "  inflating: dataset/1906.02242v1.txt  \n",
      "  inflating: dataset/1906.02365v1.txt  \n",
      "  inflating: dataset/1906.02780v1.txt  \n",
      "  inflating: dataset/1906.02829v1.txt  \n",
      "  inflating: dataset/1906.02900v1.txt  \n",
      "  inflating: dataset/1906.03134.txt  \n",
      "  inflating: dataset/1906.03221v1.txt  \n",
      "  inflating: dataset/1906.03672v1.txt  \n",
      "  inflating: dataset/1906.04165.txt  \n",
      "  inflating: dataset/1906.04281v1.txt  \n",
      "  inflating: dataset/1906.04341v1.txt  \n",
      "  inflating: dataset/1906.04501v1.txt  \n",
      "  inflating: dataset/1906.07155v1.txt  \n",
      "  inflating: dataset/1906.07241v2.txt  \n",
      "  inflating: dataset/1906.08101v1.txt  \n",
      "  inflating: dataset/1906.08237v1 (2).txt  \n",
      "  inflating: dataset/1906.08237v1 (3).txt  \n",
      "  inflating: dataset/1906.08237v1 (4).txt  \n",
      "  inflating: dataset/1906.08237v1 (5).txt  \n",
      "  inflating: dataset/1906.08237v1 (6).txt  \n",
      "  inflating: dataset/1906.08237v1.txt  \n",
      "  inflating: dataset/1906.08237v1_18-28-27.txt  \n",
      "  inflating: dataset/1906.08511v2.txt  \n",
      "  inflating: dataset/1906.08646.txt  \n",
      "  inflating: dataset/1906.08646v1.txt  \n",
      "  inflating: dataset/1906.08934v2.txt  \n",
      "  inflating: dataset/1906.09217v1.txt  \n",
      "  inflating: dataset/1906.09506v1.txt  \n",
      "  inflating: dataset/1906.09978v1.txt  \n",
      "  inflating: dataset/1906.12230v1.txt  \n",
      "  inflating: dataset/1906.12330v1.txt  \n",
      "  inflating: dataset/1907.01968v1.txt  \n",
      "  inflating: dataset/1907.03752v1.txt  \n",
      "  inflating: dataset/1907.03802.txt  \n",
      "  inflating: dataset/1907.03802v1.txt  \n",
      "  inflating: dataset/1907.05190v1.txt  \n",
      "  inflating: dataset/1907.05242v1.txt  \n",
      "  inflating: dataset/1907.05338v1.txt  \n",
      "  inflating: dataset/1907.05982v1.txt  \n",
      "  inflating: dataset/1907.06902v3.txt  \n",
      "  inflating: dataset/1907.07247v1.txt  \n",
      "  inflating: dataset/1907.08346v1.txt  \n",
      "  inflating: dataset/1907.08610v1.txt  \n",
      "  inflating: dataset/1907.08679v1.txt  \n",
      "  inflating: dataset/1907.10371v1.txt  \n",
      "  inflating: dataset/1907.10529v2.txt  \n",
      "  inflating: dataset/1907.11692v1.txt  \n",
      "  inflating: dataset/1907.11932v2.txt  \n",
      "  inflating: dataset/1907.11983v1.txt  \n",
      "  inflating: dataset/1907.12412v1.txt  \n",
      "  inflating: dataset/1907.12484v1.txt  \n",
      "  inflating: dataset/1907.12665v1.txt  \n",
      "  inflating: dataset/1908.00413v1.txt  \n",
      "  inflating: dataset/1908.01832v1.txt  \n",
      "  inflating: dataset/1908.01853v1.txt  \n",
      "  inflating: dataset/1908.03265v1.txt  \n",
      "  inflating: dataset/1908.03557v1.txt  \n",
      "  inflating: dataset/1908.04364v2.txt  \n",
      "  inflating: dataset/1908.05391v2.txt  \n",
      "  inflating: dataset/1908.05428v1.txt  \n",
      "  inflating: dataset/1908.06039v1.txt  \n",
      "  inflating: dataset/1908.06267v1.txt  \n",
      "  inflating: dataset/1908.07123v2.txt  \n",
      "  inflating: dataset/1908.07125v2.txt  \n",
      "  inflating: dataset/1908.07195v1.txt  \n",
      "  inflating: dataset/1908.07490v2.txt  \n",
      "  inflating: dataset/1908.07738v1.txt  \n",
      "  inflating: dataset/1908.08326v8.txt  \n",
      "  inflating: dataset/1908.08345v2.txt  \n",
      "  inflating: dataset/1908.08835v1.txt  \n",
      "  inflating: dataset/1908.09876v1.txt  \n",
      "  inflating: dataset/1908.09972v1.txt  \n",
      "  inflating: dataset/1908.10383v1.txt  \n",
      "  inflating: dataset/1908.10419v1.txt  \n",
      "  inflating: dataset/1908.11355v1.txt  \n",
      "  inflating: dataset/1909.00015v2.txt  \n",
      "  inflating: dataset/1909.00161v1.txt  \n",
      "  inflating: dataset/1909.00385v1.txt  \n",
      "  inflating: dataset/1909.00430v1.txt  \n",
      "  inflating: dataset/1909.00599v1.txt  \n",
      "  inflating: dataset/1909.01066v2.txt  \n",
      "  inflating: dataset/1909.01259.txt  \n",
      "  inflating: dataset/1909.01259v2.txt  \n",
      "  inflating: dataset/1909.01259_18-35-01.txt  \n",
      "  inflating: dataset/1909.01377v1.txt  \n",
      "  inflating: dataset/1909.02027v1.txt  \n",
      "  inflating: dataset/1909.02107v1.txt  \n",
      "  inflating: dataset/1909.02164v2.txt  \n",
      "  inflating: dataset/1909.02480v1.txt  \n",
      "  inflating: dataset/1909.03477v1.txt  \n",
      "  inflating: dataset/1909.03564v1.txt  \n",
      "  inflating: dataset/1909.04076v2.txt  \n",
      "  inflating: dataset/1909.04985v1.txt  \n",
      "  inflating: dataset/1909.06695v1.txt  \n",
      "  inflating: dataset/1909.08053v2.txt  \n",
      "  inflating: dataset/1909.08593v1.txt  \n",
      "  inflating: dataset/1909.08723v1.txt  \n",
      "  inflating: dataset/1909.08837v1.txt  \n",
      "  inflating: dataset/2016_interspeech_mmi.txt  \n",
      "  inflating: dataset/28ff6712d62fef4d4846fca5be5df06a8ffd41d2.txt  \n",
      "  inflating: dataset/5360-content-based-recommendations-with-poisson-factorization.txt  \n",
      "  inflating: dataset/6139-supervised-word-movers-distance.txt  \n",
      "  inflating: dataset/684662.full.txt  \n",
      "  inflating: dataset/7859-deep-neural-networks-with-box-convolutions.txt  \n",
      "  inflating: dataset/aa0bc6d2b33199a5dc3aba019adbc0440fbeecc6.txt  \n",
      "  inflating: dataset/Abushariah - 2012 - Automatic Continuous Speech Recognition based on Phonetically Rich and Balanced Arabic Speech Corpus-annotated.txt  \n",
      "  inflating: dataset/Abushariah et al. - 2010 - Natural speaker-independent Arabic speech recognition system based on Hidden Markov Models using Sphinx tools-annotated.txt  \n",
      "  inflating: dataset/Abuzeina, Al-Khatib, Elshafei - 2011 - Small-word pronunciation modeling for Arabic speech recognition A data-driven approach-annotated.txt  \n",
      "  inflating: dataset/agarwal18b.txt  \n",
      "  inflating: dataset/Ahmed, Ghabayen - 2017 - Arabic Automatic Speech Recognition Enhancement-annotated.txt  \n",
      "  inflating: dataset/Al-Abdullah et al. - 2019 - Artificial Neural Network for Arabic Speech Recognition in Humanoid Robotic Systems-annotated.txt  \n",
      "  inflating: dataset/Al-Anzi, Abuzeina - Unknown - Literature Survey of Arabic Speech Recognition-annotated.txt  \n",
      "  inflating: dataset/Al-Qatab, Ainon - 2010 - Arabic speech recognition using Hidden Markov Model Toolkit(HTK)-annotated.txt  \n",
      "  inflating: dataset/Ali et al. - 2008 - Generation of arabic phonetic dictionaries for speech recognition-annotated.txt  \n",
      "  inflating: dataset/Beier_Fusion_Moves_for_2015_CVPR_paper.txt  \n",
      "  inflating: dataset/Besacier et al. - 2014 - Automatic speech recognition for under-resourced languages A survey-annotated.txt  \n",
      "  inflating: dataset/blocksparsepaper.txt  \n",
      "  inflating: dataset/C14-1212.txt    \n",
      "  inflating: dataset/C16-1116.txt    \n",
      "  inflating: dataset/C16-1147.txt    \n",
      "  inflating: dataset/C16-1150.txt    \n",
      "  inflating: dataset/C16-2064.txt    \n",
      "  inflating: dataset/C18-1003.txt    \n",
      "  inflating: dataset/C18-1005.txt    \n",
      "  inflating: dataset/C18-1041.txt    \n",
      "  inflating: dataset/C18-1070.txt    \n",
      "  inflating: dataset/C18-1079.txt    \n",
      "  inflating: dataset/C18-1139.txt    \n",
      "  inflating: dataset/C18-1168.txt    \n",
      "  inflating: dataset/C18-1172.txt    \n",
      "  inflating: dataset/C18-1174.txt    \n",
      "  inflating: dataset/C18-1230.txt    \n",
      "  inflating: dataset/C18-1237.txt    \n",
      "  inflating: dataset/C18-1239.txt    \n",
      "  inflating: dataset/C18-1312.txt    \n",
      "  inflating: dataset/C18-2029.txt    \n",
      "  inflating: dataset/Camgoz_Neural_Sign_Language_CVPR_2018_paper.txt  \n",
      "  inflating: dataset/Categorization of Security Design Patterns.txt  \n",
      "  inflating: dataset/ctrl.txt        \n",
      "  inflating: dataset/D14-1162.txt    \n",
      "  inflating: dataset/D15-1063.txt    \n",
      "  inflating: dataset/D15-1243.txt    \n",
      "  inflating: dataset/D16-1030.txt    \n",
      "  inflating: dataset/D16-1123.txt    \n",
      "  inflating: dataset/D16-1169.txt    \n",
      "  inflating: dataset/D16-1171.txt    \n",
      "  inflating: dataset/D16-1191.txt    \n",
      "  inflating: dataset/D16-1193.txt    \n",
      "  inflating: dataset/D16-1250.txt    \n",
      "  inflating: dataset/D16-1257.txt    \n",
      "  inflating: dataset/D17-1024.txt    \n",
      "  inflating: dataset/D17-1047.txt    \n",
      "  inflating: dataset/D17-1296.txt    \n",
      "  inflating: dataset/D18-1032.txt    \n",
      "  inflating: dataset/D18-1033.txt    \n",
      "  inflating: dataset/D18-1084.txt    \n",
      "  inflating: dataset/D18-1094.txt    \n",
      "  inflating: dataset/D18-1129.txt    \n",
      "  inflating: dataset/D18-1181.txt    \n",
      "  inflating: dataset/D18-1237.txt    \n",
      "  inflating: dataset/D18-1274.txt    \n",
      "  inflating: dataset/D18-1280.txt    \n",
      "  inflating: dataset/D18-1390.txt    \n",
      "  inflating: dataset/D18-1424.txt    \n",
      "  inflating: dataset/D18-1495.txt    \n",
      "  inflating: dataset/D18-1532.txt    \n",
      "  inflating: dataset/D18-2010.txt    \n",
      "  inflating: dataset/D18-2024.txt    \n",
      "  inflating: dataset/E14-1056.txt    \n",
      "  inflating: dataset/E17-1011.txt    \n",
      "  inflating: dataset/E17-1067.txt    \n",
      "  inflating: dataset/E17-2082.txt    \n",
      "  inflating: dataset/E17-2092.txt    \n",
      "  inflating: dataset/e850be08cfaa00ece0d2dcba1c26fddd2107.txt  \n",
      "  inflating: dataset/Essa, Tolba, Elmougy - 2008 - A comparison of combined classifier architectures for arabic speech recognition-annotated.txt  \n",
      "  inflating: dataset/fulltext.txt    \n",
      "  inflating: dataset/Gayar et al. - 2018 - Arabic Speech Recognition Challenges and State of the Art-annotated.txt  \n",
      "  inflating: dataset/gcbc19-wsdm.txt  \n",
      "  inflating: dataset/Ghai, Singh - 2012 - Literature Review on Automatic Speech Recognition-annotated.txt  \n",
      "  inflating: dataset/Hmad - 2015 - Deep Neural Network Acoustic models for Multi-dialect Arabic Speech Recognition Arabic Speech Recognition-annotated.txt  \n",
      "  inflating: dataset/Hyassat, Abu Zitar - 2006 - Arabic speech recognition using SPHINX engine-annotated.txt  \n",
      "  inflating: dataset/IR-1132.txt     \n",
      "  inflating: dataset/J18-2005.txt    \n",
      "  inflating: dataset/K17-1009.txt    \n",
      "  inflating: dataset/K17-1010.txt    \n",
      "  inflating: dataset/K18-1018.txt    \n",
      "  inflating: dataset/K18-2004.txt    \n",
      "  inflating: dataset/Kirchhoff et al. - 2006 - Morphology-based language modeling for conversational Arabic speech recognition-annotated.txt  \n",
      "  inflating: dataset/kumar, Hussian, Ali - 2014 - Experimental Investigation on Turbulent Heat Transfer, Nusselt Number and Friction Factor Characteristics o-annotated.txt  \n",
      "  inflating: dataset/L16-1662.txt    \n",
      "  inflating: dataset/L18-1043.txt    \n",
      "  inflating: dataset/L18-1085.txt    \n",
      "  inflating: dataset/L18-1221.txt    \n",
      "  inflating: dataset/L18-1404.txt    \n",
      "  inflating: dataset/L18-1470.txt    \n",
      "  inflating: dataset/L18-1510.txt    \n",
      "  inflating: dataset/L18-1530.txt    \n",
      "  inflating: dataset/L18-1714.txt    \n",
      "  inflating: dataset/Labidi, Maraoui, Zrigui - Unknown - Unsupervised Method for Improving Arabic Speech Recognition Systems-annotated.txt  \n",
      "  inflating: dataset/Langley2011_Article_TheChangingScienceOfMachineLea.txt  \n",
      "  inflating: dataset/language-models.txt  \n",
      "  inflating: dataset/language_understanding_paper.txt  \n",
      "  inflating: dataset/Lee - Unknown - Lee_K_F_1990_1.Pdf-annotated.txt  \n",
      "  inflating: dataset/Li et al. - 2016 - Introduction-annotated.txt  \n",
      "  inflating: dataset/Likelihood of a Personal Computer to Be Infected with Malware.txt  \n",
      "  inflating: dataset/liu2014.txt     \n",
      "  inflating: dataset/muller18a.txt   \n",
      "  inflating: dataset/N13-1030.txt    \n",
      "  inflating: dataset/N13-1073.txt    \n",
      "  inflating: dataset/N15-3009.txt    \n",
      "  inflating: dataset/N16-1119.txt    \n",
      "  inflating: dataset/N16-1139.txt    \n",
      "  inflating: dataset/N18-1103.txt    \n",
      "  inflating: dataset/N18-1140.txt    \n",
      "  inflating: dataset/N18-2045.txt    \n",
      "  inflating: dataset/N19-1259.txt    \n",
      "  inflating: dataset/N19-1408.txt    \n",
      "  inflating: dataset/N19-2001.txt    \n",
      "  inflating: dataset/N19-3018.txt    \n",
      "  inflating: dataset/N19-4017.txt    \n",
      "  inflating: dataset/Narang, Divya Gupta - 2015 - International Journal of Computer Science and Mobile Computing Speech Feature Extraction Techniques A Revie-annotated.txt  \n",
      "  inflating: dataset/neural-network-language.txt  \n",
      "  inflating: dataset/ocz149.txt      \n",
      "  inflating: dataset/P13-1013.txt    \n",
      "  inflating: dataset/P13-1115.txt    \n",
      "  inflating: dataset/P14-1022.txt    \n",
      "  inflating: dataset/P14-1024.txt    \n",
      "  inflating: dataset/P14-1108.txt    \n",
      "  inflating: dataset/P15-1044.txt    \n",
      "  inflating: dataset/P16-2008.txt    \n",
      "  inflating: dataset/P18-1215.txt    \n",
      "  inflating: dataset/S19-1015.txt    \n",
      "  inflating: dataset/W17-4714.txt    \n",
      "  inflating: dataset/1-s2.0-S2211381912001087-main.txt  \n",
      "  inflating: dataset/1409.2944v2.txt  \n",
      "  inflating: dataset/1411.4952v3.txt  \n",
      "  inflating: dataset/1504.04317v1.txt  \n",
      "  inflating: dataset/1508.04999v3.txt  \n",
      "  inflating: dataset/1511.07916v1.txt  \n",
      "  inflating: dataset/1605.03795v3.txt  \n",
      "  inflating: dataset/1606.02891v2.txt  \n",
      "  inflating: dataset/1606.07792v1.txt  \n",
      "  inflating: dataset/1609.04621v1.txt  \n",
      "  inflating: dataset/1609.07033v1.txt  \n",
      "  inflating: dataset/1610.02424v2.txt  \n",
      "  inflating: dataset/1611.06639v1.txt  \n",
      "  inflating: dataset/1701.04783v1.txt  \n",
      "  inflating: dataset/1702.08139v2.txt  \n",
      "  inflating: dataset/1704.05091v1.txt  \n",
      "  inflating: dataset/1704.07657v3.txt  \n",
      "  inflating: dataset/1705.10498v2.txt  \n",
      "  inflating: dataset/1706.09254v2.txt  \n",
      "  inflating: dataset/1708.00781v1.txt  \n",
      "  inflating: dataset/1708.03436v1.txt  \n",
      "  inflating: dataset/1709.01121v2.txt  \n",
      "  inflating: dataset/1709.05027v7.txt  \n",
      "  inflating: dataset/1710.05370v1.txt  \n",
      "  inflating: dataset/1710.11041v2.txt  \n",
      "  inflating: dataset/1803.05170v3.txt  \n",
      "  inflating: dataset/1804.00344v3.txt  \n",
      "  inflating: dataset/1804.09057v1.txt  \n",
      "  inflating: dataset/1805.04803v1.txt  \n",
      "  inflating: dataset/1806.05662v3.txt  \n",
      "  inflating: dataset/1807.03096v3.txt  \n",
      "  inflating: dataset/1807.03756v2.txt  \n",
      "  inflating: dataset/1808.09381v2.txt  \n",
      "  inflating: dataset/1809.02847v2.txt  \n",
      "  inflating: dataset/1810.12936v1.txt  \n",
      "  inflating: dataset/1811.00347v2.txt  \n",
      "  inflating: dataset/1811.10996v1.txt  \n",
      "  inflating: dataset/1812.09355v1.txt  \n",
      "  inflating: dataset/1901.06168v1.txt  \n",
      "  inflating: dataset/1901.09321v2.txt  \n",
      "  inflating: dataset/1901.09821v1.txt  \n",
      "  inflating: dataset/1901.10125v4.txt  \n",
      "  inflating: dataset/1902.04094v2.txt  \n",
      "  inflating: dataset/1903.09588v1.txt  \n",
      "  inflating: dataset/1904.02338v2.txt  \n",
      "  inflating: dataset/1904.12058v1.txt  \n",
      "  inflating: dataset/1905.10070v2.txt  \n",
      "  inflating: dataset/1905.12777v1.txt  \n",
      "  inflating: dataset/1906.01161v2.txt  \n",
      "  inflating: dataset/1906.03820v1.txt  \n",
      "  inflating: dataset/1906.04386v1.txt  \n",
      "  inflating: dataset/1907.05572v1.txt  \n",
      "  inflating: dataset/1908.04319v1.txt  \n",
      "  inflating: dataset/1908.08788v1.txt  \n",
      "  inflating: dataset/1908.09701v1.txt  \n",
      "  inflating: dataset/1909.09436v1.txt  \n",
      "  inflating: dataset/7081-dropoutnet-addressing-cold-start-in-recommender-systems.txt  \n",
      "  inflating: dataset/D15-1299.txt    \n",
      "  inflating: dataset/L18-1241.txt    \n",
      "  inflating: dataset/Mubarak, Darwish - 2015 - Using Twitter to Collect a Multi-Dialectal Corpus of Arabic-annotated.txt  \n",
      "  inflating: dataset/N15-1142.txt    \n",
      "  inflating: dataset/P14-2089.txt    \n",
      "  inflating: dataset/P15-1119.txt    \n",
      "  inflating: dataset/P15-1162.txt    \n",
      "  inflating: dataset/P16-1085.txt    \n",
      "  inflating: dataset/P16-1110.txt    \n",
      "  inflating: dataset/P16-2036.txt    \n",
      "  inflating: dataset/P16-2080.txt    \n",
      "  inflating: dataset/P16-4016.txt    \n",
      "  inflating: dataset/P17-1052.txt    \n",
      "  inflating: dataset/P17-1081.txt    \n",
      "  inflating: dataset/P17-1121.txt    \n",
      "  inflating: dataset/P17-1158.txt    \n",
      "  inflating: dataset/P17-1187.txt    \n",
      "  inflating: dataset/P17-3007.txt    \n",
      "  inflating: dataset/P18-1014.txt    \n",
      "  inflating: dataset/P18-1032.txt    \n",
      "  inflating: dataset/P18-1079.txt    \n",
      "  inflating: dataset/P18-1103.txt    \n",
      "  inflating: dataset/P18-1114.txt    \n",
      "  inflating: dataset/P18-1161.txt    \n",
      "  inflating: dataset/P18-1171.txt    \n",
      "  inflating: dataset/P18-1226.txt    \n",
      "  inflating: dataset/P18-2112.txt    \n",
      "  inflating: dataset/P18-4005.txt    \n",
      "  inflating: dataset/P19-1103.txt    \n",
      "  inflating: dataset/P19-1189.txt    \n",
      "  inflating: dataset/P19-1227.txt    \n",
      "  inflating: dataset/P19-1501.txt    \n",
      "  inflating: dataset/P19-2057.txt    \n",
      "  inflating: dataset/Q16-1017.txt    \n",
      "  inflating: dataset/Q18-1028.txt    \n",
      "  inflating: dataset/R13-1074.txt    \n",
      "  inflating: dataset/Richard_Temporal_Action_Detection_CVPR_2016_paper.txt  \n",
      "  inflating: dataset/S14-2139.txt    \n",
      "  inflating: dataset/S17-2126.txt    \n",
      "  inflating: dataset/Saini, Kaur - 2013 - Automatic Speech Recognition A Review-annotated.txt  \n",
      "  inflating: dataset/sparse_transformers.txt  \n",
      "  inflating: dataset/suri2002.txt    \n",
      "  inflating: dataset/swj1738.txt     \n",
      "  inflating: dataset/U17-1006.txt    \n",
      "  inflating: dataset/Unknown - 2009 - Arabic statistical N-Gram models-annotated.txt  \n",
      "  inflating: dataset/Vergyri, Kirchhoff - Unknown - Automatic diacritization of Arabic for Acoustic Modeling in Speech Recognition-annotated.txt  \n",
      "  inflating: dataset/VimalaC - Unknown - A Review on Speech Recognition Challenges and Approaches-annotated.txt  \n",
      "  inflating: dataset/W15-2709v2.txt  \n",
      "  inflating: dataset/W16-1626.txt    \n",
      "  inflating: dataset/W16-3622.txt    \n",
      "  inflating: dataset/W16-4616.txt    \n",
      "  inflating: dataset/W17-1003.txt    \n",
      "  inflating: dataset/W17-1902.txt    \n",
      "  inflating: dataset/W17-5714.txt    \n",
      "  inflating: dataset/W18-0205.txt    \n",
      "  inflating: dataset/W18-1707.txt    \n",
      "  inflating: dataset/W18-2506.txt    \n",
      "  inflating: dataset/W18-3012.txt    \n",
      "  inflating: dataset/W18-6230.txt    \n",
      "  inflating: dataset/W18-6402.txt    \n",
      "  inflating: dataset/W18-6521.txt    \n",
      "  inflating: dataset/W18-6557.txt    \n",
      "  inflating: dataset/W19-2006.txt    \n",
      "  inflating: dataset/W19-3506.txt    \n",
      "  inflating: dataset/W19-4608.txt    \n",
      "  inflating: dataset/W19-4621.txt    \n",
      "  inflating: dataset/yin1993.txt     \n",
      "  inflating: dataset/Q16-1029.txt    \n",
      "  inflating: dataset/Wahyuni - 2018 - Arabic speech recognition using MFCC feature extraction and ANN classification-annotated.txt  \n"
     ]
    }
   ],
   "source": [
    "#unzip the dataset folder\n",
    "!unzip ./drive/MyDrive/dataset.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c979c",
   "metadata": {
    "id": "198c979c"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac58d753",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac58d753",
    "outputId": "4bb33356-7b9e-4275-c74a-f036df561a9c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/karmel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/karmel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/karmel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/karmel/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2022-05-11 15:25:58.878825: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-11 15:25:58.878861: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ec03206",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ec03206",
    "outputId": "7206b479-5978-4372-b5f4-063c48e03e52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1064"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the names of each folders\n",
    "files_names = os.listdir('./dataset')\n",
    "len(files_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0f3999",
   "metadata": {
    "id": "cc0f3999"
   },
   "source": [
    "# Convert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24dd0950",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24dd0950",
    "outputId": "777058cd-b335-4367-864a-a83d343da852"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data frame is Empty DataFrame\n",
      "Columns: [id, articles]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame([], columns = [\"id\", \"articles\"])\n",
    "print(f\"data frame is {df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "706513d6",
   "metadata": {
    "id": "706513d6"
   },
   "outputs": [],
   "source": [
    "def convert_to_df(df, files_names):\n",
    "        id = []\n",
    "        articles= []\n",
    "        # Define files place and read each sub file for cleaning\n",
    "        for index, file in enumerate(files_names):\n",
    "                # read each article files\n",
    "                path = os.getcwd()\n",
    "                file_path = '{}/dataset/{}'.format(path,file)\n",
    "                with open(file_path,'r', encoding='utf-16',  errors=\"ignore\") as f:\n",
    "                    texts = f.read() \n",
    "                    articles.append(texts) \n",
    "                    id.append(index)\n",
    "                    f.close()\n",
    "        print(\"the ids\", id)\n",
    "        print(\"the articles\", articles)\n",
    "        df['id'] = id\n",
    "        df[\"articles\"] = articles\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd7c238f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd7c238f",
    "outputId": "b10c00b9-9a40-430e-8495-c6684d5be17d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = convert_to_df(df, files_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "174e36e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "174e36e3",
    "outputId": "f423a30c-89ef-4b57-d8f6-a4ed35418313"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Statistical Machine Translation\\n\\nDraft of Ch...\n",
       "1       Found in Translation:\\n\\nLearning Robust Joint...\n",
       "2       Available online at www.sciencedirect.com\\n\\nS...\n",
       "3       J OURNAL OF I NFORMATION S CIENCE AND E NGINEE...\n",
       "4       The University of Sheﬃeld\\n\\nT. E. Dunning\\n\\n...\n",
       "                              ...                        \n",
       "1059    Multi-label Hate Speech and Abusive Language D...\n",
       "1060    hULMonA (      ): The Universal Language Model...\n",
       "1061    Mazajak: An Online Arabic Sentiment Analyser\\n...\n",
       "1062    2017 2nd International Conferences on Informat...\n",
       "1063    INVESTIGATIVE RADIOLOGY Volume 28, Number 6, 4...\n",
       "Name: articles, Length: 1064, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"articles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f19114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_df = pd.DataFrame(df['articles'].iloc[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d16683f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d16683f2",
    "outputId": "ca54dc4e-938a-4f42-c68f-a27e6f52b0aa"
   },
   "outputs": [],
   "source": [
    "# count the number of words for each doc\n",
    "limited_df['word_count'] = limited_df['articles'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66466052",
   "metadata": {
    "id": "66466052"
   },
   "source": [
    " # Text pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8352e695",
   "metadata": {
    "id": "8352e695"
   },
   "outputs": [],
   "source": [
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    # Remove leading and trailing spaces\n",
    "    text= text.strip()  \n",
    "    text= re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text= re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    # get ride of one letter word!\n",
    "    text = re.sub(r'\\b\\w\\b','',text) \n",
    "    \n",
    "      \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "HqeYuDXZNB-h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HqeYuDXZNB-h",
    "outputId": "18645fd2-10c3-455f-9ab7-c18de2692ae8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/karmel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42fe2349",
   "metadata": {
    "id": "42fe2349"
   },
   "outputs": [],
   "source": [
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe6a93b8",
   "metadata": {
    "id": "fe6a93b8"
   },
   "outputs": [],
   "source": [
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e99623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd034880",
   "metadata": {
    "id": "fd034880"
   },
   "outputs": [],
   "source": [
    "#Remove non English words\n",
    "dictionary = enchant.Dict(\"en_US\")\n",
    "   \n",
    "def remove_non_english(text):\n",
    "        en_words= []\n",
    "        for word in text:\n",
    "            if dictionary.check(word):\n",
    "                en_words.append(word)\n",
    "        text = ' '.join(str(e) for e in en_words)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f837434",
   "metadata": {
    "id": "2f837434"
   },
   "outputs": [],
   "source": [
    "limited_df['english_text'] =limited_df['articles'].apply(lambda text: remove_non_english(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf3b5414",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "bf3b5414",
    "outputId": "acc7d6a8-1596-4cbe-f333-958985071431"
   },
   "outputs": [],
   "source": [
    "#limited_df['english_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d58900d1",
   "metadata": {
    "id": "d58900d1"
   },
   "outputs": [],
   "source": [
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51Xh5mpTM4nW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "51Xh5mpTM4nW",
    "outputId": "4a2b49e6-2d35-4100-f27d-b4d18323f5e0"
   },
   "outputs": [],
   "source": [
    "limited_df['clean_text'] = limited_df['english_text'].apply(lambda x: finalpreprocess(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec6fc8cf",
   "metadata": {
    "id": "ec6fc8cf"
   },
   "outputs": [],
   "source": [
    "articles = limited_df['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8348db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec\n",
    "# Word2Vec runs on tokenized sentences\n",
    "tok = [nltk.word_tokenize(i) for i in articles]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30c73b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tok[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64dab654",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(tok, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4930106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector = model.wv['computer']  # get numpy vector of a word\n",
    "#sims = model.wv.most_similar('computer', topn=10)  # get other similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c71f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store just the words + their trained embeddings.\n",
    "word_vectors = model.wv\n",
    "word_vectors.save(\"word2vec.wordvectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccc7b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_vectors.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "567d94ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building Word2Vec model\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb4d4519",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))\n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "# converting texts to numerical data using Word2Vec\n",
    "vectors_w2v = modelw.transform(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8e6f733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectors_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a68faaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d70422e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (1, 2), (2, 2), (3, 2), (4, 14), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 2), (13, 1), (14, 2), (15, 8), (16, 1), (17, 1), (18, 1), (19, 1)]\n"
     ]
    }
   ],
   "source": [
    "id2word = corpora.Dictionary(tok)\n",
    "\n",
    "corpus = []\n",
    "for text in tok:\n",
    "    new = id2word.doc2bow(text)\n",
    "    corpus.append(new)\n",
    "\n",
    "print (corpus[0][0:20])\n",
    "# Create Dictionary\n",
    "word = id2word[[0][:1][0]]\n",
    "#print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b82523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal number of Topics for LDA\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus,texts, limit, start,step,alpha, beta):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=alpha,\n",
    "                                           per_word_topics=True,\n",
    "                                           eta = beta)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d271cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "\n",
    "#alpha_list = ['symmetric',0.3,0.5,0.7]\n",
    "#beta_list = ['auto',0.3,0.5,0.7]\n",
    "\n",
    "\n",
    "#for alpha in alpha_list:\n",
    "        #for beta in beta_list:\n",
    "            #model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=tok, start=2, limit=40, step=6,alpha=alpha,beta =beta)\n",
    "            #print(f\"alpha : {alpha} ; beta : {beta} ; Score : {coherence_values}\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c875578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show graph\n",
    "# limit=40; start=2; step=6;\n",
    "# x = range(start, limit, step)\n",
    "# plt.plot(x, coherence_values)\n",
    "# plt.xlabel(\"Num Topics\")\n",
    "# plt.ylabel(\"Coherence score\")\n",
    "# plt.legend((\"coherence_values\"), loc='best')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b41c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the ladmodel with the best parameter after the tunning\n",
    "# alpha string\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=32,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           per_word_topics=True,\n",
    "                                           eta = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce014d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.38900823601447\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherencemodel = CoherenceModel(model=lda_model, texts=tok, dictionary=id2word, coherence='c_v')\n",
    "coherence= coherencemodel.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4b2a31ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.000*\"use\" + 0.000*\"model\" + 0.000*\"word\" + 0.000*\"data\" + 0.000*\"network\" + 0.000*\"language\" + 0.000*\"train\" + 0.000*\"learn\" + 0.000*\"sequence\" + 0.000*\"neural\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.018*\"word\" + 0.016*\"use\" + 0.013*\"model\" + 0.011*\"vector\" + 0.010*\"image\" + 0.009*\"sentence\" + 0.008*\"learn\" + 0.007*\"train\" + 0.006*\"set\" + 0.006*\"question\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.000*\"model\" + 0.000*\"use\" + 0.000*\"word\" + 0.000*\"language\" + 0.000*\"set\" + 0.000*\"learn\" + 0.000*\"system\" + 0.000*\"data\" + 0.000*\"result\" + 0.000*\"network\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.022*\"model\" + 0.017*\"word\" + 0.016*\"neural\" + 0.015*\"use\" + 0.014*\"network\" + 0.011*\"translation\" + 0.009*\"language\" + 0.009*\"input\" + 0.009*\"train\" + 0.008*\"machine\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.000*\"use\" + 0.000*\"model\" + 0.000*\"word\" + 0.000*\"data\" + 0.000*\"test\" + 0.000*\"method\" + 0.000*\"learn\" + 0.000*\"set\" + 0.000*\"system\" + 0.000*\"base\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.017*\"arc\" + 0.014*\"km\" + 0.013*\"flow\" + 0.009*\"hub\" + 0.008*\"route\" + 0.006*\"cost\" + 0.004*\"algorithm\" + 0.003*\"transport\" + 0.003*\"otherwise\" + 0.003*\"solution\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.001*\"use\" + 0.001*\"model\" + 0.001*\"term\" + 0.001*\"test\" + 0.001*\"method\" + 0.001*\"word\" + 0.000*\"likelihood\" + 0.000*\"document\" + 0.000*\"log\" + 0.000*\"weight\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.000*\"model\" + 0.000*\"use\" + 0.000*\"word\" + 0.000*\"neural\" + 0.000*\"network\" + 0.000*\"learn\" + 0.000*\"language\" + 0.000*\"train\" + 0.000*\"sequence\" + 0.000*\"test\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.000*\"use\" + 0.000*\"model\" + 0.000*\"measure\" + 0.000*\"set\" + 0.000*\"class\" + 0.000*\"true\" + 0.000*\"word\" + 0.000*\"base\" + 0.000*\"predict\" + 0.000*\"result\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.012*\"naive\" + 0.011*\"bayes\" + 0.010*\"class\" + 0.009*\"probability\" + 0.006*\"text\" + 0.005*\"feature\" + 0.005*\"conditional\" + 0.004*\"sample\" + 0.004*\"prior\" + 0.004*\"spam\"\n",
      "\n",
      "\n",
      "Topic: 10 \n",
      "Words: 0.026*\"domain\" + 0.011*\"adaptation\" + 0.010*\"target\" + 0.009*\"source\" + 0.008*\"data\" + 0.007*\"learn\" + 0.007*\"set\" + 0.006*\"label\" + 0.006*\"image\" + 0.005*\"adversarial\"\n",
      "\n",
      "\n",
      "Topic: 11 \n",
      "Words: 0.026*\"term\" + 0.017*\"weight\" + 0.013*\"scheme\" + 0.009*\"category\" + 0.006*\"base\" + 0.006*\"classifier\" + 0.005*\"text\" + 0.004*\"task\" + 0.004*\"prob\" + 0.004*\"use\"\n",
      "\n",
      "\n",
      "Topic: 12 \n",
      "Words: 0.000*\"model\" + 0.000*\"use\" + 0.000*\"word\" + 0.000*\"neural\" + 0.000*\"network\" + 0.000*\"learn\" + 0.000*\"data\" + 0.000*\"text\" + 0.000*\"train\" + 0.000*\"time\"\n",
      "\n",
      "\n",
      "Topic: 13 \n",
      "Words: 0.000*\"model\" + 0.000*\"use\" + 0.000*\"word\" + 0.000*\"network\" + 0.000*\"neural\" + 0.000*\"language\" + 0.000*\"data\" + 0.000*\"translation\" + 0.000*\"test\" + 0.000*\"layer\"\n",
      "\n",
      "\n",
      "Topic: 14 \n",
      "Words: 0.000*\"word\" + 0.000*\"model\" + 0.000*\"use\" + 0.000*\"language\" + 0.000*\"train\" + 0.000*\"data\" + 0.000*\"neural\" + 0.000*\"system\" + 0.000*\"learn\" + 0.000*\"machine\"\n",
      "\n",
      "\n",
      "Topic: 15 \n",
      "Words: 0.011*\"cnn\" + 0.010*\"feature\" + 0.009*\"music\" + 0.009*\"region\" + 0.008*\"layer\" + 0.006*\"pool\" + 0.006*\"bag\" + 0.006*\"audio\" + 0.005*\"convolution\" + 0.005*\"size\"\n",
      "\n",
      "\n",
      "Topic: 16 \n",
      "Words: 0.022*\"word\" + 0.020*\"model\" + 0.016*\"use\" + 0.011*\"representation\" + 0.010*\"language\" + 0.009*\"translation\" + 0.009*\"learn\" + 0.006*\"table\" + 0.006*\"vector\" + 0.005*\"method\"\n",
      "\n",
      "\n",
      "Topic: 17 \n",
      "Words: 0.015*\"hop\" + 0.011*\"memory\" + 0.006*\"support\" + 0.006*\"yes\" + 0.005*\"task\" + 0.005*\"model\" + 0.004*\"go\" + 0.004*\"answer\" + 0.003*\"story\" + 0.003*\"john\"\n",
      "\n",
      "\n",
      "Topic: 18 \n",
      "Words: 0.000*\"word\" + 0.000*\"use\" + 0.000*\"model\" + 0.000*\"neural\" + 0.000*\"language\" + 0.000*\"train\" + 0.000*\"data\" + 0.000*\"translation\" + 0.000*\"machine\" + 0.000*\"result\"\n",
      "\n",
      "\n",
      "Topic: 19 \n",
      "Words: 0.000*\"word\" + 0.000*\"model\" + 0.000*\"use\" + 0.000*\"neural\" + 0.000*\"network\" + 0.000*\"translation\" + 0.000*\"train\" + 0.000*\"sentence\" + 0.000*\"language\" + 0.000*\"learn\"\n",
      "\n",
      "\n",
      "Topic: 20 \n",
      "Words: 0.000*\"model\" + 0.000*\"use\" + 0.000*\"word\" + 0.000*\"network\" + 0.000*\"neural\" + 0.000*\"data\" + 0.000*\"train\" + 0.000*\"set\" + 0.000*\"test\" + 0.000*\"translation\"\n",
      "\n",
      "\n",
      "Topic: 21 \n",
      "Words: 0.001*\"use\" + 0.001*\"model\" + 0.001*\"learn\" + 0.001*\"word\" + 0.001*\"walk\" + 0.001*\"vertex\" + 0.000*\"network\" + 0.000*\"language\" + 0.000*\"representation\" + 0.000*\"neural\"\n",
      "\n",
      "\n",
      "Topic: 22 \n",
      "Words: 0.002*\"modality\" + 0.001*\"cyclic\" + 0.001*\"sentiment\" + 0.001*\"joint\" + 0.000*\"model\" + 0.000*\"translation\" + 0.000*\"use\" + 0.000*\"bimodal\" + 0.000*\"learn\" + 0.000*\"source\"\n",
      "\n",
      "\n",
      "Topic: 23 \n",
      "Words: 0.000*\"model\" + 0.000*\"use\" + 0.000*\"neural\" + 0.000*\"word\" + 0.000*\"network\" + 0.000*\"train\" + 0.000*\"layer\" + 0.000*\"learn\" + 0.000*\"data\" + 0.000*\"translation\"\n",
      "\n",
      "\n",
      "Topic: 24 \n",
      "Words: 0.008*\"relation\" + 0.005*\"entity\" + 0.004*\"pattern\" + 0.004*\"security\" + 0.003*\"extraction\" + 0.003*\"sw\" + 0.002*\"bootstrapping\" + 0.002*\"text\" + 0.002*\"information\" + 0.002*\"score\"\n",
      "\n",
      "\n",
      "Topic: 25 \n",
      "Words: 0.000*\"model\" + 0.000*\"use\" + 0.000*\"word\" + 0.000*\"neural\" + 0.000*\"language\" + 0.000*\"train\" + 0.000*\"network\" + 0.000*\"layer\" + 0.000*\"translation\" + 0.000*\"km\"\n",
      "\n",
      "\n",
      "Topic: 26 \n",
      "Words: 0.020*\"measure\" + 0.015*\"class\" + 0.011*\"set\" + 0.011*\"true\" + 0.010*\"base\" + 0.010*\"predict\" + 0.010*\"pair\" + 0.008*\"node\" + 0.007*\"figure\" + 0.007*\"hierarchical\"\n",
      "\n",
      "\n",
      "Topic: 27 \n",
      "Words: 0.000*\"model\" + 0.000*\"use\" + 0.000*\"word\" + 0.000*\"network\" + 0.000*\"learn\" + 0.000*\"neural\" + 0.000*\"sequence\" + 0.000*\"train\" + 0.000*\"representation\" + 0.000*\"data\"\n",
      "\n",
      "\n",
      "Topic: 28 \n",
      "Words: 0.000*\"model\" + 0.000*\"use\" + 0.000*\"word\" + 0.000*\"learn\" + 0.000*\"data\" + 0.000*\"network\" + 0.000*\"language\" + 0.000*\"neural\" + 0.000*\"representation\" + 0.000*\"show\"\n",
      "\n",
      "\n",
      "Topic: 29 \n",
      "Words: 0.000*\"use\" + 0.000*\"model\" + 0.000*\"set\" + 0.000*\"word\" + 0.000*\"data\" + 0.000*\"learn\" + 0.000*\"domain\" + 0.000*\"neural\" + 0.000*\"test\" + 0.000*\"system\"\n",
      "\n",
      "\n",
      "Topic: 30 \n",
      "Words: 0.017*\"use\" + 0.013*\"model\" + 0.012*\"test\" + 0.008*\"method\" + 0.008*\"word\" + 0.008*\"document\" + 0.008*\"likelihood\" + 0.007*\"system\" + 0.007*\"data\" + 0.007*\"sequence\"\n",
      "\n",
      "\n",
      "Topic: 31 \n",
      "Words: 0.005*\"paragraph\" + 0.005*\"vector\" + 0.003*\"wikipedia\" + 0.002*\"article\" + 0.002*\"word\" + 0.001*\"near\" + 0.001*\"paper\" + 0.001*\"model\" + 0.001*\"triplet\" + 0.001*\"use\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc45927",
   "metadata": {},
   "source": [
    "# Finding the dominant topic in each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54e97442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>model, word, neural, use, network, translation...</td>\n",
       "      <td>[statistical, machine, translation, draft, cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.9431</td>\n",
       "      <td>word, model, use, representation, language, tr...</td>\n",
       "      <td>[find, learn, robust, joint, representation, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9170</td>\n",
       "      <td>arc, km, flow, hub, route, cost, algorithm, tr...</td>\n",
       "      <td>[available, online, system, engineering, route...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.8117</td>\n",
       "      <td>term, weight, scheme, category, base, classifi...</td>\n",
       "      <td>[xx, xxx, xxx, inverse, category, frequency, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>use, model, test, method, word, document, like...</td>\n",
       "      <td>[university, dun, finding, structure, genome, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.9367</td>\n",
       "      <td>word, model, use, representation, language, tr...</td>\n",
       "      <td>[beyond, stem, ultra, stem, improve, automatic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6615</td>\n",
       "      <td>model, word, neural, use, network, translation...</td>\n",
       "      <td>[detect, phrase, mathematical, text, corpus, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4418</td>\n",
       "      <td>word, use, model, vector, image, sentence, lea...</td>\n",
       "      <td>[fast, accurate, sentiment, classification, us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>measure, class, set, true, base, predict, pair...</td>\n",
       "      <td>[evaluation, measure, hierarchical, view, nove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9918</td>\n",
       "      <td>model, word, neural, use, network, translation...</td>\n",
       "      <td>[generate, sequence, recurrent, neural, networ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0             3.0              0.9999   \n",
       "1            1            16.0              0.9431   \n",
       "2            2             5.0              0.9170   \n",
       "3            3            11.0              0.8117   \n",
       "4            4            30.0              1.0000   \n",
       "5            5            16.0              0.9367   \n",
       "6            6             3.0              0.6615   \n",
       "7            7             1.0              0.4418   \n",
       "8            8            26.0              0.9998   \n",
       "9            9             3.0              0.9918   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  model, word, neural, use, network, translation...   \n",
       "1  word, model, use, representation, language, tr...   \n",
       "2  arc, km, flow, hub, route, cost, algorithm, tr...   \n",
       "3  term, weight, scheme, category, base, classifi...   \n",
       "4  use, model, test, method, word, document, like...   \n",
       "5  word, model, use, representation, language, tr...   \n",
       "6  model, word, neural, use, network, translation...   \n",
       "7  word, use, model, vector, image, sentence, lea...   \n",
       "8  measure, class, set, true, base, predict, pair...   \n",
       "9  model, word, neural, use, network, translation...   \n",
       "\n",
       "                                                Text  \n",
       "0  [statistical, machine, translation, draft, cha...  \n",
       "1  [find, learn, robust, joint, representation, c...  \n",
       "2  [available, online, system, engineering, route...  \n",
       "3  [xx, xxx, xxx, inverse, category, frequency, b...  \n",
       "4  [university, dun, finding, structure, genome, ...  \n",
       "5  [beyond, stem, ultra, stem, improve, automatic...  \n",
       "6  [detect, phrase, mathematical, text, corpus, c...  \n",
       "7  [fast, accurate, sentiment, classification, us...  \n",
       "8  [evaluation, measure, hierarchical, view, nove...  \n",
       "9  [generate, sequence, recurrent, neural, networ...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=tok):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "            # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = ldamodel.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=tok)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f597d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dominant_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1331adde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6266</td>\n",
       "      <td>word, use, model, vector, image, sentence, lea...</td>\n",
       "      <td>[show, neural, image, caption, generator, goog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5927</td>\n",
       "      <td>word, use, model, vector, image, sentence, lea...</td>\n",
       "      <td>[accepted, workshop, contribution, university,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8466</td>\n",
       "      <td>word, use, model, vector, image, sentence, lea...</td>\n",
       "      <td>[neural, network, sentence, kim, new, york, un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8144</td>\n",
       "      <td>word, use, model, vector, image, sentence, lea...</td>\n",
       "      <td>[accepted, workshop, contribution, metric, lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8908</td>\n",
       "      <td>word, use, model, vector, image, sentence, lea...</td>\n",
       "      <td>[distribute, representation, sentence, documen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>word, use, model, vector, image, sentence, lea...</td>\n",
       "      <td>[open, question, answer, weakly, supervise, em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7556</td>\n",
       "      <td>word, use, model, vector, image, sentence, lea...</td>\n",
       "      <td>[text, understanding, scratch, c, c, computer,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6531</td>\n",
       "      <td>word, use, model, vector, image, sentence, lea...</td>\n",
       "      <td>[improve, semantic, representation, tree, stru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8915</td>\n",
       "      <td>word, use, model, vector, image, sentence, lea...</td>\n",
       "      <td>[caption, visual, concept, back, fang, gupta, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>word, use, model, vector, image, sentence, lea...</td>\n",
       "      <td>[fast, accurate, dependency, parser, mohammad,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7481</td>\n",
       "      <td>word, use, model, vector, image, sentence, lea...</td>\n",
       "      <td>[document, embed, paragraph, vector, andrew, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4418</td>\n",
       "      <td>word, use, model, vector, image, sentence, lea...</td>\n",
       "      <td>[fast, accurate, sentiment, classification, us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>word, use, model, vector, image, sentence, lea...</td>\n",
       "      <td>[good, evaluation, word, rouge, jun, ping, new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>model, word, neural, use, network, translation...</td>\n",
       "      <td>[critical, review, recurrent, neural, network,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>model, word, neural, use, network, translation...</td>\n",
       "      <td>[attend, spell, william, chan, carnegie, mello...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9949</td>\n",
       "      <td>model, word, neural, use, network, translation...</td>\n",
       "      <td>[semantically, condition, natural, language, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>model, word, neural, use, network, translation...</td>\n",
       "      <td>[effective, approach, attention, base, neural,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>model, word, neural, use, network, translation...</td>\n",
       "      <td>[accepted, workshop, contribution, earn, emory...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>model, word, neural, use, network, translation...</td>\n",
       "      <td>[end, end, attention, base, large, vocabulary,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6346</td>\n",
       "      <td>model, word, neural, use, network, translation...</td>\n",
       "      <td>[deep, bag, feature, model, music, auto, tag, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>model, word, neural, use, network, translation...</td>\n",
       "      <td>[use, large, target, vocabulary, neural, machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6857</td>\n",
       "      <td>model, word, neural, use, network, translation...</td>\n",
       "      <td>[ask, dynamic, memory, network, natural, langu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>model, word, neural, use, network, translation...</td>\n",
       "      <td>[address, rare, word, problem, neural, machine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>model, word, neural, use, network, translation...</td>\n",
       "      <td>[statistical, machine, translation, draft, cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>model, word, neural, use, network, translation...</td>\n",
       "      <td>[sequence, sequence, learn, neural, network, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>model, word, neural, use, network, translation...</td>\n",
       "      <td>[publish, conference, paper, earn, jacob, univ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>model, word, neural, use, network, translation...</td>\n",
       "      <td>[learn, phrase, representation, use, statistic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.7099</td>\n",
       "      <td>model, word, neural, use, network, translation...</td>\n",
       "      <td>[generalize, language, model, combination, ski...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8559</td>\n",
       "      <td>model, word, neural, use, network, translation...</td>\n",
       "      <td>[one, billion, word, benchmark, measure, progr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6615</td>\n",
       "      <td>model, word, neural, use, network, translation...</td>\n",
       "      <td>[detect, phrase, mathematical, text, corpus, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9918</td>\n",
       "      <td>model, word, neural, use, network, translation...</td>\n",
       "      <td>[generate, sequence, recurrent, neural, networ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9170</td>\n",
       "      <td>arc, km, flow, hub, route, cost, algorithm, tr...</td>\n",
       "      <td>[available, online, system, engineering, route...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.6546</td>\n",
       "      <td>naive, bayes, class, probability, text, featur...</td>\n",
       "      <td>[naive, bayes, text, introduction, theory, seb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.6431</td>\n",
       "      <td>domain, adaptation, target, source, data, lear...</td>\n",
       "      <td>[journal, machine, learn, research, submit, pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.8117</td>\n",
       "      <td>term, weight, scheme, category, base, classifi...</td>\n",
       "      <td>[xx, xxx, xxx, inverse, category, frequency, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.4448</td>\n",
       "      <td>cnn, feature, music, region, layer, pool, bag,...</td>\n",
       "      <td>[effective, use, word, order, text, categoriza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.5714</td>\n",
       "      <td>word, model, use, representation, language, tr...</td>\n",
       "      <td>[kim, character, aware, neural, language, mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.9431</td>\n",
       "      <td>word, model, use, representation, language, tr...</td>\n",
       "      <td>[find, learn, robust, joint, representation, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.8449</td>\n",
       "      <td>word, model, use, representation, language, tr...</td>\n",
       "      <td>[find, function, character, model, open, vocab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.9367</td>\n",
       "      <td>word, model, use, representation, language, tr...</td>\n",
       "      <td>[beyond, stem, ultra, stem, improve, automatic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>word, model, use, representation, language, tr...</td>\n",
       "      <td>[journal, machine, learn, research, submit, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.8612</td>\n",
       "      <td>word, model, use, representation, language, tr...</td>\n",
       "      <td>[exploit, similarity, among, language, machine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.8279</td>\n",
       "      <td>word, model, use, representation, language, tr...</td>\n",
       "      <td>[online, learn, social, representation, bryan,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.9285</td>\n",
       "      <td>word, model, use, representation, language, tr...</td>\n",
       "      <td>[morphology, word, representation, language, j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>word, model, use, representation, language, tr...</td>\n",
       "      <td>[collaborative, deep, learn, system, wang, hon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.6960</td>\n",
       "      <td>word, model, use, representation, language, tr...</td>\n",
       "      <td>[neural, machine, translation, rare, word, uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.5394</td>\n",
       "      <td>hop, memory, support, yes, task, model, go, an...</td>\n",
       "      <td>[end, end, memory, network, dept, computer, sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>relation, entity, pattern, security, extractio...</td>\n",
       "      <td>[towards, relation, extraction, framework, cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>measure, class, set, true, base, predict, pair...</td>\n",
       "      <td>[evaluation, measure, hierarchical, view, nove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>use, model, test, method, word, document, like...</td>\n",
       "      <td>[university, dun, finding, structure, genome, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "24           24             1.0              0.6266   \n",
       "28           28             1.0              0.5927   \n",
       "18           18             1.0              0.8466   \n",
       "29           29             1.0              0.8144   \n",
       "15           15             1.0              0.8908   \n",
       "14           14             1.0              0.9997   \n",
       "31           31             1.0              0.7556   \n",
       "32           32             1.0              0.6531   \n",
       "25           25             1.0              0.8915   \n",
       "33           33             1.0              0.9996   \n",
       "40           40             1.0              0.7481   \n",
       "7             7             1.0              0.4418   \n",
       "47           47             1.0              0.9993   \n",
       "38           38             3.0              0.9992   \n",
       "41           41             3.0              0.9997   \n",
       "42           42             3.0              0.9949   \n",
       "44           44             3.0              0.9996   \n",
       "30           30             3.0              0.9996   \n",
       "45           45             3.0              0.9996   \n",
       "46           46             3.0              0.6346   \n",
       "27           27             3.0              0.9995   \n",
       "39           39             3.0              0.6857   \n",
       "23           23             3.0              0.9996   \n",
       "0             0             3.0              0.9999   \n",
       "21           21             3.0              0.9996   \n",
       "19           19             3.0              0.9997   \n",
       "17           17             3.0              0.9997   \n",
       "13           13             3.0              0.7099   \n",
       "11           11             3.0              0.8559   \n",
       "6             6             3.0              0.6615   \n",
       "9             9             3.0              0.9918   \n",
       "2             2             5.0              0.9170   \n",
       "22           22             9.0              0.6546   \n",
       "37           37            10.0              0.6431   \n",
       "3             3            11.0              0.8117   \n",
       "26           26            15.0              0.4448   \n",
       "48           48            16.0              0.5714   \n",
       "1             1            16.0              0.9431   \n",
       "43           43            16.0              0.8449   \n",
       "5             5            16.0              0.9367   \n",
       "36           36            16.0              0.9986   \n",
       "10           10            16.0              0.8612   \n",
       "12           12            16.0              0.8279   \n",
       "16           16            16.0              0.9285   \n",
       "20           20            16.0              0.9724   \n",
       "49           49            16.0              0.6960   \n",
       "34           34            17.0              0.5394   \n",
       "35           35            24.0              0.6825   \n",
       "8             8            26.0              0.9998   \n",
       "4             4            30.0              1.0000   \n",
       "\n",
       "                                             Keywords  \\\n",
       "24  word, use, model, vector, image, sentence, lea...   \n",
       "28  word, use, model, vector, image, sentence, lea...   \n",
       "18  word, use, model, vector, image, sentence, lea...   \n",
       "29  word, use, model, vector, image, sentence, lea...   \n",
       "15  word, use, model, vector, image, sentence, lea...   \n",
       "14  word, use, model, vector, image, sentence, lea...   \n",
       "31  word, use, model, vector, image, sentence, lea...   \n",
       "32  word, use, model, vector, image, sentence, lea...   \n",
       "25  word, use, model, vector, image, sentence, lea...   \n",
       "33  word, use, model, vector, image, sentence, lea...   \n",
       "40  word, use, model, vector, image, sentence, lea...   \n",
       "7   word, use, model, vector, image, sentence, lea...   \n",
       "47  word, use, model, vector, image, sentence, lea...   \n",
       "38  model, word, neural, use, network, translation...   \n",
       "41  model, word, neural, use, network, translation...   \n",
       "42  model, word, neural, use, network, translation...   \n",
       "44  model, word, neural, use, network, translation...   \n",
       "30  model, word, neural, use, network, translation...   \n",
       "45  model, word, neural, use, network, translation...   \n",
       "46  model, word, neural, use, network, translation...   \n",
       "27  model, word, neural, use, network, translation...   \n",
       "39  model, word, neural, use, network, translation...   \n",
       "23  model, word, neural, use, network, translation...   \n",
       "0   model, word, neural, use, network, translation...   \n",
       "21  model, word, neural, use, network, translation...   \n",
       "19  model, word, neural, use, network, translation...   \n",
       "17  model, word, neural, use, network, translation...   \n",
       "13  model, word, neural, use, network, translation...   \n",
       "11  model, word, neural, use, network, translation...   \n",
       "6   model, word, neural, use, network, translation...   \n",
       "9   model, word, neural, use, network, translation...   \n",
       "2   arc, km, flow, hub, route, cost, algorithm, tr...   \n",
       "22  naive, bayes, class, probability, text, featur...   \n",
       "37  domain, adaptation, target, source, data, lear...   \n",
       "3   term, weight, scheme, category, base, classifi...   \n",
       "26  cnn, feature, music, region, layer, pool, bag,...   \n",
       "48  word, model, use, representation, language, tr...   \n",
       "1   word, model, use, representation, language, tr...   \n",
       "43  word, model, use, representation, language, tr...   \n",
       "5   word, model, use, representation, language, tr...   \n",
       "36  word, model, use, representation, language, tr...   \n",
       "10  word, model, use, representation, language, tr...   \n",
       "12  word, model, use, representation, language, tr...   \n",
       "16  word, model, use, representation, language, tr...   \n",
       "20  word, model, use, representation, language, tr...   \n",
       "49  word, model, use, representation, language, tr...   \n",
       "34  hop, memory, support, yes, task, model, go, an...   \n",
       "35  relation, entity, pattern, security, extractio...   \n",
       "8   measure, class, set, true, base, predict, pair...   \n",
       "4   use, model, test, method, word, document, like...   \n",
       "\n",
       "                                                 Text  \n",
       "24  [show, neural, image, caption, generator, goog...  \n",
       "28  [accepted, workshop, contribution, university,...  \n",
       "18  [neural, network, sentence, kim, new, york, un...  \n",
       "29  [accepted, workshop, contribution, metric, lea...  \n",
       "15  [distribute, representation, sentence, documen...  \n",
       "14  [open, question, answer, weakly, supervise, em...  \n",
       "31  [text, understanding, scratch, c, c, computer,...  \n",
       "32  [improve, semantic, representation, tree, stru...  \n",
       "25  [caption, visual, concept, back, fang, gupta, ...  \n",
       "33  [fast, accurate, dependency, parser, mohammad,...  \n",
       "40  [document, embed, paragraph, vector, andrew, g...  \n",
       "7   [fast, accurate, sentiment, classification, us...  \n",
       "47  [good, evaluation, word, rouge, jun, ping, new...  \n",
       "38  [critical, review, recurrent, neural, network,...  \n",
       "41  [attend, spell, william, chan, carnegie, mello...  \n",
       "42  [semantically, condition, natural, language, g...  \n",
       "44  [effective, approach, attention, base, neural,...  \n",
       "30  [accepted, workshop, contribution, earn, emory...  \n",
       "45  [end, end, attention, base, large, vocabulary,...  \n",
       "46  [deep, bag, feature, model, music, auto, tag, ...  \n",
       "27  [use, large, target, vocabulary, neural, machi...  \n",
       "39  [ask, dynamic, memory, network, natural, langu...  \n",
       "23  [address, rare, word, problem, neural, machine...  \n",
       "0   [statistical, machine, translation, draft, cha...  \n",
       "21  [sequence, sequence, learn, neural, network, g...  \n",
       "19  [publish, conference, paper, earn, jacob, univ...  \n",
       "17  [learn, phrase, representation, use, statistic...  \n",
       "13  [generalize, language, model, combination, ski...  \n",
       "11  [one, billion, word, benchmark, measure, progr...  \n",
       "6   [detect, phrase, mathematical, text, corpus, c...  \n",
       "9   [generate, sequence, recurrent, neural, networ...  \n",
       "2   [available, online, system, engineering, route...  \n",
       "22  [naive, bayes, text, introduction, theory, seb...  \n",
       "37  [journal, machine, learn, research, submit, pu...  \n",
       "3   [xx, xxx, xxx, inverse, category, frequency, b...  \n",
       "26  [effective, use, word, order, text, categoriza...  \n",
       "48  [kim, character, aware, neural, language, mode...  \n",
       "1   [find, learn, robust, joint, representation, c...  \n",
       "43  [find, function, character, model, open, vocab...  \n",
       "5   [beyond, stem, ultra, stem, improve, automatic...  \n",
       "36  [journal, machine, learn, research, submit, re...  \n",
       "10  [exploit, similarity, among, language, machine...  \n",
       "12  [online, learn, social, representation, bryan,...  \n",
       "16  [morphology, word, representation, language, j...  \n",
       "20  [collaborative, deep, learn, system, wang, hon...  \n",
       "49  [neural, machine, translation, rare, word, uni...  \n",
       "34  [end, end, memory, network, dept, computer, sc...  \n",
       "35  [towards, relation, extraction, framework, cor...  \n",
       "8   [evaluation, measure, hierarchical, view, nove...  \n",
       "4   [university, dun, finding, structure, genome, ...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_soreted = df_dominant_topic.sort_values(by='Dominant_Topic', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "df_soreted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af9aeca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = df_soreted[['Dominant_Topic','Text']].values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd2731f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_numbers = [item[0] for item in topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb094f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df_soreted.groupby('Dominant_Topic').agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3ca628e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>[24, 28, 18, 29, 15, 14, 31, 32, 25, 33, 40, 7...</td>\n",
       "      <td>[0.6266000270843506, 0.5927000045776367, 0.846...</td>\n",
       "      <td>[word, use, model, vector, image, sentence, le...</td>\n",
       "      <td>[[show, neural, image, caption, generator, goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>[38, 41, 42, 44, 30, 45, 46, 27, 39, 23, 0, 21...</td>\n",
       "      <td>[0.9991999864578247, 0.9997000098228455, 0.994...</td>\n",
       "      <td>[model, word, neural, use, network, translatio...</td>\n",
       "      <td>[[critical, review, recurrent, neural, network...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.9169999957084656]</td>\n",
       "      <td>[arc, km, flow, hub, route, cost, algorithm, t...</td>\n",
       "      <td>[[available, online, system, engineering, rout...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>[22]</td>\n",
       "      <td>[0.6546000242233276]</td>\n",
       "      <td>[naive, bayes, class, probability, text, featu...</td>\n",
       "      <td>[[naive, bayes, text, introduction, theory, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>[37]</td>\n",
       "      <td>[0.6431000232696533]</td>\n",
       "      <td>[domain, adaptation, target, source, data, lea...</td>\n",
       "      <td>[[journal, machine, learn, research, submit, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Document_No  \\\n",
       "Dominant_Topic                                                      \n",
       "1.0             [24, 28, 18, 29, 15, 14, 31, 32, 25, 33, 40, 7...   \n",
       "3.0             [38, 41, 42, 44, 30, 45, 46, 27, 39, 23, 0, 21...   \n",
       "5.0                                                           [2]   \n",
       "9.0                                                          [22]   \n",
       "10.0                                                         [37]   \n",
       "\n",
       "                                               Topic_Perc_Contrib  \\\n",
       "Dominant_Topic                                                      \n",
       "1.0             [0.6266000270843506, 0.5927000045776367, 0.846...   \n",
       "3.0             [0.9991999864578247, 0.9997000098228455, 0.994...   \n",
       "5.0                                          [0.9169999957084656]   \n",
       "9.0                                          [0.6546000242233276]   \n",
       "10.0                                         [0.6431000232696533]   \n",
       "\n",
       "                                                         Keywords  \\\n",
       "Dominant_Topic                                                      \n",
       "1.0             [word, use, model, vector, image, sentence, le...   \n",
       "3.0             [model, word, neural, use, network, translatio...   \n",
       "5.0             [arc, km, flow, hub, route, cost, algorithm, t...   \n",
       "9.0             [naive, bayes, class, probability, text, featu...   \n",
       "10.0            [domain, adaptation, target, source, data, lea...   \n",
       "\n",
       "                                                             Text  \n",
       "Dominant_Topic                                                     \n",
       "1.0             [[show, neural, image, caption, generator, goo...  \n",
       "3.0             [[critical, review, recurrent, neural, network...  \n",
       "5.0             [[available, online, system, engineering, rout...  \n",
       "9.0             [[naive, bayes, text, introduction, theory, se...  \n",
       "10.0            [[journal, machine, learn, research, submit, p...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6fa15422",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = grouped_df.index\n",
    "grouped_df['Topic_num'] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "23e073e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dominant_Topic\n",
       "1.0      1.0\n",
       "3.0      3.0\n",
       "5.0      5.0\n",
       "9.0      9.0\n",
       "10.0    10.0\n",
       "11.0    11.0\n",
       "15.0    15.0\n",
       "16.0    16.0\n",
       "17.0    17.0\n",
       "24.0    24.0\n",
       "26.0    26.0\n",
       "30.0    30.0\n",
       "Name: Topic_num, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df['Topic_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "25f2618f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_num</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[show, neural, image, caption, generator, goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[[critical, review, recurrent, neural, network...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[[available, online, system, engineering, rout...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>[[naive, bayes, text, introduction, theory, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>[[journal, machine, learn, research, submit, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Topic_num                                               Text\n",
       "Dominant_Topic                                                              \n",
       "1.0                   1.0  [[show, neural, image, caption, generator, goo...\n",
       "3.0                   3.0  [[critical, review, recurrent, neural, network...\n",
       "5.0                   5.0  [[available, online, system, engineering, rout...\n",
       "9.0                   9.0  [[naive, bayes, text, introduction, theory, se...\n",
       "10.0                 10.0  [[journal, machine, learn, research, submit, p..."
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_char = grouped_df[['Topic_num', 'Text']]\n",
    "df_char.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5707890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char.index.names = ['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ae8063e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_num</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[show, neural, image, caption, generator, goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[[critical, review, recurrent, neural, network...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[[available, online, system, engineering, rout...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>[[naive, bayes, text, introduction, theory, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>[[journal, machine, learn, research, submit, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11.0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>[[xx, xxx, xxx, inverse, category, frequency, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15.0</th>\n",
       "      <td>15.0</td>\n",
       "      <td>[[effective, use, word, order, text, categoriz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16.0</th>\n",
       "      <td>16.0</td>\n",
       "      <td>[[kim, character, aware, neural, language, mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17.0</th>\n",
       "      <td>17.0</td>\n",
       "      <td>[[end, end, memory, network, dept, computer, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24.0</th>\n",
       "      <td>24.0</td>\n",
       "      <td>[[towards, relation, extraction, framework, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26.0</th>\n",
       "      <td>26.0</td>\n",
       "      <td>[[evaluation, measure, hierarchical, view, nov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30.0</th>\n",
       "      <td>30.0</td>\n",
       "      <td>[[university, dun, finding, structure, genome,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Topic_num                                               Text\n",
       "index                                                              \n",
       "1.0          1.0  [[show, neural, image, caption, generator, goo...\n",
       "3.0          3.0  [[critical, review, recurrent, neural, network...\n",
       "5.0          5.0  [[available, online, system, engineering, rout...\n",
       "9.0          9.0  [[naive, bayes, text, introduction, theory, se...\n",
       "10.0        10.0  [[journal, machine, learn, research, submit, p...\n",
       "11.0        11.0  [[xx, xxx, xxx, inverse, category, frequency, ...\n",
       "15.0        15.0  [[effective, use, word, order, text, categoriz...\n",
       "16.0        16.0  [[kim, character, aware, neural, language, mod...\n",
       "17.0        17.0  [[end, end, memory, network, dept, computer, s...\n",
       "24.0        24.0  [[towards, relation, extraction, framework, co...\n",
       "26.0        26.0  [[evaluation, measure, hierarchical, view, nov...\n",
       "30.0        30.0  [[university, dun, finding, structure, genome,..."
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "744a4054",
   "metadata": {},
   "outputs": [],
   "source": [
    "art = df_char['Text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e6473fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "art_string = []\n",
    "\n",
    "for a in art:\n",
    "    corpus = ''\n",
    "    for string in a[0]:\n",
    "        corpus += \" \" + string  \n",
    "    a = corpus\n",
    "    art_string.append(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "18d734da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' available online system engineering route optimization algorithm solution web service engineering wuhan university abstract many modern service system rely network hub facility help concentrate flow freight passenger exploit economy scale transportation whereas possible defect bypass cost cause hub seem paper employ novel optimal hub speak network base decision approach unite bypass cost congestion effect algorithm time effect compare set computational work perform personal computer data postal operation draw conclusion approach present paper much good time traditional way keywords route engineering introduction service orient foresees creation business application independently develop similar compete service correspond functional description best set web select run time order maximize quality minimize price end user application workload vary order magnitude even within business optimization perform process execution start iterate run time order take account workload real world price user vision close volume especially extended web service physical flow flow etc paper exploit refine idea first present propose hub arc base framework allow optimal service composition set cost objective hub arc model present relax restriction flow cost every pair hubs hub arc location problem seek locate hub end point solve scale economy bypass cost congestion effect excessively depend scale economy paper present route optimize algorithm solution web service address major deficiency hub arc location model try unite scale economy bypass cost three difference hub arc problem hub arc problem number hub arcs limit determined flow volume hub arc design decision select hub arc flow path exceed total number hub arcs limit explicitly hub arc location problem origin destination path include least one hub mail address publish ltd selection peer review responsibility dash wu open access cc license system engineering direct path connect node permit even nodes hub node hub arc location problem may include three type arc join two reduced unit flow arc join arcs join two without reduce unit flow cost compare hub arc location include incompletely different type arc join two reduced unit flow direct arc hub arc transfer include least one hub arc agglomeration flow can not reach give value therefore belong hub station location also different hub arc location problem present primary decision make determine optimal strictly belong route optimization problem remainder paper organize section provide background formulation describes solution algorithm section include computational result use real section conclude remark mention future work model description section present formulation formulate variety selection decision integer program formulation different property lead different solution approach li introduce approach track flow origin destination flow access arc origin hub four subscript formulation introduce perhaps km flow origin destination via hub order approach easily incorporate multiple origin destination flow track paper use latter approach introduce campbell section present formulation hub arc location design allow efficient solution use mixed integer lp solver formulate track flow arcs complete graph node set node correspond potential hub flow node node distance satisfy triangle inequality distance node node km unit cost flow along let binary km denote arc hub arcs related economy km furthermore km base assumption flow exceed km arc hub arc aggregate coefficient exceed hub arc enjoy discount unit cost present give factor km threshold hub arc km flow volume aggregate arc parameter system engineering select hub km flow aggregate arc select hub arc parameter use discount factor provide reduce unit cost hub arcs reflect economy scale parameter km denote flow volume along route account proportion route aggregate flow may divide three type along route arc transfer arc along route along route model min km km km km ac km km km km km km km km km objective function minimize transport cost arc arc select hub transport cost ac km km cost km km minimize transport cost object function model decide arc select hub arc automatically ensure cost arc flow aggregate arc left side show flow arc include flow arc select hub arc select hub arc right side show aggregate flow three type route node along route flow node along route flow node route km ensure km km arc select hub arc km km possible km impossible km system engineering km km km km possible minimize value object function km km km ensure km capacity restriction arc large constant flow aggregate hub arc less times flow original direct route flow proceed algorithm smart enumeration present formulation general hub arc location problem large number way formulation remainder consider optimal equal collection distribution cost lemma flow along route minimal unit cost route call short transport route spite aggregate flow short transport transfer part flow route whose flow exceed threshold hub arc therefore arc select hub bring whole optimal cost lemma flow flow call direct route lemma former route flow transfer part flow call route load route load flow model use branch bound algorithm algorithm hardly find complete graph node set transport route pares know km km different flow go across arc km range neighborhood large different km comprise complicate arc combination complete graph need consider type km variable hardly give neighborhood give solution key decision variable transport route system engineering km give initial aggregated volume arc let flow km denote aggregated volume arc flow km km know flow unit cost hub speak network make arc km know route accord new transport route km whole km new transport route bring minimal cost call process arc flow optimize algorithm know route km cost reduce basic idea optimal new route load flow transport route without consider affect decision algorithm actually greed algorithm assume arc km threshold hub arc km flow along route km arc whole transport cost flow algorithm table cost optimize algorithm step content initialize solution threshold hub flow along route aggregate flow arc km km otherwise arc km km arc arrange transport route km km route flow optimize evaluate whole transport cost tc km arc combination know transport route flow part give initial new optimal route load flow till whole tc arc arc km tc arc arc km cost descend order update arc arc km system engineering tc tc return step otherwise end algorithm final transport route km finally update tc whole transport cost variable denote transport cost flow route variable denote transport cost flow route km table route flow optimize algorithm step content express step initial load flow let km regard km temp variable aggregated let km flow km reduce flow arc km km variable adopt summarized appendix step load search benefit max benefit record step max step step optimal load flow route step step update transport flow cost step arc km km variable adopt summarized appendix km time optimal load flow denote flow arc exceed threshold load flow lower threshold consequently pair along arc obtain save cost parameter avoid flow benefit arc route accord flow optimize algorithm load system engineering add constraint example arc appear twice route benefit arc twice without constrain flow arc large threshold small threshold reduce unit cost flow across arc increase parameter approach parallel algorithms proven useful discrete optimization branch bound branch also prove useful hub arc approach parallel implementation smart enumerate algorithm facilitate independence cost evaluation use set hub iteration step experimental analysis section present computational result two different optimal solution first use commercial solver lingo solve formulation describe second approach smart enumerate algorithm describe problem solve use se algorithm cod personal computer ghz intel intel processor operate window professional gb memory experiment flat use australia data set include distance pare use data ap set nodes computational test test parameter affect whole cost computation time assume unit cost non hub arc discount scale parameter table provide result solve formulation use lingo table test use lingo base ap data direct transport cost object value time system engineering cost greedy algorithm result gap time time cost difference lingo greedy algorithm enumeration hub arc consider following ordering potential quality solution algorithm get optimal solution dont consider combination flow route even route optimal load route load flow step base one route without consider route solution heuristic algorithm optimal solution lingo limit percent solution satisfy graph find time cost lingo much greedy algorithm optimization increase time cost lingo greedy algorithm increase greatly result table show clearly problem become much harder increase condition ensure accuracy well use greedy algorithm advantage time cost conclusion system engineering paper provide model result new hub spoke arc acclaim hub arc flow aggregate arc exceed fix value get discount flow divide direct transport one hub two hub route optimize problem build analyze character optimal solution base character solution design greedy enumerate algorithm last find unite bypass cost scale economy hub speak furthermore network economic paper part ongoing work concern evaluation framework business engineering study analysis multi broker broker compete instead cooperate use web service engineering extend web service engineering paper support fundamental research fund central university national natural science foundation china grant appendix otherwise arc arc otherwise arc arc otherwise arc arc otherwise arc arc km km km km km otherwise arc km arc otherwise arc arc otherwise arc arc otherwise arc arc otherwise arc arc system engineering km appendix km km km km otherwise arc km arc flow flow flow km flow km flow flow arc arc ac ac flow otherwise flow otherwise arc arc arc arc ac ac flow otherwise flow otherwise arc arc arc ac flow otherwise arc reference energy server resource host symposium operate system principle soft contract transaction base web service transaction service composition strategy consideration equilibrium international symposium computer science society optimization layer computational heuristic algorithm multiple allocation hub median exact solution approach base short path hub median programming formulation discrete hub location'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art_string[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7998916e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9333/423988610.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_char['corpus'] = art_string\n"
     ]
    }
   ],
   "source": [
    "df_char['corpus'] = art_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a6f6f116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_num</th>\n",
       "      <th>Text</th>\n",
       "      <th>test</th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[show, neural, image, caption, generator, goo...</td>\n",
       "      <td>[['show', 'neural', 'image', 'caption', 'gener...</td>\n",
       "      <td>show neural image caption generator google al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[[critical, review, recurrent, neural, network...</td>\n",
       "      <td>[['critical', 'review', 'recurrent', 'neural',...</td>\n",
       "      <td>critical review recurrent neural network sequ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[[available, online, system, engineering, rout...</td>\n",
       "      <td>[['available', 'online', 'system', 'engineerin...</td>\n",
       "      <td>available online system engineering route opt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>[[naive, bayes, text, introduction, theory, se...</td>\n",
       "      <td>[['naive', 'bayes', 'text', 'introduction', 't...</td>\n",
       "      <td>naive bayes text introduction theory sebastia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>[[journal, machine, learn, research, submit, p...</td>\n",
       "      <td>[['journal', 'machine', 'learn', 'research', '...</td>\n",
       "      <td>journal machine learn research submit publish...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Topic_num                                               Text  \\\n",
       "index                                                                 \n",
       "1.0          1.0  [[show, neural, image, caption, generator, goo...   \n",
       "3.0          3.0  [[critical, review, recurrent, neural, network...   \n",
       "5.0          5.0  [[available, online, system, engineering, rout...   \n",
       "9.0          9.0  [[naive, bayes, text, introduction, theory, se...   \n",
       "10.0        10.0  [[journal, machine, learn, research, submit, p...   \n",
       "\n",
       "                                                    test  \\\n",
       "index                                                      \n",
       "1.0    [['show', 'neural', 'image', 'caption', 'gener...   \n",
       "3.0    [['critical', 'review', 'recurrent', 'neural',...   \n",
       "5.0    [['available', 'online', 'system', 'engineerin...   \n",
       "9.0    [['naive', 'bayes', 'text', 'introduction', 't...   \n",
       "10.0   [['journal', 'machine', 'learn', 'research', '...   \n",
       "\n",
       "                                                  corpus  \n",
       "index                                                     \n",
       "1.0     show neural image caption generator google al...  \n",
       "3.0     critical review recurrent neural network sequ...  \n",
       "5.0     available online system engineering route opt...  \n",
       "9.0     naive bayes text introduction theory sebastia...  \n",
       "10.0    journal machine learn research submit publish...  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_char.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "852c729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_char[['Topic_num', 'corpus']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "6581222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Number 1.0\n",
      "[' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Char to int {' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "Int to char {0: ' ', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
      "The number of total characters are 19427\n",
      "\n",
      "The character vocab size is 27\n",
      "Total Patterns:  19327\n",
      "The X input 19327\n",
      "The Y output [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "The Y output [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "input_features = []\n",
    "output = []\n",
    "total_n_v = 0\n",
    "\n",
    "for article in data[0:1]:\n",
    "    raw_text = article[1]\n",
    "    print(\"Topic Number\", article[0])\n",
    "    chars = sorted(list(set(raw_text)))\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "    print(chars)\n",
    "    print(\"Char to int\" , char_to_int)\n",
    "    print(\"Int to char\", int_to_char)\n",
    "    \n",
    "    # Prints the total characters and character vocab size\n",
    "    n_chars = len(raw_text)\n",
    "    n_vocab = len(chars)\n",
    "    \n",
    "    total_n_v += n_vocab\n",
    "\n",
    "    print(\"The number of total characters are\", n_chars)\n",
    "    print(\"\\nThe character vocab size is\", n_vocab)\n",
    "    \n",
    "    #Prepares dataset where the input is sequence of 100 characters and target is next character.\n",
    "    seq_length = 100\n",
    "\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "\n",
    "    for i in range(0, n_chars - seq_length, 1):\n",
    "          seq_in = raw_text[i:i + seq_length]\n",
    "          seq_out = raw_text[i + seq_length]\n",
    "          dataX.append([char_to_int[char] for char in seq_in])\n",
    "          dataY.append(char_to_int[seq_out])\n",
    "\n",
    "    n_patterns = len(dataX)\n",
    "    print (\"Total Patterns: \", n_patterns)\n",
    "    \n",
    "    from keras.utils import np_utils\n",
    "    # reshapes X to be [samples, time steps, features]\n",
    "    X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "    print(\"The X input\", len(X))\n",
    "    input_features.append(X)\n",
    "    # one hot encodes the output variable\n",
    "    Y = np_utils.to_categorical(dataY)\n",
    "    print(\"The Y output\", Y[1])\n",
    "    print(\"The Y output\", Y[0])\n",
    "    output.append(Y)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9dc7f2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19327, 100, 1)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "848ca7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_Y = output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2dd8cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim =100\n",
    "max_length =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "712c7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ab959fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 100)          2700      \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 100, 256)          365568    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 100, 256)          0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 27)                6939      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 900,519\n",
      "Trainable params: 900,519\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(n_vocab, embedding_dim, input_length=max_length))\n",
    "model1.add(LSTM(256, input_shape=(X.shape[1], embedding_dim),return_sequences=True))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(LSTM(256))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(y.shape[1], activation='softmax'))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam',  metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "49f6c5e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "151/151 [==============================] - 119s 774ms/step - loss: 2.8626 - accuracy: 0.1643\n",
      "Epoch 2/20\n",
      "151/151 [==============================] - 113s 750ms/step - loss: 2.5495 - accuracy: 0.2519\n",
      "Epoch 3/20\n",
      "151/151 [==============================] - 112s 740ms/step - loss: 2.1726 - accuracy: 0.3551\n",
      "Epoch 4/20\n",
      "151/151 [==============================] - 109s 720ms/step - loss: 1.9058 - accuracy: 0.4327\n",
      "Epoch 5/20\n",
      "151/151 [==============================] - 107s 708ms/step - loss: 1.6758 - accuracy: 0.5042\n",
      "Epoch 6/20\n",
      "151/151 [==============================] - 107s 709ms/step - loss: 1.4970 - accuracy: 0.5518\n",
      "Epoch 7/20\n",
      "151/151 [==============================] - 106s 702ms/step - loss: 1.3668 - accuracy: 0.5910\n",
      "Epoch 8/20\n",
      "151/151 [==============================] - 104s 692ms/step - loss: 1.2632 - accuracy: 0.6165\n",
      "Epoch 9/20\n",
      "151/151 [==============================] - 106s 699ms/step - loss: 1.1708 - accuracy: 0.6428\n",
      "Epoch 10/20\n",
      "151/151 [==============================] - 107s 710ms/step - loss: 1.0856 - accuracy: 0.6667\n",
      "Epoch 11/20\n",
      "151/151 [==============================] - 107s 708ms/step - loss: 1.0268 - accuracy: 0.6847\n",
      "Epoch 12/20\n",
      "151/151 [==============================] - 108s 713ms/step - loss: 0.9585 - accuracy: 0.7022\n",
      "Epoch 13/20\n",
      "151/151 [==============================] - 107s 711ms/step - loss: 0.8992 - accuracy: 0.7220\n",
      "Epoch 14/20\n",
      "151/151 [==============================] - 106s 705ms/step - loss: 0.8380 - accuracy: 0.7405\n",
      "Epoch 15/20\n",
      "151/151 [==============================] - 107s 710ms/step - loss: 0.7744 - accuracy: 0.7612\n",
      "Epoch 16/20\n",
      "151/151 [==============================] - 107s 708ms/step - loss: 0.7191 - accuracy: 0.7817\n",
      "Epoch 17/20\n",
      "151/151 [==============================] - 108s 712ms/step - loss: 0.6650 - accuracy: 0.7975\n",
      "Epoch 18/20\n",
      "151/151 [==============================] - 107s 711ms/step - loss: 0.6132 - accuracy: 0.8154\n",
      "Epoch 19/20\n",
      "151/151 [==============================] - 107s 706ms/step - loss: 0.5649 - accuracy: 0.8251\n",
      "Epoch 20/20\n",
      "151/151 [==============================] - 107s 711ms/step - loss: 0.5227 - accuracy: 0.8421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f96bcb1d8b0>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(input_features, real_Y, epochs = 20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "338bcc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the sequence similar to above methods. Gets the generated string using the model.\n",
    "def predict_next_n_chars(pattern, n):\n",
    "    for i in range(n):\n",
    "      x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "      prediction = model1.predict(x, verbose=0)\n",
    "      print (int_to_char[np.argmax(prediction)], end = '')   #get next char index.\n",
    "      seq_in = [int_to_char[value] for value in pattern]\n",
    "      pattern.append(np.argmax(prediction))\n",
    "      pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "48a2dfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed -\n",
      "\n",
      "es image adequately main inspiration work come recent ad machine task transform sentence write sourc\n",
      "\n",
      "Generated string -\n",
      "\n",
      "e target maximize likelihood sentence give image description give image see approach image annotation use neural network encodes variable length input decoder generates complete sentence generate desc"
     ]
    }
   ],
   "source": [
    "#picks a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "input_str = ''.join([int_to_char[value] for value in pattern])\n",
    "print (\"Seed -\",  input_str, sep = '\\n\\n')\n",
    "print (\"\\nGenerated string -\\n\")\n",
    "\n",
    "predict_next_n_chars(pattern, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b6721430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed -\n",
      "\n",
      "many modern service systems rely on a network of hub facilities to help concentrate flows of freight or passengers\n",
      "\n",
      "Generated string -\n",
      "\n",
      " memory block contains cell control layer apply thus noise convert train maximize likelihood sentence give image description give image see approach image annotation use neural network encodes variabl"
     ]
    }
   ],
   "source": [
    "input_str = \"Many modern service systems rely on a network of hub facilities to help concentrate flows of freight or passengers\"\n",
    "\n",
    "#Uses the first 100 characters from given input_str as input to generate next 200 characters. \n",
    "input_str = input_str.lower()\n",
    "input_string = ''\n",
    "for each in input_str:\n",
    "    if each in chars:\n",
    "           if (len (input_string) < 100):\n",
    "                input_string += each\n",
    "\n",
    "pattern = []\n",
    "pattern.append([char_to_int[char] for char in input_string])\n",
    "\n",
    "print (\"Seed -\",  input_str, sep = '\\n\\n')\n",
    "print (\"\\nGenerated string -\\n\")\n",
    "predict_next_n_chars(pattern[0], 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9777ba8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed -\n",
      "\n",
      "recently most of the research based on the neural networks models, because they are fast and accurate models \n",
      "\n",
      "Generated string -\n",
      "\n",
      " deep convolution neural network encodes variable length input decoder generates complete sentence generate description give image see approach image annotation use neural network encodes variable len"
     ]
    }
   ],
   "source": [
    "input_str = \"recently most of the research based on the Neural networks models, because they are fast and accurate models \"\n",
    "\n",
    "#Uses the first 100 characters from given input_str as input to generate next 200 characters. \n",
    "input_str = input_str.lower()\n",
    "input_string = ''\n",
    "for each in input_str:\n",
    "    if each in chars:\n",
    "           if (len (input_string) < 100):\n",
    "                input_string += each\n",
    "\n",
    "pattern = []\n",
    "pattern.append([char_to_int[char] for char in input_string])\n",
    "\n",
    "print (\"Seed -\",  input_str, sep = '\\n\\n')\n",
    "print (\"\\nGenerated string -\\n\")\n",
    "predict_next_n_chars(pattern[0], 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6909bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
