{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "339eafae",
   "metadata": {
    "id": "339eafae"
   },
   "source": [
    "# Final Project NLP Smart Scientific Content Auto-completion System\n",
    "\n",
    "\n",
    "a text auto-complete system that supports scientists in writing articles in:\n",
    "- completing some typed words/letters.\n",
    "- Identifying words/phrases that are very unlikely to occur in some text contexts and suggesting proper replacements.\n",
    "\n",
    "\n",
    "Karmel Salah & Fatimah Najwan\n",
    "15/4/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FZVlVeATK31U",
   "metadata": {
    "id": "FZVlVeATK31U"
   },
   "source": [
    "# We will use Colab for a Free GPU so if you are using Colab frist connect to your drive to get the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ntkBblK8IURV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntkBblK8IURV",
    "outputId": "582dff94-4d99-4f0d-e2c5-f80a6b29def6"
   },
   "outputs": [],
   "source": [
    "# # Load the Drive helper and mount\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "J-f70JR2IvsP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J-f70JR2IvsP",
    "outputId": "82d9f6ff-762d-46da-b5c7-bda6367d0397"
   },
   "outputs": [],
   "source": [
    "# #unzip the dataset folder\n",
    "# !unzip ./drive/MyDrive/dataset.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c979c",
   "metadata": {
    "id": "198c979c"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac58d753",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac58d753",
    "outputId": "4bb33356-7b9e-4275-c74a-f036df561a9c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/karmel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/karmel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/karmel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/karmel/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2022-05-24 08:16:23.518207: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-24 08:16:23.518235: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ec03206",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ec03206",
    "outputId": "7206b479-5978-4372-b5f4-063c48e03e52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1064"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the names of each folders\n",
    "files_names = os.listdir('./dataset')\n",
    "len(files_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0f3999",
   "metadata": {
    "id": "cc0f3999"
   },
   "source": [
    "# Convert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24dd0950",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24dd0950",
    "outputId": "777058cd-b335-4367-864a-a83d343da852"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data frame is Empty DataFrame\n",
      "Columns: [id, articles]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame([], columns = [\"id\", \"articles\"])\n",
    "print(f\"data frame is {df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "706513d6",
   "metadata": {
    "id": "706513d6"
   },
   "outputs": [],
   "source": [
    "def convert_to_df(df, files_names):\n",
    "        id = []\n",
    "        articles= []\n",
    "        # Define files place and read each sub file for cleaning\n",
    "        for index, file in enumerate(files_names):\n",
    "                # read each article files\n",
    "                path = os.getcwd()\n",
    "                file_path = '{}/dataset/{}'.format(path,file)\n",
    "                with open(file_path,'r', encoding='utf-16',  errors=\"ignore\") as f:\n",
    "                    texts = f.read() \n",
    "                    articles.append(texts) \n",
    "                    id.append(index)\n",
    "                    f.close()\n",
    "        print(\"the ids\", id)\n",
    "        print(\"the articles\", articles)\n",
    "        df['id'] = id\n",
    "        df[\"articles\"] = articles\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd7c238f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd7c238f",
    "outputId": "b10c00b9-9a40-430e-8495-c6684d5be17d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = convert_to_df(df, files_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "174e36e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "174e36e3",
    "outputId": "f423a30c-89ef-4b57-d8f6-a4ed35418313"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Statistical Machine Translation\\n\\nDraft of Ch...\n",
       "1       Found in Translation:\\n\\nLearning Robust Joint...\n",
       "2       Available online at www.sciencedirect.com\\n\\nS...\n",
       "3       J OURNAL OF I NFORMATION S CIENCE AND E NGINEE...\n",
       "4       The University of Sheﬃeld\\n\\nT. E. Dunning\\n\\n...\n",
       "                              ...                        \n",
       "1059    Multi-label Hate Speech and Abusive Language D...\n",
       "1060    hULMonA (      ): The Universal Language Model...\n",
       "1061    Mazajak: An Online Arabic Sentiment Analyser\\n...\n",
       "1062    2017 2nd International Conferences on Informat...\n",
       "1063    INVESTIGATIVE RADIOLOGY Volume 28, Number 6, 4...\n",
       "Name: articles, Length: 1064, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"articles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f19114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only 50 articles for our experiment\n",
    "limited_df = pd.DataFrame(df['articles'].iloc[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d16683f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d16683f2",
    "outputId": "ca54dc4e-938a-4f42-c68f-a27e6f52b0aa"
   },
   "outputs": [],
   "source": [
    "# count the number of words for each doc\n",
    "limited_df['word_count'] = limited_df['articles'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66466052",
   "metadata": {
    "id": "66466052"
   },
   "source": [
    " # Text pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f22a259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, spacy, logging, warnings\n",
    "from gensim.utils import simple_preprocess\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8352e695",
   "metadata": {
    "id": "8352e695"
   },
   "outputs": [],
   "source": [
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    # Remove leading and trailing spaces\n",
    "    text= text.strip()  \n",
    "    text= re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text= re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    # get ride of one letter word!\n",
    "    text = re.sub(r'\\b\\w\\b','',text) \n",
    "    \n",
    "      \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "HqeYuDXZNB-h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HqeYuDXZNB-h",
    "outputId": "18645fd2-10c3-455f-9ab7-c18de2692ae8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/karmel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42fe2349",
   "metadata": {
    "id": "42fe2349"
   },
   "outputs": [],
   "source": [
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe6a93b8",
   "metadata": {
    "id": "fe6a93b8"
   },
   "outputs": [],
   "source": [
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e99623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd034880",
   "metadata": {
    "id": "fd034880"
   },
   "outputs": [],
   "source": [
    "#Remove non English words\n",
    "dictionary = enchant.Dict(\"en_US\")\n",
    "   \n",
    "def remove_non_english(text):\n",
    "        en_words= []\n",
    "        for word in text:\n",
    "            if dictionary.check(word):\n",
    "                en_words.append(word)\n",
    "        text = ' '.join(str(e) for e in en_words)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f837434",
   "metadata": {
    "id": "2f837434"
   },
   "outputs": [],
   "source": [
    "# remove non English words\n",
    "limited_df['english_text'] =limited_df['articles'].apply(lambda text: remove_non_english(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf3b5414",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "bf3b5414",
    "outputId": "acc7d6a8-1596-4cbe-f333-958985071431"
   },
   "outputs": [],
   "source": [
    "#limited_df['english_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d58900d1",
   "metadata": {
    "id": "d58900d1"
   },
   "outputs": [],
   "source": [
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51Xh5mpTM4nW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "51Xh5mpTM4nW",
    "outputId": "4a2b49e6-2d35-4100-f27d-b4d18323f5e0"
   },
   "outputs": [],
   "source": [
    "# clean each article\n",
    "limited_df['clean_text'] = limited_df['english_text'].apply(lambda x: finalpreprocess(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "607c9f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = limited_df.clean_text.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f20c21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['statistical machine translation draft chapter neural machine translation center speech language processing department computer science john hopkins university st public draft august nd public draft september content neural machine translation short history introduction neural network linear model multiple layer non linearity inference back propagation training computation graph neural network computation graph gradient computation deep learn framework neural language model fee forward neural language model word embed inference training recurrent neural language model long short term memory model gate recurrent unit deep model neural translation model encoder decoder approach add alignment model train beam search content ensemble decode large vocabulary use monolingual data deep model guide alignment training model coverage adaptation add linguistic annotation multiple language pair alternate architecture neural network neural network attention self attention current challenge domain mismatch amount training data noisy data word alignment beam search read additional topic bibliography author index index chapter neural machine translation major recent development statistical machine translation adoption neural net work neural network model promise well share statistical evidence similar word inclusion rich context chapter introduces several neural network model technique explains apply problem machine translation short history already last wave neural network research machine trans sight researcher explore method model propose strike similar current dominant neural machine translation approach none model train data size large enough produce reasonable result anything toy ex computational complexity involve far exceeded computational resource hence idea abandon almost two decade hibernation data driven approach phrase base statistical chine translation rise obscurity dominance make machine translation useful tool many information increase productivity professional translator modern resurrection neural method machine translation start neural language model traditional statistical machine translation system work showed large improvement public evaluation campaign ideas slowly mainly due computational concern use training also pose challenge many research group simply lack hardware experience exploit move beyond use language neural network method creep com traditional statistical machine provide additional score ex tend translation table lu reorder li chapter neural machine translation model joint translation language model since show large quality improvement top competitive statistical machine translation system ambitious effort aim pure neural machine abandon exist approach completely early step use model sequence sequence model able produce reasonable translation short fell apart crease sentence length addition attention mechanism yield competitive result jean byte pair encode back translation target side monolingual neural machine translation become new state art within year entire research machine translation go neural give indication speed share task machine translation organize conference machine translation one pure neural machine translation system submit outperformed traditional statistical system year neural machine translation system almost language pair almost submission neural machine translation system time neural machine translation research progress rapid pace many direction explore come range core machine learn improvement deep model linguistically inform model insight strength weaknesses neural machine translation gather inform future work extensive proliferation available neural machine translation system time number rather consolidate quite hard premature make recommendation promise marian implementation sockeye introduction neural network neural network machine learning technique take number input predict outputs many different machine learn method distinct strengths introduction neural network figure graphical illustration linear model feature value input arrow score output node linear model linear model core element statistical machine translation potential translation sentence represent set feature feature weighted parameter obtain overall score ignore exponential function use previously turn linear model log linear follow formula sum model score linear model illustrated feature value input arrow score output node figure use linear model combine different component machine translation language phrase translation reorder property length accumulate jump distance phrase translation train method assign weight value feature relate importance contribute score good translation higher machine call tune linear model allow u complex relationship feature let u say short sentence language model less important translation average phrase translation probability higher similarly reasonable value really terrible hypothetical example implies dependence feature second example implies non linear relationship feature value impact score linear model can not handle case commonly cite counter example use linear model operator truth table linear model two feature possible come weight give correct output case linear model assume represent point feature linearly separable case may case type feature use machine translation chapter neural machine translation figure neural network hide layer multiple layer neural network modify linear model two important way use multiple layer instead compute output value directly input hide layer introduce call observe input output train mechanism connects use concept hidden similar meaning hidden markov model see figure illustration network process two step linear combination weight input node compute produce hidden node value linear combination weight hidden node compute produce output node value let u introduce mathematical notation neural network literature neural network hide layer consist vector input node value vector hidden node value vector output node value matrix weight connect input node hide node matrix weight connect hidden node output node computation neural network hidden sketch note possibility multiple output node although far show one introduction neural network hyperbolic tangent logistic function linear unit cosh output range output range output range figure typical activation function neural network non linearity carefully think addition hide realize gain anything far model relationship easily away hidden layer multiply weight salient element neural network use non linear activation function compute linear combination weight feature value obtain value node apply function popular choice hyperbolic tangent logistic function see figure detail function good way think activation function segment range value linear combination segment node turn close transition segment node partly turn segment node turn close different popular choice activation function linear unit allow negative value alter value positive value simpler fast compute could view hidden node feature detector certain input node turn others turn advocate neural network claim chapter neural machine translation figure simple neural network bias node input hidden layer use hidden node obviates least drastically need feature engineer instead manually detect useful pattern input train hidden node discovers automatically stop single hidden layer currently fashionable name deep learn neural network stem fact often well performance achieve deeply stack together layer layer hide node inference let u walk neural network output value compute input concrete example consider neural network figure network one additional innovation present bias unit node always value bias unit give network something work case input value weight sum would matter weight let u use neural network process say value input node second input node value bias input node compute value hide node carry follow calculation calculation node summarize table output value node input expect binary would understand result value since threshold range possible output value output possible binary introduction neural network layer node summation activation hide hidden output table calculation input network figure input input hide hidden output neural network computes xor look hidden node notice act like boolean value high least two input value three otherwise low value hide node act like boolean high value input xor effectively implement subtraction hidden node note non linearity key since value node much high input oppose single input v distinct high value node case manage push output threshold would possible value input would simply sum linear model mention recently use name deep learn neural network become fashionable emphasizes often high performance achieve use network multiple hidden layer xor example hint power come single input output layer network possible mimic basic boolean operation since model linear xor express neural network example implement boolean operation subtraction second layer function require intricate operation may chain hence neural network architecture hidden layer may need may possible train build neural network computer number hidden layer match depth computation line research banner neural turing machine explores kind architecture need implement basic neural network two hidden layer implement algorithm sort bit number back propagation training train neural network require optimization weight value network correct output set training example repeatedly fee input chapter neural machine translation optimum gradient comb current point figure gradient descent compute gradient regard every dimension case gradient respect weight small gradient respect weight move leave arrows point negative gradient point training example compare compute output network correct output train update weight several pass train data carry pas data call epoch common training method neural network call back propagation since update weight output propagate back error information earlier layer whenever train example node error term compute basis update value incoming weight formula use compute updated value weight follow principle gradient descent training error node understood function incoming weight reduce error give compute gradient error function respect move gradient reduce error move alongside gradient good consider optimize multiple dimension time look low point area look water ground fall steep west also slightly south would go direction mainly west slightly south go alongside gradient see figure illustration follow two derive formula update weight example network less interested skip section continue read summarize update formulae page weight output node let u review extend notation output node compute linear combination weight hidden node value introduction neural network sum pass activation function compute output value compare compute output value target output value train example various way compute error value value let u use norm state goal compute gradient error respect weight direction move weight value weight separately break computation gradient three essentially unfolding equation let u work three step since error term output value compute component follow derivative output value respect linear combination weight hidden node depend activation function case keep treatment general possible commit activation use shorthand note give train example give differentiable activation value always compute compute derivative respect weight turn quite simply value hidden node equation compute three step need compute gradient error function give unfolded laid equation put chapter neural machine translation factor learn rate give u follow update formula weight note also remove minus since move gradient towards minimum useful introduce concept error term note term associate weight update concern weight error term compute use incoming weight reduces update formula weight hide node computation gradient hence update formula hide node quite anal linear combination input value hide value weight weight weight lead computation value hide node follow principle gradient need compute derivative error respect weight decompose derivative dz error term output value value hide node idea behind back propagation track error cause hidden node contribute error next layer apply chain rule give dz computation complex case output since introduction neural network already encounter two term previously third term equation compute straightforward put equation equation equation solve give rise quite intuitive interpretation error matter hide node depend error term subsequent node weight impact hidden node output node let u tie remain loose end miss piece equation second term dz dz third term dz put equation equation equation together give u gradient dz dz error term hidden node analogous output node analogous update formula chapter neural machine translation summary train neural network process train one update weight time drive weight updates gradient towards small error weight update compute base error term associate non input node network output error term compute actual output node current target output node hidden error term compute via back propagate error term subsequent node connect weight compute require derivative activation weight sum incoming value pass give error weight proceed node temper learn rate weight next training example process typically pass training call epochs example give neural network figure let u see train example pro let u start calculation error term output node inference table page compute linear combination weight hidden node value node value target value compute weight weight since hidden node lead one output node calculation error term computationally complex table summarizes update weight introduction neural network node error term weight update table weight update learn rate neural network figure training example present local optimum global optimum high learn rate bad initialization local optimum figure problem gradient descent training motivate detail section high learn rate may lead drastic parameter overshoot bad initialization may require many update escape existence local optimum trap train chapter neural machine translation error validation minimum validation train train progress figure train progress time error training set continuously decrease validation set use point error increase train stop validation minimum set conclude introduction neural network basic motivate consider figure gradient descent training may run practical problem set learn rate high lead update overshoot optimum con low learn rate lead slow convergence bad initialization weight may lead long path many update step reach optimum especially problem activation function like short interval change existence local optima lead search get trap miss global optimum validation set neural network train proceeds several full iteration train data track training see error training set continuously decrease point set train data memorize generalized check additional set call validation set use train see figure illustration measure error validation set point see point error increase stop minimum validation set reach introduction neural network weight initialization training weight initialize random value value uniform distribution prefer initial weight lead node value transition area activation low high shallow slope would take long time push towards change activation feeding value range activation function lead activation value range activation commonly use formula weight layer network size previous layer hidden choose weight range size previous size next layer momentum term consider case weight value far optimum even train exam push weight value may still take small update accumulate weight reach optimum common trick use momentum term speed train momentum term get updated time step train combine previous value momentum term current raw weight update value use result momentum term value update weight decay rate update formula change adapt learn rate per parameter common training strategy reduce learn rate time begin parameter far away optimal value change later train stage concern large learn rate may cause parameter bounce around optimum different parameter may different stage path optimal different learn rate parameter may helpful one call record gradient compute parameter accumulates square value us sum adjust learn rate chapter neural machine translation update formula base sum gradient error respect weight time step divide learn rate weight accumulate sum big change parameter value big gradient lead reduction learn rate weight parameter combine idea momentum term adjust parameter update change inspiration adam another method transform raw gradient parameter update idea compute equation idea square gradient adjust learn rate since raw accumulation run risk become large hence permanently depress learn adam us exponential like momentum term hyper parameter set typically close also mean early training value close initialization value adjust correct bias increase training time step correction go piece hand rate momentum accumulate change weight update per adam compute common value hyper parameter various adaptation scheme active area research second order gradient give useful information rate change often expensive shortcut take introduction neural network dropout parameter space back propagation learning variant operate lit local optimum hill climb algorithm may climb mole hill stick instead move towards climb high mountain various method propose get train local optimum one currently popular method neural machine translation call drop sound bit simplistic wacky node neural network ignore value set associated parameter update drop node choose may account much even nod training resume number iteration without different set drop node select drop node play useful role model train point ignore node pick slack end result robust model several node share similar role layer normalization layer normalization address problem arises especially deep neural network use neural machine compute proceeds large sequence layer train average value one layer may become feed follow also produce large output especially problem activation function limit output narrow linear unit train examples average value layer may small cause problem train recall equation gradient update strongly effect node value large node value lead explode gradient small node value lead diminish gradient remedy idea normalize value per layer basis do add additional computational step neural network recall feed forward layer consist matrix multiplication weight matrix node value previous layer result weight sum follow activation function compute mean variance value weight sum vector chapter neural machine translation use normalize vector use two additional bias vector element wise multiplication difference subtracts scalar average vector element formula normalize value shift average hence ensure average afterwards result vector divide variance additional bias vector give may share across multiple layer multiple time step recurrent neural network introduce section page mini batch train example yield set weight update may process train example afterwards apply updates neural network advantage immediately learn train example training method update model train example call online learn online learn variant gradient descent training call stochastic gradient descent online learn generally take few pass train set epochs con since train constantly change hard may want process train data accumulate weight apply collectively small set train example call mini batch distinguish approach batch training entire training set consider one batch variation organize process training typically motivate restriction parallel process process training data mini computation weight update value synchronize summation application weight want distribute training number computationally convenient break training data equally size perform online learning part use small mini average weight break training often lead good result straightforward linear processing scheme call run several train thread immediately update even though thread still use weight value compute gradient clearly violate safe guard typically take parallel hurt practical experience vector matrix operation express calculation need handle neural network vector matrix operation computation graph forward activation error propagation error weight execute operation computationally expensive layer matrix operation require multiplication matrix also common another highly use area computer graphic process render image geometric property dimensional object process generate color value dimensional image screen since high demand fast graphic instance use realistic look computer specialize hardware become graphic process unit processor massive number core nvidia gpu provide thread rather lightweight instruction set provide instruction apply many data point exactly need vector space computation list program support various become essential part develop large scale neural network application general term matrix tensor tensor may also sequence matrix pack dimensional tensor large object actually frequently use today neural network reading good introduction modern neural network research textbook also book neural network method apply natural language process general number key technique recently develop entered standard neural machine translation research training make robust method drop training interval number node randomly mask avoid explode vanish gradient back propagation several gradient clip layer normalization ba similar ensure node value within reasonable bound active topic research optimization method adjust learn rate gradient descent training popular method currently adam computation graph example neural network section painstakingly work gradient computation need gradient descent train hard may chapter neural machine translation prod sum prod sum figure two layer fee forward neural network computation consist input value weight parameter computation node right parameter value show leave input computation show input process graph come surprise likely never do even arbitrarily complex neural network architecture number allow network take care rest take close look work neural network computation graph take different look network build previously represent neural network graph consist node connection figure page mathematical equation equation describe fee forward neural network use run example represent math form computation graph see figure illustration computation graph network graph contain nodes parameter model weight matrix bias vector input mathematical operation carry next show value computation graphs neural view computation arbitrary connect operation input number parameter operation may little inspiration neuron stretch term neural network quite bite graph nice tree structure may direct graph anything go long straightforward process direction cycle another way view graph fancy way visualize sequence function call take argument previously compute combination recursion loop process input neural network require place input value node carry computation show input vector result number look familiar since previously work example section move let u take stock computation node graph accomplish consist function execute computation operation link input node process computed value add two item node follow section gradient computation show computation graph use process input value examine use vastly simply model train model training require error function computation gradient derive update rule parameter quite straightforward compute need add another computation end computation graph computation take compute put value give correct output value train data produce error value typical error function norm view result execution computation graph error value part update rule parameter look computation model update originate error value propagate back model parameter call computation need compute date value also backward pas oppose forward pas compute output error calculus refresher chain rule formula compute derivative two function chain rule ex press derivative composition function map term derivative product function write explicitly term variable let chapter neural machine translation consider chain operation con weight matrix error com sum prod value hide layer result early computation compute update rule pa matrix view error function parameter take derivative respect case break step use chain rule sum prod sum prod note purpose compute update rule treat computation target value bias vector hidden node value constant break derivative error respect parameter chain derivative along line node computation graph gradient computation come derivative node computation graph example sum prod want compute gradient update parameter compute value backward start error term see figure illustration give detail computation gradient backward start bottom recall compute computation graph prod sum prod sum figure computation graph gradient compute backward pas training example gradient compute respect input node two input also two gradient see text detail computation value chapter neural machine translation use formula give target output value give train data compute forward pas gradient norm note use value compute forward pas gradient computation low use formula recall formula plug value compute forward pass formula give u chain rule require u multiply value compute give u low sum simply copy previous since sum note two gradient associate sum node one respect output prod one parameter derivative value gradient case low prod use formula prod deal scalar value encounter vector value hide node chain rule require u multiply previously compute scalar sum two input hence two gradient gradient respect output upper node prod similarly compute computation graph gradient read relevant value weight date gradient associate trainable parameter weight second gradient prod node new value time step prod remain computation carry similar since form simply another layer feed forward neural network example include one special output computation may use multiple time subsequent step computation graph multiple output node fee back gradient back propagation pas add step factor add impact let u take second look node computation graph function compute value link input node obtain argument process example forward compute value function execute gradient computation link child node obtain downstream gradient process example forward compute gradient object orient program node computation graph provide forward backward function value gradient computation instantiate computation connect input also aware dimension variable value gradient forward backward variable deep learn framework next encounter various network architecture need vector matrix well computation obtain weight update formula would quite tedious write almost identical code deal variant number framework emerge sup port develop neural network method choose problem time prominent one python library generates compiles code build torch machine learn library script language base program python variant chapter neural machine translation implementation natural language processing researcher use library recent entry genre framework less geared towards ready use neural network provide implementation vector space operation computation seamless support example section implement line python show use example framework quite execute follow command python command line interface instal pip install import import import map input layer hide layer use weight matrix bias vector map function consist linear combination activation function note matrix allow u process several training example sequence good way think term functional programming language symbolically operation actually function method function use example call compute value hide node number table page map hidden layer output layer fashion callable function test full network predict neural language model model train require cost function use formulate need variable correct output overall cost compute average training examples cost gradient descent training require computation derivative cost function respect model parameter value weight matrix bias vector great use computes derivative follow also example function multiple input multiple output need train function update model parameter return current prediction cost us learn rate train let u train data train function return prediction cost update call train function prediction cost change well would loop train function convergence discuss may also break train data mini batch train one mini batch time neural language model neural network powerful method model conditional probability distribution multiple input robust unseen data point unobserved training data use traditional statistical estimation may address sparse data problem back require insight prob part condition context drop arbitrary choice many chapter neural machine translation word word word word word figure sketch neural language predict word base precede word gram language model reduce probability sentence product word probability context previous word model prime example conditional probability distribution rich conditioning context often lack data point would like cluster information statistical language complex discounting back scheme use balance rich evidence low order model model sparse estimate high order model turn neural network help fee forward neural language model figure give basic sketch gram neural network language model network node represent context word connection hide connects put layer predict word represent word immediately face rep resent node neural network carry real numbered word discrete item large vocabulary can not simply use token since neural network assume token similar token practice completely arbitrary argument applies idea use bit encode token id word similar may nothing idea use bit vector occasionally appear consider next represent word high dimensional one dimension per word value dimension match rest type vector call one hot vector dog cat eat neural language model word word word word word figure full architecture feed forward neural network language model context word represent one hot project continuous space word weight matrix predict word compute one hot vector via hide layer large continue wrestle impact choice represent word one stopgap limit vocabulary pool word token could also use word class automatic cluster linguistically motivate class part speech reduce vector revisit problem large vocabulary later pool evidence introduce another layer input layer hidden layer context word individually project low space use weight matrix context thus generate continuous space representation independent position condition context representation commonly refer word embedding word occur similar context similar word training data language model frequently contain gram cute dog jump cute cat jump child hug cat tightly child hugged dog tightly like watch cat video like watch dog videos language model would knowledge dog cat occur similar context hence somewhat interchangeable like predict context dog occur see context word cat would still like treat positive evidence word enable generalizing word hence robust prediction unseen contexts chapter neural machine translation neural network architecture see figure visualization architecture fully fee forward neural network language consist context word one hot vector input word embed hidden layer predict output word layer context word encode one hot vector pass embed matrix result vector point word embed embed vector typically order node note use embed matrix context word also note mathematically much go since input multiplication matrix one hot input value matrix multiplication zero select one column matrix correspond input word id use activation function embed matrix lookup table word indexed word id map hidden layer model require concatenation context word em input typical feed forward use activation function output layer interpret probability distribution word linear combination weight hidden node value compute node ensure indeed proper probability use activation function ensure value add one describe close neural probabilistic language model propose model one add direct connection context word output word equation replace paper report direct connection context word output word speed although ultimately improve performance en counter idea short cut hidden layer bite late discus deep model hide layer also call residual connection skip connection even highway connection neural language model train train parameter neural language model embed weight bias process gram train corpus feed context word network match network output one hot vector correct word predict weight update use back propagation go detail next language model commonly evaluate related probability give proper english text language model like proper english good language model train objective language model increase likelihood train data give context correct value hot vector train example likelihood log note one value others really come probability give correct word likelihood way allow u update also one lead wrong output word word embed move worth role word neural machine trans many natural language processing task introduce compact encode word relatively high dimensional say point natural language time word acquire reputation almost magical quality consider role play neural language language describe represent context word enable prediction next word sequence recall part earlier cute dog jump cute cat jump since dog cat occur similar predicting word jump similar different word dress unlikely trigger completion jump idea word occur similar context semantically similar powerful idea lexical semantics point researcher love cite john rupert shall know word company keep ludwig wittgenstein put bit chapter neural machine translation figure word project semantically similar word occur close meaning word use meaning semantics quite concept largely unresolved idea distributional lexical semantics word mean distributional prop context occur word occur similar context dog cat similar representation vector space word use similarity measure distance cosine distance angle vector project high dimensional word two visualize word show figure word similar festival cluster together stop would like semantic representation carry semantic inference queen king queen queen indeed evidence word embed allow good stop note word crucial tool neural machine translation neural language model inference train train neural language model computationally expensive billion word even use train take several day modern compute cluster even use neural language model score component statistical machine translation decode require lot computation could restrict use rank best list consider method inference training cache inference actually possible use neural language model within decoder word actually need carry map ping one hot vector word store beforehand computation hide layer also partly carry note word occur one slot condition context gram language matrix multiplication word embed vector correspond weight run sum hidden layer apply activation function compute value output node insanely since many output node vocabulary item interested score give word produce translation model compute node score use last point require long discussion compute node value word want score language miss important step obtain proper need normalize require computation value node could simply ignore problem use score face value likely word give context get high score less likely main objective since place constraint may work model contexts give high score many contexts give preference would node value layer already normalized probability method enforce train let u discuss train move method section noise estimation discuss earlier problem compute tie neural language model expensive due need normalize output node value use function require compute value output even interested score particular gram overcome need chapter neural machine translation explicit normalization would like train model already value normalized one way include constraint normalization factor close objective function instead simple likelihood may include norm log factor note log log log another way train self normalize model call noise estimation main idea optimize model separate correct training example create noise example method need less computation since require computation output node value try learn model distribution give noise case language model model good choice generate set noise example addition correct train example set size probability give example predict correct train example correct objective noise estimation maximize correct correct train example minimize noise example use log objective function log correct log correct return original goal self normalize note noise normalize model distribution encourage produce value would generally overshoot would also give high value noise example generally undershoot would give low value correct translation example train since need compute output node value give train noise example need compute since function give train objective complete computation graph implement use standard deep learning do via gradient descent training may immediately obvious optimize towards classify correct noise examples give rise model also predict correct probability grams variant method common statistical machine translation tune phase mira infused relaxation pro rank follow principle compute gradient parameter use parameter update neural language model word word word word copy value copy value word word figure recurrent neural language predict word context follow word use hidden layer correct word predict word hide layer prediction use prediction word recurrent neural language model fee forward neural language model describe able use longer con texts traditional statistical back since mean deal unknown context use word make use similar robust handling unseen word context position possible condition much large contexts traditional statistical model large gram report use instead use context word recurrent neural network may condition context sequence length trick use hidden layer predict word additional input predict word see figure illustration model look different feed forward neural language model discuss far inputs network word sentence second set neuron point indicate start sentence word embed start sentence neuron map hidden layer use predict output word model use architecture word represent one hot word hide layer real value neuron use activation function hide layer function output layer thing get interesting move predict third word sequence one input directly precede word neuron network use represent start sentence value hide layer previous prediction word neuron encode chapter neural machine translation word word word word word word figure back propagation unfold recurrent neural network number prediction step derive update formula base train objective predicting output word back propagation error via gradient descent previous sentence context enrich step information new input word hence condition full history sentence even last word sentence condition part word sentence model less weights gram feed forward neural language model train model arbitrarily long one initial stage second word architecture hence training procedure fee forward neural network assess error output layer propagate update back input layer could process every training example way essentially treat hide layer previous train example input current example never provide feedback representation prior history hidden layer back propagation time train procedure figure unfold current neural network number go back word note despite limit unfolding time network still able learn dependency longer distance back propagation time either applied training example call time computationally quite expensive time computation carry several step compute apply weight update mini batch section process large number training example entire update weight give modern compute fully unfolding recurrent neural network become common recurrent neural network theory arbitrary give train size actually know fully construct com graph give train error sum word prediction carry back propagation entire sentence require quickly build computation graph call dynamic computation graph currently support good others neural language model long short term memory model consider follow step word prediction sequential language much economic progress country directly precede word country informative prediction word previous word much less relevant importance word decay distance hidden state recurrent neural network always update recent memory older word likely diminish time distant word much follow example country make much economic progress year still verb depends subject country separate long subordinate clause recurrent neural network allow model arbitrarily long sequences architecture simple simplicity cause number problem hide layer play double duty memory network continuous space representation use predict output word may sometimes want pay attention directly previous sometimes pay attention longer clear mechanism con train model long update need back propagate beginning sentence propagate many step raise concern impact recent information step drown old information rather confusingly name long short term memory neural network address issue design quite although use practice core distinction basic building block call cell contain explicit memory state memory state cell motivate digital memory cell ordinary computer digital memory cell offer operation reset digital memory cell may store single cell store real number operation cell regulate real call gate figure note correspond explode gradient long distance gradient value become large typically suppress clip limit maximum value set hyper parameter chapter neural machine translation layer time precede layer forget gate next layer layer time figure cell neural network recurrent neural receives input layer hide layer value previous time step memory state update input state previous time value memory state various gate channel information cell towards output value input gate parameter regulates much new input change memory state forget gate parameter regulate much prior memory state retain output gate parameter regulate strongly memory state pass next layer mark output value time step information within cell follow memory gate input input gate forget memory output gate output memory hidden node value pass next layer application activation function output value output layer consist vector traditional layer consist vector node input layer compute way input recurrent neural network node give node value prior layer value hide layer previous time step input value typical combination matrix multiplication weight activation function input neural language model gate parameter actually play fairly important role par would like give preference recent input input rather past memory forget pay less attention cell current point time output decision inform broad view context compute value complex condition treat like node neural network gate input forget output matrix ha compute gate parameter value multiplication weight node value previous layer hide layer previous time memory state previous time step memory follow activation function gate ha memory train way recurrent neural use back propagation time fully unrolling network operation within cell complex recurrent neural operation still base matrix differentiable activation function compute gradient objective function respect parameter model compute update function gate recurrent unit cell add large number additional parameter gate multiple weight matrix add parameter lead long train time risk simpler gate recurrent unit propose used neural translation model time cell seem make comeback neural machine still commonly used see figure illustration cell separate memory hidden state serf purpose two gate gate predict input previous state update update input update state bias update reset reset input reset state bias reset gate use combination input previous state combination identical traditional recurrent neural except previous state impact scale reset gate since gate value may give preference current input combination input reset state update gate use interpolation previous state compute combination do weighted update gate balance two chapter neural machine translation layer time reset gate precede layer next layer layer time figure gate recurrent unit long short term memory cell state update state update combination bias one extreme update gate previous state pass directly another extreme update gate new state mainly determine much impact previous state reset gate allows may seem bit redundant two operation gate combine prior state input play different role operation yield combination classic recurrent neural network component allow complex computation combination input output second operation yield new hidden state output unit allow bypass enable long distant memory simply pass information back pass thus enable long distance dependency deep model currently fashionable name deep learning late wave neural network research real motivation large gain see task vision speech recognition due stack multiple hidden layer together layer allow complex sequence traditional computation component allow complex computation multiplication number generally recognize long modern hardware enable train deep neural network real world problem neural language model input hide layer output input hidden layer hide layer hide layer output input hidden layer hide layer hide layer output shallow deep stack deep transition figure deep recurrent neural network input pass hidden layer output prediction make deep stacked hidden layer also connect layer value time step depend value time step well previous layer time step deep transitional layer time step sequentially connect hidden layer also inform last layer time step learn experiment vision speech even dozen layer give increasingly good quality idea deep neural network apply sequence prediction task common several option figure give two example shallow neural input pass single hidden output predict sequence hidden layer use hidden layer may deeply stacked layer act like hidden layer shallow recurrent neural network state condition value previous time step value previous layer sequence layer prediction last layer hide layer may directly connect deep transitional hidden layer inform last hidden layer previous time step hidden layer connect value previous time step layer prediction last layer chapter neural machine translation function may layer multiplication plus activation cell cell experiment use neural language model traditional statistical machine show hidden layer modern hardware allow train deep stretch computational resource practical limit computation neural convergence training typically slow add skip connection input directly output hidden sometimes speed still talk several time longer train time shallow network reading vanguard neural network research tackle language model prominent reference neural language model implement gram model feed forward neural network history word input predict word output introduce language model machine translation call space language use similar earlier work speech recognition propose number speed ups make implementation avail able open source toolkit also support train graphical processing unit cluster word class encode word pair class word class reduce computational complexity allow integration neural network language model decoder another way reduce computational complexity enable decoder integration use noise estimation roughly self normalizes output score model hence remove need compute value possible output word compare two technique class base word encode normalized score vs estimation without normalized score show letter give good performance much high speed another way allow straightforward decoder wang convert space language model short list word traditional gram language model format wang present method merge continuous space language model traditional gram language take advantage well estimate word short list full coverage traditional model finch use recurrent neural network language model best list system compare fee forward long short term neural network language variant recurrent neural network language show good performance latter speech recognition rank task report improvement best list machine translation system recurrent neural network language model neural language model deep learning model sense use lot hidden layer show hidden layer improve typical layer language model neural machine traditional statistical machine translation model straightforward mechanism integrate additional knowledge large domain language model hard end end neural machine translation add language model train additional monolingual data form recurrently neural translation model house big give word embed hidden state predict word house big figure sequence sequence encoder decoder extend language con english input sentence house big german output sentence dark green box process end sentence token contain embed entire input sentence neural network run parallel compare use language model rank deep integration gate unit regulates relative contribution language model translation model predict word neural translation model prepared look actual translation model already do since commonly use architecture neural machine translation straightforward extension neural language model one alignment model encoder decoder approach stab neural translation model straightforward extension language model recall idea recurrent neural network model language sequential process give previous model predicts next word reach end proceed predict translation one word time see figure illustration train simply concatenate input output sentence use method train language model feed input go prediction model predict end sentence token network processing reach end input sentence predict end sentence marker hidden state encodes mean vector hold value node hide layer input sentence embed encoder phase model hidden state use produce translation decoder phase chapter neural machine translation ask lot hidden state recurrent neural network encoder need incorporate information input sentence can not forget word towards end sentence decoder need enough information predict next also need account part input sentence already still need cover proposed model work reasonable well short sentence fail long sentence minor model use sentence embed state input hidden state decoder phase model make decoder structurally different encoder reduces load hidden state since need remember anymore input another idea reverse order output last word input sentence close last word output sentence follow embark improvement explicitly alignment output word input word add alignment model time state art neural machine translation sequence sequence encoder decoder model attention essentially model describe previous explicitly alignment mechanism deep learning alignment call attention use word alignment attention interchangeable since attention mechanism add bit complexity slowly build take look attention mechanism encoder task encoder provide representation input sentence input sentence sequence consult embed matrix basic language model describe process word recurrent neural network result hide state encode word leave precede word also get right also build recurrent neural network run right end sentence begin figure illustrate model two recurrent neural network run two direction call bidirectional recurrent neural network encoder consist embed lookup input word map step hidden state neural translation model input word leave right recurrent right leave recurrent figure neural machine translation part input encoder consist two recurrent neural run right leave leave right recurrent neural encoder state combination two hidden state recurrent neural network equation use generic function cell recurrent neural net work function may typical fee forward neural network layer ax complex gate recurrent unit long short term memory cell original paper propose approach use lately become popular note could train model add step predicts next word actually train context full machine translation model limit description output sequence word representation concatenate two hidden state decoder decoder also recurrent neural network take representation input con text next section attention previous hidden state output word generate new hidden decoder state new output word prediction see figure illustration start recurrent neural network maintains sequence hidden state compute previous hidden state embed previous output word input context still several choice function combine input generate next hidden linear transforms activation etc choice match encoder use also use decoder hidden state predict output word prediction take form probability distribution entire output vocabulary vocabulary prediction dimensional element correspond probability predict one word vocabulary chapter neural machine translation context state prediction word select word embed figure neural machine translation part output decoder give context input embed previously select new decoder state word compute prediction vector condition decoder hidden state embed previous output word input context cc note repeat conditioning since use hidden state separate encoder state progression prediction output word use convert raw vector probability sum value high value vector indicate output word token word embed informs next time step recurrent neural network correct output word training proceeds word training objective give much probability mass possible correct output word cost function drive train hence negative log probability give correct word translation cost log want give correct word probability would mean negative log probability typically low hence high cost note cost function tie individual overall sentence cost sum word cost inference new test typically chose word high value use embed next step also explore beam search next likely word select create different condition context next word later neural translation model encoder state attention input context hidden state output word figure neural machine translation part attention model association compute last hidden state decoder word representation use compute weight sum encoder state attention mechanism currently two loose end decoder give u sequence word representation decoder expect context step describe attention mechanism tie end together attention hard visualize use typical neural network figure give least idea input output relation attention mechanism inform input word representation previous hidden state decoder produce context state motivation want compute association decoder state contains information output sentence input word base strong association word relevant particular input word produce next output want weight impact word compute association layer weight vector bias value output computation scalar indicate important input word produce output word normalize attention attention value across input word add use exp exp use normalized attention value weigh contribution input word representation context vector do chapter neural machine translation simply add word representation vector may seem odd simplistic thing common practice deep learn natural language processing researcher qualms use sentence simply sum word scheme train complete model take close look train one challenge number step decoder number step encoder varies train example sentence pair consist sentence different can not computation graph train example instead dynamically create computation graph technique call unrolling recurrent neural already discuss regard language model section fully unrolled computation graph short sentence pair show figure note couple thing error compute one sentence pair sum error compute word proceed next word use correct word condition context decoder hidden state word prediction train objective base probability mass give correct give perfect context attempt use different training yet show superior practical training neural machine translation model require well suit high degree parallelism inherent deep learning model think many matrix increase parallelism even process several sentence pair implies increase state tensor give example represent input word sentence pair vector since already sequence input line matrix process batch sentence line matrix dimensional tensor give another decoder hidden state vector output word since process batch line hide state matrix note case helpful line state output since state compute sequentially recall computation attention mechanism pas computation gpu matrix encoder state dimensional tensor input result matrix attention value sentence one dimension input due massive use value well inherent parallelism show true power may feel create glare contradiction argue process one training example since sentence pair typically different neural translation model house big input word leave right recurrent right leave recurrent attention input context hidden state output word prediction error give output word output word embed figure fully unrolled computation graph train example input token house big output token cost function compute output word sum across sentence walk correct previous output word use condition context chapter neural machine translation figure make good use parallelism process batch training example time convert batch training example set mini batch similar length waste less computation word hence computation graph different size argue sentence pair together well exploit parallelism indeed goal see figure batch training example consider maxi mum size input output sentence batch unroll computation graph maximum sizes shorter remain gap non word keep track valid data mask ensure attention give word beyond length input error gradient update compute output word beyond length output sentence avoid waste computation nice trick sort sentence pair batch length break mini batch similar length training consist follow step train corpus avoid undue bias due temporal topical break corpus maxi batch break maxi batch mini batch process gather gradient apply gradient maxi batch update parameter train neural machine translation model take epochs entire training common stop criterion check progress model val set part training halt error validation set improve training longer would lead improvement may even degrade performance due beam search translate neural translation model proceed one step time predict one output word compute probability distribution word bite confusion technical term entire training corpus call batch use contrast batch update online update small batch subset call mini batch section page use term batch maxi batch mini batch subset subset neural translation model context cat state prediction word fish select word dog embed figure elementary decoding model predict word prediction probability distribution select likely word embed part condition context next word prediction decoder pick likely word move next prediction step since model condition previous output word equation use word embed condition context next step see figure illustration time obtain probability distribution word distribution often quite word maybe even one word amass almost probability word receive high pick output word real example neural machine translation model translate german sentence english show figure model tend give almost mass top sentence translation also indicate word choice believe v think different v various also ambiguity grammatical sentence start discourse connective subject process suggest perform best greedy search make u vulnerable call garden path problem sometimes follow sequence word realize late make mistake early best sequence consist less probable word initially redeem subsequent word context full output consider case produce idiomatic phrase word phrase may really odd word choice piece cake easy full phrase choice redeem note face problem traditional statistical machine translation model arguable even since rely sparse context make next word decode algorithm model keep list best candidate hypothesis expand keep best expanded hypothesis neural translation model chapter neural machine translation input sentence er clever um seine art output word prediction best alternative however yet also also think believe believe think feel clever smart enough around keep maintain hold make statement statement testimony message comment vague ambiguous enough may could might interpret get interpret differently different various several way way way manner figure word predictions neural machine translation model mass give top semantically related word may rank believe v think unit explain section page neural translation model cat fish dog cat cat cat dog cat figure beam search neural machine translation commit short list output word beam new word prediction make differ since commit output word part condition context make prediction predict word output keep beam top likely word choice score probability use word beam condition context next word due make different word prediction multiply score partial translation point probability probability word prediction select high score word pair next beam see figure illustration process continue time accumulate word translation give u score hypothesis sentence translation end sentence token produce remove complete hypothesis beam reduce beam size search hypotheses leave beam search produce graph show figure start start sentence symbol path terminate end sentence symbol give compete result translation obtain follow back pointer complete hypothesis one end high score point best translation choose among best score product word prediction probability get good result normalize score output length divide number word carry normalization search complete translation beam normalization would make difference note traditional statistical machine able combine share condition context future feature function possible anymore recurrent neural network since condition entire output word sequence begin search graph generally less diverse search chapter neural machine translation figure search graph beam search decode neural translation model time best partial translation select output sentence complete end sentence token predict reduce beam terminate full sentence translation complete follow back pointer end sentence token allow u read empty box represent hypothesis part complete path graph statistical machine translation model really search tree number complete path size beam reading attention model root sequence sequence model use recurrent neural network approach use short term network reverse order source sentence decode seminal work add alignment model call link generate output word source include condition hidden state produce precede target word source word represent two hidden state recurrent neural network process source sentence leave right right leave propose variant attention mechanism call attention also hard constraint attention model attention restrict gaussian distribution around input word explicitly model trade source context input target context already produce target tu introduce interpolation weight scale impact source context state previous hidden state last word predict next hidden state decoder tu augment attention model reconstruction step generate output translate back input language training objective extend include likelihood target sentence also likelihood reconstruct input sentence previous section give comprehensive description currently commonly use basic neural translation model architecture perform fairly well box many refinement checkpoint ensemble multi run ensemble figure two method generate alternative system checkpoint use model dump various stage train multi run start dent training run different initial weight order training data language pair since number propose describe section fairly target particular use case data give one best performing system recent evaluation campaign use ensemble decode byte pair encode address large add synthetic data derive monolingual target side data use deep model ensemble decode common technique machine learn build one system multiple one combine call ensemble system successful strategy various method propose systematically build instance use different feature different subset data neural one straightforward way use different stop different point train process intuitive argument system make different mistake two system likely rather make mistake one also see general principle play human set committee make decision democratic voting election apply ensemble method case neural machine address two generate alternate combining output generate alternative system see figure illustration two method generate alter native system train neural translation iterate training data stop criterion meet typically lack improvement cost function apply validation set translation performance validation set dump model interval every iteration batch training look back performance chapter neural machine translation model model model model model average cat fish dog figure combine prediction ensemble model independently predict probability distribution output average combined distribution model different stage pick model best performance translation quality measure call checkpoint since select model different checkpoint train process multi run require build system completely different training run mention accomplished use different random initialization lead train seek different local optimum also randomly train use different random order also lead different training outcome multi run usually work good deal also computationally much expensive note multi run also build checkpoint instead combine end point apply checkpoint combine ensemble combine system output neural translation model allow combination several system fairly deeply recall model predict probability distribution possible output com one word combine different train model model predict probability distribution combine prediction combination do simple average distribution average distribution basis select output word see figure illustration may weigh different system although way generate similar typically do refinement right leave decode one tweak idea instead build multiple system different random also build one set system second set system reverse order output sentence second set system call right leave although arguably good name since make sense language arabic hebrew normal write order right leave deep integration describe work anymore combination leave right right leave since produce output different order resort involve several use ensemble leave right system generate best list candidate input sentence score candidate translation individual leave right right leave combine score different model select candidate best score input sentence score give candidate translation right leave system require require forced decode special mode run inference input predict give output sentence mode actually much closer training also output translation regular inference large vocabulary law tell u word language unevenly distribution always large tail rare word new word come language time wake also deal large inventory include company names microsoft neural method well equip deal large ideal representation neural network continuous space vector convert discrete object word word ultimately discrete nature word show input need train embed matrix map word embed output side predict probability distribution output word latter generally big since amount computation involve linear size make large matrix operation neural translation model typically restrict vocabulary word initial work neural machine frequent word others represent unknown tag translation rare word handle back dictionary chapter neural machine translation obama receive relationship obama exactly friendly two want talk implementation international agreement activity middle east meeting also plan cover palestinian dispute two state solution relation obama year washington continuous building settlement israel use lack initiative peace process relationship two deteriorate deal obama negotiate iran atomic march invitation an make controversial speech u congress partly see obama speech agree obama reject meeting reference election time pending israel figure byte pair encode applied english use word split indicate note data also true case common approach today break rare word unit may seem bit crude actually similar standard approach statistical machine translation handle compound website web site morphology follow convolution convolution even decent approach problem transliteration name traditionally handle sub modular letter translation component popular method create inventory unit legitimate word byte pair encode method train parallel corpus word corpus split character original space special space frequent pair character merge may step repeat give number time step increase vocabulary beyond original inventory single character example mirror quite well behavior algorithm real world data set start group together frequent letter combination join frequent word end frequent word emerge single rare word consist still see figure unit indicate two symbol byte pair encode vast majority word rarer word break split seem motivate pending mostly note also decomposition relatively rare name reading limitation neural machine translation model burden support large vocabulary avoid typically vocabulary reduce refinement shortlist remain token replace unknown word ken translate unknown jean resort separate dictionary arthur argue neural translation model bad rare word interpolate traditional probabilistic bilingual dictionary prediction neural machine translation model use attention mechanism link target word distribution source word weigh word translation accordingly source word name number may also directly copy target use call switching network predict either traditional translation operation copying operation aid layer source sentence training data change target word word position copy source word augment word prediction step neural translation model either translate word copy source word observe attention mechanism mostly drive semantics language model case word location case copy speed mi use traditional statistical machine translation word phrase translation model target vocabulary mini batch split word sub word use character gram model segmentation base byte pair encode compression algorithm use monolingual data key feature statistical machine translation system language train large monolingual data set large language high translation quality language model train trillion word crawl general web use surprise basic neural translation model use additional monolingual language model aspect condition previous hidden decoder state previous train jointly translation model aspect input two main idea propose improve neural translation model data one transform additional monolingual translation parallel data syn miss half integrate language model component neural network architecture back translation language model improve output use large amount monolingual data target language give machine evidence common sequence word can not use monolingual target side data neural translation model since miss source side one idea synthesize data back translation see figure illustration step involve train reverse system translates intend target language source language typically use neural machine translation setup source target may use even traditional phrase base system chapter neural machine translation reverse system system figure create synthetic parallel data target side monolingual train system reverse use translate target side monolingual data source combine generate synthetic parallel data true parallel data system building use reverse system translate target side monolingual create synthetic parallel corpus combine generate synthetic parallel data true parallel data building system open question much synthetic parallel data use relation amount exist true parallel data magnitude monolingual data also want drown actual real data successful idea use equal amount synthetic true data may also generate much synthetic parallel ensure train process equal amount sample true parallel data add language model idea train language model separate component neural translation model train large language model recurrent neural net work available include target side parallel corpus add language model neural translation model since language model translation model predict output natural point connect two model join output prediction node network concatenate condition contexts expand equation add hidden state neural language model hidden state neural translation model tm source context previous english word tm training combine leave parameter large neural language model update parameter translation model layer concern otherwise output side parallel corpus would overwrite refinement mt mt figure round trip addition train two model do traditionally parallel also optimize model convert sentence restore back use monolingual data may add correspond round trip start memory large monolingual corpus language model would parallel train data less general one question much weight give translation model much weight give language equation considers instance way may output word translation model relevant translation content word distinct output word language model relevant introduction relevant function word balance translation model language model achieve type gate unit encounter discussion long short term memory neural network architecture gate unit may predict solely model state use factor multiply language model state use prediction equation gate gate tm round trip training look idea strict machine learn see two learn objective objective learn transformation give parallel do traditionally goal learn convert output sentence input back output language objective match traditional sentence good machine translation model able preserve meaning output language sentence map input language back see figure illustration two machine translation model one translate sentence language direction opposite direction two system may train traditional use parallel corpus also round trip sentence back two objective model train chapter neural machine translation translation give monolingual sentence valid sentence language measure language model reconstruction translation back original language measure translation model mt two objective use update model parameter translation model mt mt typical model update drive correct prediction word round trip translation compute usual training model mt give sentence pair make good use training best list translation compute model update compute also update model mt monolingual data language scaling date language model cost forward translation cost mt translation best list use monolingual data language training do reverse round trip direction detail refer reading back translate monolingual data input use obtain synthetic parallel corpus additional training data use monolingual data dual learning setup machine translation engine train addition regular model training parallel monolingual data translate round trip evaluate language model language reconstruction match back cost function drive gradient descent update model deep model learn lesson research vision speech recent work machine translation also look deep model simply involve add intermediate layer baseline architecture core component neural machine translation encoder take input word convert sequence contextualized representation decoder gen output sequence word recurrent neural network recall already discuss build deep recurrent neural network back section page extend idea recurrent neural network encoder decoder recurrent neural network common process input sequence output time step information new input combine hidden state previous time step predict new hidden state hidden state additional prediction may make word case next word sequence case language hidden state use otherwise attention mechanism case refinement context decoder stack transition decoder stack transition decoder stack transition decoder stack transition figure deep instead single recurrent neural network layer decoder deep consist several layer illustration show combination deep transition stack omits word word selection output word embed step identical original show figure page decoder see figure part decoder neural machine use par deep architecture see instead single hidden state give time step sequence hidden state give time step various option hide state may connect sec present two idea stack recurrent neural network hide state condition hidden state previous layer hidden state depth previous time step deep transition recurrent neural net hidden state condition last hidden state previous time step hidden layer condition previous previous layer figure combine two idea layer stack previous time step previous layer others deep transition previous layer break stack layer deep transition layer either chapter neural machine translation input word embed encoder layer encoder layer encoder layer encoder layer figure deep alternating combination idea bidirectional recurrent neural network previously propose neural machine translation figure page stack recurrent neural network figure page architecture may extend idea deep show decoder figure function compute sequence function call function may implement feed forward neural network layer multiplication plus activation long short term memory cell gate recurrent unit either function set trainable model parameter encoder deep recurrent neural network encoder may draw idea one baseline neural translation use bidirectional recurrent neural network condition leave right context want deep version encoder figure show one idea could call alternate recurrent neural network look basically like stack recurrent neural one hidden state layer alternately condition hidden state previous time step next time step formulate even number hidden state condition leave context odd number hidden state condition right context extend idea deep transition note deep model typically augment direct connection input output case may mean direct connection embed encoder connection layer pas input directly output refinement die obama figure alignment v alignment point traditional word align method show attention state shade box depend alignment value generally match note instance prediction output auxiliary verb pay attention entire verb group strain residual connection help train early deep architecture skip basic function model deep architecture exploit enrich typically see residual connection early train stage initial reduction model le improvement converge model reading recent work show good result stack deep transition encoder well alternate network encoder large number variation use skip choice v number layer still need explored empirical various data condition guide alignment training attention mechanism neural machine translation model motivate need align output word input word figure show example attention weight give english input word german output word translation sentence attention value typically match pretty well word alignment use statistical machine obtain tool fast align implement variant ibm model several good us word alignments beyond intrinsic value improve quality translation instance next look use attention chapter neural machine translation mechanism explicitly track coverage input may also want override preference neural machine translation model translation certain terminology expression measurement well handle rule base require know neural model translate source word also end user may interest alignment translator use machine translation computer aid translation tool may want check output word originate instead trust attention mechanism implicitly acquire role word may enforce role idea provide parallel corpus train also word alignment use traditional mean additional information may even train model converge faster overcome data sparsity low resource condition straightforward way add give word alignment training process change model modify train objective goal train neural machine translation model generate correct output word add goal also match give word alignment assume access alignment matrix alignment point input word output word way output word alignment score add model estimate attention score also add output equation page mismatch give alignment score compute attention score measure several cross entropy cost ce log mean square error cost cost add train objective may weight reading chen add supervised word alignment information traditional statistical word alignment training augment objective function also optimize match attention mechanism give alignment model coverage one impressive aspect neural machine translation model well able trans late entire input even lot reorder involved aspect occasionally model translates input word multiple sometimes miss translate refinement um problem figure example generation input tokens around social housing attend lead hallucinate output word company end sentence fresh start attend untranslated see figure example translation two related attention begin phrase alliance receives much result faulty translation hallucinate company society social education end input phrase fresh start receive attention hence untranslated output obvious idea strictly model coverage give attention reasonable way coverage add attention state complete sentence roughly expect input word receive similar amount attention input word never receive attention much attention signal problem translation enforce coverage inference may restrict enforce proper coverage decoder consider multiple hypothesis beam age one pay much attention input word hypotheses penalize pay little attention input various way come score function generation generation chapter neural machine translation coverage generation max generation min coverage coverage use multiple score function decoder common practice traditional machine translation neural machine translation challenge give proper weight different score function two three optimize grid search possible value may borrow method mira statistical machine translation coverage model vector accumulate coverage input word may directly use inform attention model attention give input word condition previous state decoder representation input word also add condition context accumulated attention give word equation page coverage coverage tracking may also integrate train objective take page guide alignment train previous section augment training function coverage penalty weight log coverage note problematic add additional function learn ob since distract main goal produce good translation fertility describe coverage need cover input word roughly evenly even early statistical machine translation model consider fertility number output word generate input word consider english language require equivalent negate verb word translate multiple output word german may translate course thus generate output word may augment model coverage add fertility component predict number output word input word one example model predict fertility input use normalize coverage statistic coverage refinement fertility predict neural network layer condition input word representation use activation function result value scale maximum fertility feature engineering versus machine learn work model coverage neural machine translation model nice example contrast engineering approach belief generic machine learn technique engineering good way improve system analyze weak point consider change overcome notice generation generation respect add component model overcome problem proper coverage one feature good translation machine learn able get training data able may need deep robust estimation way adjustment give right amount power need problem hard carry analysis need make generic machine learn give complexity task like machine translation argument deep learning require feature add coverage model remain see neural machine translation evolve next move engineering machine learn direction reading well model tu add coverage state input word either sum attention scale fertility value predict input word learning coverage update function feed forward neural network layer coverage state add additional conditioning context prediction attention state condition prediction attention state also previous context state also introduce coverage state sum input source aim subtract cover word step separate hide state keep track source coverage hide state keep track produce output add number bias model alignment inspired traditional statistical machine translation model condition prediction attention state absolute word attention state previous output word limit coverage attention state limit window also add fertility model add coverage train objective adaptation text may differ degree common problem practical development machine translation system available train data different data relevant choose use case goal translate chat room realize little translated chat room data available massive quantity publication international random translation crawl maybe somewhat relevant movie subtitle translation chapter neural machine translation general training data initial train general system domain train data adaptation adapt system figure online train neural machine translation model allow straightforward domain adaptation general domain translation system train general purpose handful additional training epoch domain data allow domain adapted system problem generally frame problem domain adaptation simple one set data relevant use case domain data another set less relevant domain data traditional statistical machine vast number method domain propose model may may back domain domain may sample domain data train sub sample domain etc neural machine fairly straightforward method currently pop figure method divide train two stage train model available data convergence run iteration train domain data stop training performance domain validate set peak model training still specialize domain data practical experience method show second domain training stage may converge quickly amount domain data typically relatively handful train epoch need less commonly use method draw idea ensemble decode train separate model different set may combine ensemble decode want choose weight although choose weight trivial task domain domain may simply do line search possible value let u look special case arise practical use domain data large collection common problem amount available domain data train even secondary adaptation risk good performance see data poor everything else refinements large random collection parallel text often contain data closely match domain data may want extract domain data large collection mainly domain data general idea behind variety method build two one domain detector train domain one domain detector train domain data score sentence pair domain data detector select sentence pair prefer judged relatively domain detector classic detector language model train source target side domain domain result total language source side domain model target side domain model source side domain model target side domain model give sentence pair domain data score base relevance may use traditional gram language model neural recurrent language model work suggest replace open class word part speech tag word cluster sophisticated model consider domain relevance noisiness training data misalign mistranslate may even use domain domain neural translation model score sentence pair stead source target side sentence isolation data may use several way may train data build system use secondary adaptation stage outline monolingual domain data parallel data domain use two main idea explore may still use monolingual may source target language parallel data large pile general outline another idea use exist parallel data train domain back translate domain data section generate synthetic domain use data adapt initial model traditional statistical machine much adaptation success achieve interpolating language idea neural translation equivalent multiple domains multiple collection data clearly domain typically category information etc use technique describe build specialized translation model domains give test select appropriate model know domain test build allow u automatically chapter neural machine translation make determination may base method domain detector describe give decision select appropriate model commit single domain may instead provide distribution relevance domain model domain domain domain use weight ensemble model domain may do base whole document instead individual brings context make robust decision hard give conclusive advice handle adaptation since broad topic style text may relevant content data may differ narrowly publication united nation vs european dramatically chat room v publish amount domain domain data differs data may cleanly separate domain come massive disorganize pile data may higher translation quality may pollute noise even generate machine translation system reading often domain mismatch bulk even train data translation test data deployment rich literature traditional statistical machine translation topic common approach neural model train available training run iteration domain data already pioneer neural language model adaption demonstrate effectiveness adaptation method small domain set consist little sentence pair argue give small amount domain data lead suggest mix domain domain data adaption identify problem suggest use ensemble baseline model adapt model avoid consider alternative training method adaptation phase consistently well result traditional gradient descent training inspire domain adaptation work statistical machine translation sub sample sentence chen build domain v domain sentence pair training use prediction score reduce learn rate sentence pair domain show traditional statistical machine translation outperform neural chine translation train general purpose machine translation system collection test niche domain adaptation technique allow neural machine translation catch multi domain model may train informed run time domain input sentence apply idea initially propose augment input sentence register politeness feature token domain adaptation problem add domain token training test sentence chen report well result token approach adapt topic encode give topic membership sentence additional input vector condition context word prediction layer refinement add linguistic annotation one big debate machine translation research question key progress develop relatively machine learn method implicitly learn important feature use linguistic insight augment data model recent work statistical machine translation demonstrate motivated model best statistical machine translation system major evaluation campaign language pair syntax base translate also build syntactic structure output sentence serious effort move towards deeply semantics machine translation turn towards neural machine translation hard swing back towards well machine learn ignore much linguistic insight neural machine translation view translation generic sequence sequence happen involve sequence word different language method byte pair encode character base model even put value concept word basic unit doubt recently also attempt add linguistic annotation neural translation step towards linguistically motivate model take look successful effort integrate linguistic annotation input linguistic annotation output build linguistically structured model linguistic annotation input one great neural network cope rich context neural machine translation model word prediction condition entire input sentence previously generate put word even typically input sequence partially generate output sequence never observe neural model able generalize training data draw relevant knowledge traditional statistical mod require carefully choose independence assumption back scheme add information condition context neural translation model accommodate rather straightforwardly information would like typical linguistic treasure chest contain part speech morphological property syntactic phrase syntactic maybe even semantic annotation format annotation individual input word require bit syntactic semantic annotation span multiple word see figure example walk linguistic annotation word girl part speech noun lemma girl surface form lemma differs watch watch morphology singular chapter neural machine translation word girl watch attentively beautiful part speech adv lemma girl watch attentive beautiful morphology sing past plural noun phrase begin cont begin cont cont verb phrase begin cont cont cont cont dependency girl watch watch watched depend relation subj adv adj obj semantic role actor manner mod patient semantic type human view animate figure linguistic annotation format word level factor representation word continuation cont noun phrase start word part verb phrase syntactic head watch dependency relationship head subject subj semantic role actor many scheme semantic type instance girl could man note phrasal annotation handle noun phrase girl common use annotation scheme tag individual word phrasal begin continuation intermediate word outside phrase encode word level factor recall word initially represent hot vector encode factor factored representation hot vector concatenation vector use input word embed note mathematically factor representation map embed word embed sum factor since input neural machine translation system still sequence word embed change anything architecture neural machine translation model provide rich input representation hope model able learn take advantage come back debate linguistics versus machine learning linguistic notation propose arguable learn automatically part word contextualized word hide encoder may may true provide additional knowledge come tool produce notation particularly relevant enough training data automatically induce make job hard machine learn algorithm force machine learn discover feature readily refinement sentence girl watch attentively beautiful syntax tree np vp girl watch adv np attentively beautiful np girl vp watch adv attentively np beautiful figure phrase structure grammar tree sequence word watch tag np question resolve empirically demonstrate actually work data condition linguistic annotation output do input word could do also output word instead discuss point adjustment need make separate output let u take look another annotation scheme output successfully applied neural machine translation syntax base statistical machine translation model focus add syntax output side traditional gram language model good promoting among neighbor powerful enough ensure overall output sentence design model also produce evaluate syntactic parse output syntax base model give mean promote grammatically correct output word level annotation phrase structure syntax suggest figure rather crude nature language annotate nested phrase can not easily handle begin cont scheme typically tree structure use represent syntax see figure example show phrase structure syntactic parse tree example sentence girl watch attentively beautiful generate tree structure generally quite different process generate sequence typically build recursively bottom algorithms chart parse parse tree sequence word structural token indicate begin np end closing parenthesis syntactic phrase force syntactic parse tree annotation sequence sequence neural chine translation model may do encode parse structure additional output token perfectly idea produce output neural translation sequence sequence mix output word special token chapter neural machine translation hope force neural machine translation model produce syntactic structure encourage produce syntactically well form output evidence support despite simplicity approach linguistically structured model syntactic parsing leave untouched recent wave neural network previous section suggest syntactic parsing may do simply frame sequence sequence additional output token best perform syntactic use model structure take recur nature language heart either inspired network build parse tree neural version leave right push main stack open phrase new word may extend push stack start new phrase early work integrate syntactic parsing machine translation framework consensus best practice emerge yet time clearly still challenge future work reading wu propose use factor representation word part factor encode one hot input recurrent neural network language model use representation input output neural machine translation demonstrate good translation quality multiple language pair two language world also train data many language sometimes highly overlap european parliament proceeding sometimes unique canadian french language lot train data available language include commercially interesting language pair long history move beyond language encode mean language sometimes call machine idea map input language map output language build one map step one step language translate languages do researcher deep learn often hesitate claim intermediate state neural translation model encode semantics mean train neural machine translation system accept text language input translate refinement multiple input language let u two parallel one one train neural machine translation model corpora time simply concatenate input vocabulary contains german french word input sentence quickly recognize either german due sentence word combine model train data set one advantage two separate mod expose english side parallel corpora hence learn well language model may also general diversity lead robust model multiple output language trick output corpus give french input sentence would system know output language crude effective way signal model add tag like spanish token input sentence english pa case double spanish pa verse con train system three corpus mention also use translate sentence german spanish without ever present sentence pair train data system spanish verse con representation mean input sentence tie input language output language experiment show actually somewhat achieve good parallel data desire language pair much less standalone model figure summarize idea single neural machine translation train parallel corpus result system may translate see input output language likely increasingly deep model section may better serve multi language since deep layer compute abstract rep language idea mark output language token spanish explore widely context system single language pair token may represent domain input sentence require level politeness output sentence chapter neural machine translation french german mt english spanish figure multi language machine translation system train one language pair rotate many train even able translate german spanish share component instead throw data generic neural machine translation may want carefully consider component may share among model idea train one model per language component identical unique model encoder may share model input language decoder may share model output language attention mechanism may share model language pair share component mean parameter value use separate model update train model one language pair also change model language pair need mark output since model train language pair idea share training component also push exploit mono lingual data encoder may train monolingual input language need add train objective language model decoder may train isolation monolingual language model data since context state blank may lead learn ignore input sentence function target side language model reading johnson explore well single canonical neural translation model able learn multiple multiple simultaneously train parallel corpora several language pair show small several input language output mixed result translate multiple output language additional input language interesting result ability model translate language direction parallel corpus thus demonstrate mean representation although less well use traditional pivot method support multi language input output train encoders decoder share attention mechanism alternate architecture input word layer layer layer figure encode sentence neural network always use two size convolution differ decode revers process alternate architectures neural network research focus use recurrent neural network attention mean architecture neural network vantage use recurrent neural network input side require long sequential process consumes input word one step also prohibit ability processing word thus limit use capability alternate suggestion architecture neural machine model present section remain curiosity conquer neural network end end neural machine translation model modern era actually base recurrent neural base neural network show successful image thus look application natural next step see figure illustration network encodes input sen basic building block network convolution merges representation input word single representation use matrix apply convolution every sequence input word reduce length sentence representation repeat process lead sentence representation single vector illustration show architecture two follow layer merges sequence phrasal representation single sentence size kernel depend length sentence example show word sentence sequence layer longer big kernel need hierarchical process build sentence representation bottom well ground linguistic insight recursive nature language similar chart except commit single hierarchical structure ask chapter neural machine translation input word encode layer encode layer transfer layer decode layer decode layer select word output word embed figure neural network model convolution result single sentence embed sequence encoder also inform recurrent neural network output word decode layer awful lot result sentence embed represent mean entire sen arbitrary length generate output sentence translation reverse bottom process one problem decoder decide length output sentence one option address problem add model predicts output length input length lead selection size reverse convolution matrix see figure illustration variation idea show architecture always use result sequence phrasal single sentence embed explicit mapping step phrasal representation input word phrasal representation output call transfer layer decoder model include recurrent neural network output side sneak recurrent neural network undermine bit argument good claim still hold true encode sequential language model powerful tool disregard describe neural machine translation model help set scene neural network approach machine could demonstrate achieve competitive result compare traditional approach compression sen representation single vector especially problem long sentence model use successfully candidate translation generate traditional statistical machine translation system alternate architecture input word convolution layer convolution layer convolution layer figure encoder use stacked layer number layer may use neural network attention propose architecture neural network combine idea neural network attention mechanism essentially sequence sequence attention describe canonical neural machine translation recurrent neural network replace layer introduce convolution previous section idea combine short sequence neighboring word single representation look another convolution encode word leave right limit window let u describe detail mean encoder decoder neural model encoder see figure illustration layer use encoder input state layer inform correspond state layer two neighbor note layer shorten convolution center around use pad zero word position bound start input word ex progress sequence layer different depth maximum depth function feed forward residual connection correspond previous layer state note even representation word may inform partial sentence context contrast bi directional recurrent neural network canonical model relevant context word input sentence help disambiguation may outside window computational advantage idea word one depth process even combine one massive tensor operation gpu chapter neural machine translation input context output word prediction decoder convolution decoder convolution output word embed select word figure decoder neural network attention decoder state compute sequence layer already predict output word state also inform input context compute input sentence attention decoder decoder canonical model also core recurrent neural network recall state progression equation page encoder embed previous output input context version recurrent decoder depend previous state condition sequence recent previous word decoder convolution may encoder layer see figure illustration equation main difference canonical neural machine translation model architecture condition state decoder compute sequence also always input context attention attention mechanism essentially unchanged canonical neural trans model recall base association word compute encoder previous state decoder back equation page alternate architecture since still encoder decoder state use association score normalize use compute weight sum input word encoder state encoder state input word embed combine via addition compute context vector usual trick use residual connection assist train deep neural network self attention critique use recurrent neural network require lengthy word entire input time consume limit previous section replace recurrent neural network canonical model con limit context window enrich representation word would like architectural component allow u use wide context highly could already encounter attention mechanism considers association tween every input word output use build vector representation entire input sequence idea behind self attention extend idea encoder instead compute association input output self attention com association input word input word one way view mechanism representation input word enrich context word help disambiguate compute self attention self attention sequence vector size pack matrix self attention let u look equation detail association every word representation context word do via dot product pack matrix transpose result vector raw association value value vector scaled size word representation vector value add result vector normalize association value use weigh context word another way put equation without matrix notation use word vector exp exp normalize association self attention weight sum raw association chapter neural machine translation self attention layer self attention step describe one step self attention layer use encode input sentence four step follow combine self attention residual connection pass word representation directly self attention next layer normalization step section page layer normalization self attention standard feed forward step activation function apply also augment residual connection layer normalization layer normalization take page deep stack several layer top ex start input word embed self attention layer deep modeling reason behind residual connection self attention layer residual connection help train since allow shortcut input may utilize early stage take advantage complex deep model enable layer normalization step one standard train trick also help especially deep model attention decoder self attention also use output word decoder also traditional attention total sub layer self output word initially encode word per form exactly self attention computation describe equation association word limit word previously produce output word let u denote result sub layer output word attention mechanism model follow closely self attention difference compute self attention hide state compute attention decoder state encoder state attention sh alternate architecture input word self attention layer self attention layer decoder layer decoder layer output word prediction select output word output word embed figure attention base machine translation input encode several layer self attention decoder compute attention base representation input several initialize previous word use detailed exposition exp attention weight sum attention computation augment add residual layer additional like self attention layer describe worth note output attention computation weight sum input word representation add representation decoder state via residual connection allow skip deep thus speed train feed forward sub layer identical sub layer follow add norm step use residual layer normalization note description attention sub entire model show figure raw association sh exp normalize association chapter neural machine translation reading build comprehensive machine translation model encode source sentence neural generate target sentence reverse process propose use multiple layer encoder decoder reduce length encode sequence incorporate wider context layer replace recurrent neural network use sequence sequence model multiple self attention encoder well decoder number additional call multi head encode sentence position etc current challenge neural machine translation emerge promising machine translation approach recent show superior performance public benchmark rapid adoption deployment google also report poor system build low resource condition lorelei pro gram examine number challenge neural machine translation give empirical result well technology currently hold compare traditional statistical chine translation show despite recent neural machine translation still overcome various notably performance domain low resource condition lot problem common neural translation model show robust behavior confront condition differ train condition may due limited exposure train unusual input case domain test unlikely initial word choice beam search solution problem may hence lie general approach training step outside optimize single word prediction give perfectly match prior sequence another challenge examine neural machine translation much less answer question training data lead system decide word choice decode bury large matrix real numbered value clear need develop well neural machine translation use common neural machine translation traditional phrase base statistical machine translation common data drawn opus unless note use default beam search single model decode train data process byte pair encode word vocabulary limit evaluation current challenge statistical machine translation system train use moses build phrase base system use standard feature commonly use recent submission ding consider phrase base note statistical machine translation hierarchical phrase base model syntax base model show give superior performance language pair carry experiment large train data set available use share translation task organize alongside conference machine translation domain use opus corpus except domain use test set compose news characterize broad range formal relatively long sentence word high standard style domain mismatch know challenge translation different word different trans mean express different style crucial step develop chine translation system target use case domain adaptation expect method domain adaptation developed neural machine translation currently popular approach train general domain follow train domain data epochs large amount train data available still seek robust performance test well neural machine translation statistical machine translation hold trained different system use different corpus obtain opus additional system train train data statistic corpus size show table note domains quite distant much ted news global voice train statistical machine translation neural machine translation system domains system train tune test set sub sample data use common byte pair encode use training run see figure result domain neural statistical machine translation system similar machine translation well statistical machine translation well domain performance machine translation system bad almost sometimes dramatically use customary domain machine domain corpus may differ domain level etc chapter neural machine translation corpus word sentence law medical koran subtitle table corpus use train take opus repository corpus system law medical koran subtitle data law medical koran subtitle figure quality system train one domain test another domain neural machine translation system show degraded performance domain current challenge source um reference look around look around look around law order implement medical mb en final work around switch pause koran take heed soul see subtitle look around look around figure examples translation sentence subtitle translate system train different corpus performance domain dramatically bad neural machine translation instance medical system lead score machine v machine law test set figure display example translate sentence um look around subtitle see mostly completely unrelated output neural machine translation system translation system switch pause note output neural machine translation system often quite take heed soul completely unrelated statistical machine translation output betrays cop domain input leave word untranslated around particular concern mt use information user mislead hallucinate content neural machine translation output amount training data well know property statistical system increase amount train data lead well result statistical machine translation previously observe double amount training data give increase score hold true parallel monolingual data irvine data need statistical machine translation neural machine translation neural machine translation promise generalize good word sim condition large context input prior output chapter neural machine translation score vary amount train data phrase base big phrase base neural corpus size figure score english spanish system train million million word parallel data quality neural machine translation start much outperforms statistical machine translation million even beat statistical machine translation system big billion word domain language model high resource condition current challenge ratio word republican strategy counter election obama figure translation sentence test set use neural machine translation system train vary amount train data low resource neural machine translation produce output unrelated input build english spanish system million english word pair machine language model train spanish part respectively addition neural statistical machine translation system train also use additionally provide monolingual data big language model statistical machine translation system result show figure neural machine translation exhibit much steep outperform statistical machine translation v data million even beat statistical machine translation system big language model full data set neural machine statistical machine statistical big language contrast neural statistical machine translation learn curve quite striking neural machine translation able exploit increase amount train data unable get ground training corpus size million word less strategy election start translation become respectable noisy data statistical machine translation fairly robust noisy data quality system hold fairly even large part train data corrupt various align content wrong badly translate etc statistical chine translation model build probability distribution estimate many occur word phrase unsystematic noise training affect tail end distribution spanish last represent use data chapter neural machine translation ratio table impact noise training part train corpus contain sentence pairs neural machine translation degrade statistical machine translation hold fairly well still case neural machine chen consider one kind misalign sentence pair experiment large parallel corpus target side part train sentence pair table show result statistical machine translation system hold fairly well even data quality drop expect half valid training data neural machine translation system degrade drop com par point drop statistical system possible explanation poor behavior neural machine translation model prediction good balance language model input context main driver training observe increase ratio train input sen meaningless may generally learn rely output language model hence hallucinate inadequate output word alignment key contribution attention model neural machine translation imposition alignment output word input word take shape probability distribution input word use weigh bag word representation input sentence attention model functionally play role word alignment source least way analog statistical machine translation alignment latent variable use obtain probability distribution word arguably attention model broad role translate attention may also pay subject object since may disambiguate complicate word representation product bidirectional gated recurrent neural network effect word representation inform entire sentence context clear need alignment mechanism source target word prior work use alignment provide attention model interpolate word translation decision traditional probabilistic dictionary introduction coverage fertility model etc current challenge die relationship obama obama stretch year desire alignment mismatch alignment figure word alignment compare attention model state box probability percent alignment obtain fast align attention model fact proper examine compare soft alignment matrix sequence attention word alignment obtain traditional word alignment method use incremental fast align align input output neural machine system see figure illustration compare word attention state word alignment obtain fast align match pretty well attention state fast align alignment point bite fuzzy around function word attention model may settle alignment correspond tuition alignment point obtain fast align see figure reverse language alignment point appear one position aware intuitive explanation divergent behavior translation quality high system measure well soft alignment neural machine system match alignment fast align two match score check output align input word accord fast align indeed input word receive high attention probability mass score sum probability mass give alignment point obtain fast align chapter neural machine translation language pair match prob table score indicate overlap attention probability alignment obtain fast align handle byte pair encode many many alignment use neural machine translation model provide edinburgh run fast align parallel data set obtain alignment model use align input output neural machine translation system table show alignment score system result suggest divergence outlier see large divergence also different data condition note attention model may produce good word alignment guide alignment training supervise word alignment one produce provided model train beam search task decode full sentence translation high probability machine problem address heuristic search technique explore subset space possible translation common feature search technique beam size parameter limit number partial translation maintain per input word typically straightforward relationship beam size parameter model score result translation also quality score diminish return increase beam typically improvements score expect large beam decode neural translation model set similar fashion predict next output may commit high score word prediction neural machine translation operate fast align run full word input word split byte pair add attention score output word split take average attention vector match score probability mass score compute average output word level score output word fast align alignment ignore computation output word fast align multiple input match count correct align word among top high scoring word accord attention probability mass add attention score current challenge english czech normalize normalized beam size beam size normalized beam size normalized beam size normalized normalized beam size beam size normalized normalized beam size beam size figure translation quality vary beam size large quality especially normalizing score sentence length chapter neural machine translation also maintain next best scoring word list partial translation record partial translation word translation probability extend partial translation subsequent word prediction accumulate score since number partial translation explodes exponentially new output prune beam high score partial translation traditional statistical machine translation increase beam size allow u explore large set space possible translation hence translation well model score figure increase beam size consistently improve translation quality almost bad translation find beyond optimal beam size set use edinburgh optimal beam size varies around normalize sentence level model score length output alleviates problem somewhat also lead well optimal quality case language pair optimal beam size range almost quality still drop large beam main cause deteriorate quality short translation wider beam reading study look comparable performance neural statistical machine translation system consider different linguistic category german compare different broad aspect reorder nine language direction additional topic especially early work neural network machine translation aim build neural compo use traditional statistical machine translation system translation model include align source word condition enrich feed forward neural network language model source context add sentence embed conditional context learn use variant neural network map across language use complex neural network encode input sentence use gate layer also incorporate information output context reorder model reorder model struggle sparse data problem con rich context li show neural reorder model condition current previous phrase pair recursive neural network make decision orientation type additional topic instead hand reorder within decode may input sentence output word order use input dependency tree learn model swap child node implement use feed forward neural network formulate top leave right walk dependency tree make reorder decision node model process recurrent neural network include past decision condition context gram translation model alternative view phrase base translation model break phrase translation minimal translation employ gram model unit condition minimal translation unit previous one treat minimal translation unit atomic symbol train neural language model represent minimal translation unit bag break even single input single output single input output word use phrase lean auto encoder chapter neural machine translation bibliography philip graham incorporate discrete translation icon neural machine translation proceeding conference empirical method natural language processing association computational page neural machine translation jointly learn align translate paul phil pragmatic neural language machine translation proceeding conference north american chapter association computational lin human language technology association computational page paul phil neural language framework machine translation prague bulletin mathematical linguistics pascal christian neural probabilistic language model journal machine learn research luisa mauro federico neural versus phrase base machine translation case study proceeding conference empirical meth natural language processing association computational page christian yvette barry matthias antonio ne mariana martin matt raphael carolina scar lucia marco karin marcos finding conference machine translation proceeding first conference chine translation association computational page francisco enrique vidal machine translation use neural network model page box colin george samuel cost weight machine translation domain adaptation proceeding first workshop machine translation association computational page box roland george colin huang bilingual method bibliography adaptive training data selection machine translation annual meeting association chine translation america peter guide alignment training topic aware neural machine translation david hierarchical phrase base translation computational linguistics bart van property neural machine approach proceeding eighth workshop semantics structure statistical translation association computational page empirical comparison domain adaptation method neural machine translation proceeding th annual meeting association computational linguistics short association computational page trevor cong chris incorporate structural alignment bias neural translation model proceeding conference north american chapter association computational human language technology association computational san cal page maria anabel kathy jean patrice joshua catherine jean mar alexandra thomas natalia cyril peter pure neural machine translation system gonzalo bill fast accurate use neural network proceeding conference north american chapter association computational human language technology association computational page jacob thomas richard john fast robust neural network joint model statistical machine translation proceeding nd annual meeting association computational linguistics long association computational page kevin matt post machine translation system proceeding first conference chine translation association computational page john singer adaptive method online learn stochastic optimization journal machine learn research chris victor noah smith effective model proceeding conference north american chapter association computational human language technology association computational page bibliography marco nicola federico neural v phrase base machine translation multi domain scenario proceeding th conference european chapter association computational vol short paper association computational page nan mu ming kenny improve attention model implicit distortion fertility machine translation proceeding th international conference computational technical paper organize page andrew paul phrase base machine system recurrent neural network language model proceeding th name en workshop association computational page multilingual neural chine translation share attention mechanism proceeding conference north american chapter association computational human language technology association computational san page recursive hetero associative memory translation biological neuroscience technology page fast domain adaptation neural machine translation technical report michel jonathan kevin daniel steve wei ignacio inference training context rich syntactic translation model pro st international conference computational linguistics th annual meeting association computational linguistics association computational page michel mark kevin daniel whats translation proceeding joint conference human language technology annual meeting north american chapter association computational linguistics jonas michael david denis dauphin sequence sequence learn adam greg jimenez david timothy generative temporal model memory goldberg neural network method natural language processing volume synthesis lecture human language technology morgan san ca ian aaron deep learning mit press hang victor li incorporate copy mechanism sequence sequence learn proceeding th annual meeting association linguistics long association computational page bibliography bowen point unknown word proceeding th annual meeting association computational linguistics long association computational page kelvin use monolingual corpora neural machine trans michael minimum translation model recurrent neural network proceeding th conference european chapter association computational linguistics association computational page ann irvine chris combine bilingual comparable corpus low resource machine translation proceeding eighth workshop statistical chine translation association computational page roland use large target vocabulary neural machine translation proceeding rd annual meeting association computational linguistics th international joint conference natural language processing long association computational page roland mon neural machine translation system proceeding tenth workshop statistical machine translation association computational page melvin mike maxim martin greg jeffrey dean google multilingual neural machine translation enable zero shot translation neural chine translation ready case study translation direction proceed international workshop speak language translation phil recurrent continuous translation model proceeding conference empirical method natural language processing association computational page shin neural reorder model consider phrase translation word alignment phrase base translation proceeding rd workshop asian translation organize page jimmy ba method stochastic optimization catherine jean domain control neural machine translation technical report alexandra chris nicola bibliography brooke wade christine richard christopher alexandra evan open source toolkit statistical machine translation proceeding th annual meeting association computational linguistics companion volume proceeding demo poster session association computational czech page lei hinton layer normalization print yang neural reorder model phrase base translation proceeding th international conference com technical paper dublin city university association computational lin page andrew neural machine translation supervise attention proceeding th international conference computational technical paper organize page learn new semi supervise deep auto encoder statistical machine translation proceeding nd annual meeting association computational linguistics long association computational page christopher man stanford neural machine translation system speak language domains proceeding international workshop speak language translation page michael christopher man deep neural language mod machine translation proceeding nineteenth conference computational language learn association computational page christopher man effective approach attention base neural machine translation proceeding conference empirical method natural language processing association computational page address rare word problem neural machine translation proceeding rd annual meeting association computational linguistics th international joint conference natural language processing long association computational page hang interactive attention neural machine translation proceeding th international conference computational lin technical paper organize page hang encode source language neural network machine translation proceeding rd annual meeting association computational linguistics th international joint conference natural language processing long association computational page abe vocabulary manipulation neural machine translation proceeding th annual meeting association computational linguistics short association computational page bibliography antonio giuseppe dependency base reorder recurrent neural network machine translation proceeding rd annual meeting association computational linguistics th international joint conference nat language processing long association computational page antonio rico barry alexandra birch deep architectures neural machine translation proceeding second conference machine volume research paper association computational den page tomas statistical language model base neural network brno university technology tomas wen tau geoffrey linguistic regularity continuous space word representation proceeding conference north american chapter computational human language technology association computational page tomas train recurrent neural network proceeding th international conference machine page luis francisco online learn neural machine trans post edit continuous space language model computer speech language continuous space language model statistical machine translation prague bulletin mathematical linguistics continuous space translation model phrase base statistical machine trans proceed poster organize page marta ruiz jose smooth bilingual gram translation proceeding joint conference empirical method natural process computational natural language learning page daniel continuous space language model statistical machine translation proceeding main confer poster session association computational page anthony prune continuous space language model statistical machine translation proceeding ever really replace gram future language model association computational page rico barry linguistic input feature improve neural machine translation proceeding first conference machine translation association computational page bibliography rico barry alexandra birch control politeness neural machine translation via side constraint proceeding conference north american chapter association computational human language technology association computational san page rico barry alexandra birch edinburgh neural chine translation system proceeding first conference machine translation association computational page rico barry alexandra birch improve neural machine translation mod monolingual data proceeding th annual meeting association computational linguistics long association computational page rico barry alexandra birch neural machine translation rare word unit proceeding th annual meeting association computational lin long association computational page maria jean domain post training domain adaptation neural machine translation geoffrey alex simple way prevent neural network journal machine learn research martin ben comparison recurrent network language model ieee international conference signal processing page le sequence sequence learn neural network advance neural information process system page alex incremental adaptation strategy neural network language model proceeding rd workshop continuous tor space model association computational page parallel tool interface opus khalid joseph jan proceeding eighth international conference resource evaluation european language resource association page anthology antonio multifaceted evaluation versus phrase base machine translation language direction proceeding th conference european chapter association computational volume long paper association computational page bibliography yang hang li context gate neural machine translation yang hang li neural machine trans reconstruction proceeding st conference intelligence yang hang li model coverage neural machine translation proceeding th annual meeting association computational linguistics long association computational page marco learn performance machine statistical computational analysis proceeding third workshop machine translation association computational page attention need victoria david decode large scale neural language model improve translation proceeding conference empirical meth natural language processing association computational page speech speech translation system use symbolic processing strategy proceed international conference speech signal processing page lu con continuous space language model gram language model statistical machine translation proceeding conference empirical method natural language pro association computational page neural network base bilingual language model grow statistical machine translation proceeding con empirical method natural language processing association computational page philip rico maria matthias barry edinburghs statistical machine translation system proceeding first conference machine translation association computational page wei ting improve statistical machine translation context sensitive bilingual tic embed model proceeding conference empirical method natural process association computational page mike mohammad wolfgang maxim yuan klaus jeff melvin stephan keith george wei cliff jason jason alex greg jeffrey dean google neural machine bibliography translation bridge gap human machine translation factor recurrent neural network language model ted lecture transcription pro seventh international workshop speak language translation page di tao dual learning machine translation recurrent neural network base rule sequence model machine translation proceeding rd annual meeting association com linguistics th international joint conference natural language process short association computational page matthew adaptive learning rate method local translation prediction global sentence rep proceeding twenty fourth international joint conference intelligence page bibliography author index david philip giuseppe michael jimmy paul luisa nicola alexandra phil patrice bill chris yuan francisco luis mauro victor box colin david trevor alexandra greg marta ruiz aaron brooke maria jeffrey daniel steve jacob paul author index john kevin chris christopher christian andrew jose victoria george ben michel jonas ian stephan jonathan yvette david barry di wei evan geoffrey cong mark matthias gonzalo ann abe christian antonio joshua melvin shin michael author index jeff kevin catherine maxim alex roland george thomas samuel lei hang mu victor timothy ting yang jean klaus wolfgang john christopher christopher daniel roland antonio tomas christine maria graham mariana mohammad author index martin matt alexandra tao anabel jimenez thomas jason anthony raphael alex adam carolina mike richard natalia jean rico wade jason noah lucia keith martin alex ignacio cyril antonio marco van bart karin enrique pascal wei martin greg philip author index kelvin kathy nan denis wen tau cliff marcos matthew richard bowen ming kenny peter geoffrey']\n"
     ]
    }
   ],
   "source": [
    "print(data_words[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cd8bd4",
   "metadata": {},
   "source": [
    "## Build the Bigram, Trigram Models and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47142ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd5e23ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-24 08:17:14.270046: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-24 08:17:14.270075: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /home/karmel/.local/lib/python3.8/site-packages (from en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/karmel/.local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: jinja2 in /home/karmel/.local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/karmel/.local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/karmel/.local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/karmel/.local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.21.2)\n",
      "Requirement already satisfied: setuptools in /home/karmel/.local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (62.2.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/karmel/.local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/karmel/.local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.62.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/karmel/.local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/karmel/.local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/karmel/.local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/karmel/.local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/karmel/.local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/karmel/.local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/karmel/.local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /home/karmel/.local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.15)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/karmel/.local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/karmel/.local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/karmel/.local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.22.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/karmel/.local/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/karmel/.local/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/karmel/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.0.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/karmel/.local/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/karmel/.local/lib/python3.8/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.1)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec6fc8cf",
   "metadata": {
    "id": "ec6fc8cf"
   },
   "outputs": [],
   "source": [
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e04160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ready = process_words(data_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "308de0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['statistical', 'machine', 'translation', 'draft', 'chapter', 'translation', 'center', 'speech', 'language', 'processing', 'department', 'computer', 'public', 'draft', 'public', 'draft', 'neural', 'machine', 'translation', 'short', 'history', 'introduction', 'neural', 'network', 'linear', 'layer', 'non', 'linearity', 'inference', 'back', 'propagation', 'training', 'computation', 'graph', 'neural', 'network', 'computation', 'graph', 'gradient', 'computation', 'deep', 'learn', 'framework', 'neural', 'language', 'model', 'forward', 'neural', 'language', 'model', 'word', 'embe', 'inference', 'training', 'recurrent', 'neural', 'language', 'model', 'long', 'short', 'term', 'memory', 'model', 'gate', 'recurrent', 'unit', 'deep', 'model', 'neural', 'translation', 'model', 'encoder', 'decoder', 'approach', 'add', 'alignment', 'model', 'train', 'beam', 'search', 'content', 'ensemble', 'decode', 'large', 'vocabulary', 'monolingual', 'datum', 'deep', 'model', 'guide', 'alignment', 'training', 'model', 'coverage', 'adaptation', 'add', 'linguistic', 'annotation', 'multiple', 'language', 'pair', 'alternate', 'architecture', 'neural', 'network', 'neural', 'network', 'attention', 'self', 'attention', 'current', 'challenge', 'domain', 'mismatch', 'amount', 'training', 'datum', 'noisy', 'datum', 'word', 'alignment', 'beam', 'search', 'read', 'additional', 'topic', 'bibliography', 'author', 'index', 'index', 'chapter', 'machine', 'translation', 'major', 'recent', 'development', 'statistical', 'machine', 'translation', 'adoption', 'neural', 'net', 'work', 'neural', 'network', 'model', 'promise', 'share', 'statistical', 'evidence', 'similar', 'word', 'inclusion', 'rich', 'context', 'chapter', 'introduce', 'several', 'neural', 'network', 'model', 'technique', 'explain', 'apply', 'problem', 'machine', 'translation', 'short', 'history', 'already', 'last', 'wave', 'neural', 'network', 'sight', 'researcher', 'explore', 'method', 'model', 'propose', 'strike', 'similar', 'current', 'dominant', 'neural', 'machine', 'translation', 'approach', 'none', 'model', 'train', 'datum', 'size', 'large', 'enough', 'produce', 'reasonable', 'result', 'toy', 'ex', 'computational', 'complexity', 'involve', 'far', 'exceed', 'computational', 'resource', 'hence', 'idea', 'abandon', 'almost', 'decade', 'hibernation', 'datum', 'drive', 'approach', 'phrase', 'base', 'statistical', 'chine', 'translation', 'rise', 'obscurity', 'dominance', 'machine', 'translation', 'useful', 'tool', 'information', 'increase', 'productivity', 'professional', 'translator', 'modern', 'resurrection', 'neural', 'method', 'machine', 'translation', 'start', 'neural', 'language', 'model', 'traditional', 'statistical', 'machine', 'translation', 'system', 'work', 'show', 'large', 'improvement', 'public', 'evaluation', 'campaign', 'idea', 'slowly', 'mainly', 'due', 'computational', 'concern', 'training', 'pose', 'challenge', 'research', 'group', 'simply', 'hardware', 'experience', 'exploit', 'move', 'language', 'neural', 'network', 'method', 'creep', 'com', 'traditional', 'statistical', 'machine', 'provide', 'additional', 'score', 'ex', 'tend', 'translation', 'table', 'translation', 'model', 'joint', 'translation', 'language', 'model', 'show', 'large', 'quality', 'improvement', 'top', 'competitive', 'statistical', 'machine', 'translation', 'system', 'ambitious', 'effort', 'aim', 'pure', 'neural', 'machine', 'approach', 'completely', 'early', 'step', 'model', 'sequence', 'sequence', 'model', 'able', 'produce', 'reasonable', 'translation', 'short', 'fall', 'apart', 'crease', 'sentence', 'length', 'addition', 'attention', 'mechanism', 'yield', 'competitive', 'result', 'jean', 'encode', 'translation', 'target', 'side', 'monolingual', 'neural', 'machine', 'translation', 'become', 'new', 'state', 'art', 'year', 'entire', 'research', 'machine', 'translation', 'neural', 'give', 'indication', 'speed', 'task', 'translation', 'organize', 'conference', 'machine', 'translation', 'pure', 'neural', 'machine', 'translation', 'system', 'submit', 'outperform', 'traditional', 'statistical', 'system', 'year', 'neural', 'machine', 'translation', 'system', 'almost', 'language', 'pair', 'almost', 'submission', 'neural', 'machine', 'translation', 'system', 'time', 'neural', 'machine', 'translation', 'research', 'progress', 'rapid', 'pace', 'direction', 'explore', 'range', 'core', 'machine', 'learn', 'improvement', 'deep', 'model', 'inform', 'model', 'insight', 'strength', 'weaknesse', 'neural', 'machine', 'translation', 'gather', 'inform', 'future', 'work', 'extensive', 'proliferation', 'available', 'neural', 'machine', 'translation', 'system', 'time', 'number', 'consolidate', 'quite', 'hard', 'premature', 'recommendation', 'promise', 'marian', 'implementation', 'sockeye', 'introduction', 'neural', 'network', 'neural', 'network', 'machine', 'learn', 'technique', 'number', 'input', 'predict', 'output', 'different', 'machine', 'learn', 'method', 'distinct', 'strength', 'introduction', 'neural', 'network', 'figure', 'graphical', 'illustration', 'feature', 'value', 'arrow', 'score', 'output', 'element', 'statistical', 'machine', 'translation', 'potential', 'translation', 'sentence', 'represent', 'set', 'feature', 'feature', 'weight', 'parameter', 'obtain', 'overall', 'score', 'ignore', 'exponential', 'function', 'previously', 'turn', 'linear', 'linear', 'follow', 'formula', 'sum', 'model', 'model', 'illustrate', 'feature', 'value', 'input', 'arrow', 'score', 'output', 'figure', 'model', 'combine', 'different', 'component', 'machine', 'translation', 'phrase', 'translation', 'reorder', 'property', 'length', 'jump', 'distance', 'phrase', 'translation', 'train', 'method', 'assign', 'weight', 'value', 'feature', 'relate', 'importance', 'contribute', 'score', 'translation', 'high', 'machine', 'call', 'tune', 'allow', 'complex', 'relationship', 'feature', 'let', 'short', 'sentence', 'language', 'model', 'less', 'important', 'translation', 'average', 'phrase', 'translation', 'probability', 'high', 'similarly', 'reasonable', 'value', 'really', 'terrible', 'hypothetical', 'example', 'imply', 'dependence', 'feature', 'second', 'example', 'imply', 'non', 'linear', 'relationship', 'feature', 'value', 'impact', 'score', 'linear', 'model', 'handle', 'case', 'commonly', 'cite', 'counter', 'operator', 'truth', 'table', 'feature', 'possible', 'weight', 'give', 'correct', 'output', 'case', 'represent', 'feature', 'linearly', 'separable', 'case', 'case', 'type', 'feature', 'machine', 'translation', 'chapter', 'translation', 'figure', 'neural', 'network', 'hide', 'layer', 'multiple', 'layer', 'neural', 'network', 'modify', 'important', 'way', 'multiple', 'layer', 'instead', 'compute', 'output', 'value', 'directly', 'input', 'hide', 'layer', 'introduce', 'call', 'observe', 'input', 'output', 'train', 'mechanism', 'connect', 'concept', 'hide', 'similar', 'meaning', 'hide', 'markov', 'model', 'figure', 'illustration', 'network', 'process', 'step', 'linear', 'combination', 'weight', 'input', 'produce', 'hide', 'value', 'linear', 'combination', 'weight', 'hide', 'produce', 'output', 'node', 'value', 'let', 'introduce', 'mathematical', 'notation', 'neural', 'network', 'neural', 'network', 'hide', 'layer', 'consist', 'vector', 'input', 'value', 'vector', 'hide', 'value', 'vector', 'output', 'node', 'value', 'matrix', 'weight', 'connect', 'hide', 'matrix', 'weight', 'connect', 'hide', 'node', 'output', 'neural', 'network', 'hide', 'sketch', 'note', 'possibility', 'multiple', 'output', 'node', 'far', 'show', 'introduction', 'neural', 'network', 'hyperbolic', 'tangent', 'logistic', 'function', 'linear', 'unit', 'cosh', 'output', 'range', 'output', 'range', 'output', 'range', 'figure', 'typical', 'activation', 'function', 'neural', 'network', 'linearity', 'carefully', 'addition', 'hide', 'realize', 'gain', 'far', 'model', 'relationship', 'away', 'hide', 'layer', 'weight', 'combination', 'weight', 'feature', 'value', 'obtain', 'value', 'apply', 'function', 'popular', 'choice', 'hyperbolic', 'tangent', 'logistic', 'function', 'figure', 'detail', 'function', 'way', 'activation', 'function', 'segment', 'range', 'value', 'combination', 'segment', 'node', 'turn', 'close', 'transition', 'segment', 'partly', 'turn', 'segment', 'turn', 'close', 'different', 'popular', 'choice', 'activation', 'function', 'unit', 'allow', 'negative', 'value', 'alter', 'value', 'positive', 'value', 'simple', 'fast', 'compute', 'view', 'hide', 'node', 'feature', 'detector', 'certain', 'input', 'turn', 'turn', 'advocate', 'neural', 'network', 'claim', 'chapter', 'translation', 'figure', 'simple', 'neural', 'network', 'bias', 'input', 'hide', 'layer', 'hide', 'obviate', 'least', 'drastically', 'feature', 'engineer', 'instead', 'manually', 'detect', 'useful', 'pattern', 'input', 'train', 'hidden', 'discover', 'automatically', 'stop', 'single', 'hidden', 'layer', 'currently', 'fashionable', 'name', 'deep', 'learn', 'neural', 'network', 'stem', 'fact', 'often', 'performance', 'achieve', 'deeply', 'stack', 'together', 'layer', 'layer', 'hide', 'inference', 'let', 'walk', 'neural', 'network', 'output', 'value', 'compute', 'input', 'concrete', 'example', 'consider', 'neural', 'network', 'figure', 'network', 'additional', 'innovation', 'present', 'bias', 'unit', 'always', 'value', 'bias', 'unit', 'give', 'network', 'work', 'case', 'input', 'value', 'weight', 'weight', 'let', 'neural', 'network', 'process', 'value', 'second', 'input', 'value', 'value', 'hide', 'carry', 'follow', 'calculation', 'calculation', 'node', 'summarize', 'table', 'output', 'value', 'expect', 'binary', 'understand', 'result', 'value', 'threshold', 'range', 'possible', 'output', 'value', 'output', 'possible', 'binary', 'introduction', 'neural', 'network', 'summation', 'activation', 'hide', 'hide', 'output', 'table', 'calculation', 'input', 'network', 'figure', 'input', 'input', 'hide', 'hide', 'output', 'neural', 'network', 'compute', 'look', 'hide', 'notice', 'act', 'boolean', 'value', 'high', 'least', 'input', 'value', 'otherwise', 'low', 'value', 'hide', 'act', 'boolean', 'high', 'value', 'input', 'effectively', 'implement', 'subtraction', 'hide', 'note', 'linearity', 'key', 'value', 'much', 'high', 'input', 'oppose', 'single', 'input', 'distinct', 'high', 'value', 'case', 'manage', 'push', 'output', 'threshold', 'possible', 'value', 'input', 'simply', 'sum', 'recently', 'name', 'deep', 'learn', 'neural', 'network', 'become', 'fashionable', 'emphasize', 'often', 'high', 'performance', 'achieve', 'network', 'multiple', 'hidden', 'layer', 'power', 'single', 'input', 'output', 'layer', 'network', 'possible', 'mimic', 'basic', 'boolean', 'operation', 'neural', 'network', 'example', 'implement', 'boolean', 'operation', 'subtraction', 'second', 'layer', 'function', 'require', 'intricate', 'operation', 'chain', 'hence', 'neural', 'network', 'architecture', 'hide', 'layer', 'possible', 'train', 'build', 'neural', 'network', 'computer', 'number', 'hide', 'layer', 'match', 'depth', 'computation', 'research', 'banner', 'neural', 'ture', 'machine', 'explore', 'kind', 'architecture', 'implement', 'basic', 'neural', 'network', 'hidden', 'layer', 'implement', 'sort', 'bit', 'number', 'back', 'propagation', 'training', 'train', 'neural', 'network', 'require', 'optimization', 'weight', 'value', 'network', 'correct', 'output', 'set', 'training', 'example', 'repeatedly', 'fee', 'input', 'chapter', 'neural', 'machine', 'translation', 'optimum', 'gradient', 'comb', 'current', 'point', 'figure', 'regard', 'dimension', 'case', 'gradient', 'respect', 'weight', 'small', 'gradient', 'respect', 'weight', 'move', 'leave', 'arrow', 'point', 'negative', 'point', 'training', 'example', 'compare', 'compute', 'output', 'network', 'correct', 'output', 'train', 'update', 'weight', 'several', 'train', 'datum', 'carry', 'call', 'common', 'training', 'method', 'neural', 'network', 'call', 'propagation', 'update', 'weight', 'output', 'propagate', 'error', 'information', 'early', 'layer', 'train', 'example', 'error', 'term', 'basis', 'update', 'value', 'incoming', 'weight', 'formula', 'compute', 'update', 'value', 'weight', 'follow', 'descent', 'training', 'error', 'understand', 'function', 'incoming', 'weight', 'reduce', 'error', 'give', 'error', 'function', 'respect', 'move', 'gradient', 'reduce', 'error', 'move', 'gradient', 'consider', 'optimize', 'multiple', 'dimension', 'time', 'look', 'low', 'point', 'area', 'look', 'water', 'ground', 'fall', 'steep', 'west', 'slightly', 'south', 'direction', 'mainly', 'west', 'slightly', 'south', 'gradient', 'figure', 'illustration', 'follow', 'derive', 'formula', 'update', 'weight', 'example', 'network', 'less', 'interested', 'skip', 'section', 'continue', 'read', 'summarize', 'update', 'formulae', 'page', 'weight', 'output', 'let', 'review', 'extend', 'notation', 'combination', 'weight', 'hide', 'value', 'introduction', 'neural', 'network', 'activation', 'function', 'compute', 'output', 'value', 'compare', 'compute', 'output', 'value', 'target', 'output', 'value', 'train', 'example', 'various', 'way', 'compute', 'error', 'value', 'value', 'let', 'norm', 'state', 'goal', 'compute', 'gradient', 'error', 'respect', 'weight', 'direction', 'move', 'weight', 'value', 'weight', 'separately', 'break', 'computation', 'gradient', 'essentially', 'unfold', 'equation', 'let', 'work', 'step', 'error', 'term', 'output', 'value', 'component', 'follow', 'derivative', 'output', 'value', 'respect', 'linear', 'combination', 'weight', 'hide', 'activation', 'function', 'case', 'keep', 'treatment', 'general', 'possible', 'commit', 'activation', 'note', 'give', 'train', 'example', 'give', 'differentiable', 'activation', 'value', 'always', 'compute', 'compute', 'derivative', 'respect', 'weight', 'turn', 'quite', 'simply', 'value', 'hide', 'equation', 'compute', 'step', 'compute', 'error', 'function', 'give', 'unfold', 'lay', 'equation', 'put', 'chapter', 'machine', 'translation', 'factor', 'learn', 'rate', 'give', 'follow', 'update', 'formula', 'weight', 'note', 'remove', 'minus', 'move', 'gradient', 'minimum', 'useful', 'introduce', 'concept', 'error', 'term', 'note', 'term', 'associate', 'weight', 'update', 'concern', 'weight', 'error', 'term', 'incoming', 'weight', 'reduce', 'update', 'formula', 'weight', 'hide', 'computation', 'gradient', 'hence', 'update', 'formula', 'hide', 'node', 'quite', 'anal', 'linear', 'combination', 'input', 'value', 'hide', 'value', 'weight', 'weight', 'weight', 'lead', 'computation', 'value', 'hide', 'follow', 'derivative', 'error', 'respect', 'weight', 'decompose', 'derivative', 'dz', 'error', 'term', 'output', 'value', 'value', 'hide', 'node', 'idea', 'back', 'propagation', 'track', 'error', 'hide', 'contribute', 'error', 'next', 'layer', 'apply', 'chain', 'rule', 'give', 'computation', 'complex', 'case', 'output', 'introduction', 'neural', 'network', 'already', 'encounter', 'term', 'previously', 'third', 'term', 'equation', 'compute', 'equation', 'equation', 'equation', 'solve', 'give', 'rise', 'quite', 'intuitive', 'interpretation', 'error', 'matter', 'hide', 'depend', 'error', 'term', 'subsequent', 'weight', 'impact', 'hide', 'node', 'output', 'node', 'let', 'tie', 'remain', 'loose', 'end', 'miss', 'piece', 'equation', 'second', 'term', 'dz', 'third', 'term', 'dz', 'put', 'equation', 'equation', 'equation', 'together', 'give', 'gradient', 'dz', 'dz', 'error', 'term', 'hide', 'node', 'analogous', 'output', 'analogous', 'update', 'formula', 'chapter', 'neural', 'translation', 'summary', 'train', 'neural', 'network', 'process', 'train', 'update', 'weight', 'time', 'weight', 'update', 'gradient', 'small', 'error', 'weight', 'update', 'base', 'error', 'term', 'associate', 'non', 'network', 'output', 'error', 'term', 'compute', 'actual', 'output', 'current', 'target', 'output', 'node', 'hide', 'error', 'term', 'compute', 'back', 'propagate', 'error', 'term', 'subsequent', 'weight', 'require', 'derivative', 'activation', 'weight', 'incoming', 'value', 'pass', 'give', 'error', 'weight', 'proceed', 'temper', 'learn', 'rate', 'weight', 'next', 'training', 'example', 'process', 'typically', 'pass', 'training', 'call', 'give', 'neural', 'network', 'figure', 'let', 'train', 'example', 'let', 'start', 'calculation', 'error', 'term', 'inference', 'table', 'page', 'compute', 'linear', 'combination', 'weight', 'hide', 'value', 'value', 'target', 'weight', 'hide', 'lead', 'output', 'node', 'calculation', 'error', 'term', 'computationally', 'complex', 'table', 'summarize', 'update', 'weight', 'introduction', 'neural', 'network', 'error', 'term', 'weight', 'update', 'table', 'weight', 'update', 'learn', 'rate', 'neural', 'network', 'figure', 'training', 'example', 'present', 'local', 'optimum', 'global', 'optimum', 'high', 'learn', 'rate', 'bad', 'initialization', 'local', 'optimum', 'figure', 'problem', 'gradient', 'descent', 'training', 'motivate', 'detail', 'section', 'high', 'learn', 'rate', 'lead', 'drastic', 'parameter', 'overshoot', 'bad', 'initialization', 'require', 'update', 'escape', 'existence', 'local', 'optimum', 'trap', 'train', 'chapter', 'translation', 'error', 'validation', 'train', 'train', 'progress', 'figure', 'train', 'progress', 'time', 'error', 'training', 'set', 'continuously', 'decrease', 'validation', 'set', 'point', 'error', 'increase', 'train', 'stop', 'validation', 'minimum', 'set', 'conclude', 'introduction', 'neural', 'network', 'basic', 'motivate', 'consider', 'figure', 'gradient', 'descent', 'training', 'practical', 'problem', 'set', 'learn', 'rate', 'high', 'lead', 'update', 'overshoot', 'optimum', 'low', 'learn', 'rate', 'lead', 'slow', 'convergence', 'bad', 'initialization', 'weight', 'lead', 'long', 'path', 'update', 'step', 'reach', 'optimum', 'especially', 'problem', 'activation', 'function', 'short', 'interval', 'change', 'existence', 'local', 'optima', 'lead', 'search', 'trap', 'miss', 'global', 'optimum', 'validation', 'set', 'neural', 'network', 'train', 'proceed', 'several', 'full', 'iteration', 'train', 'data', 'track', 'training', 'error', 'training', 'set', 'continuously', 'decrease', 'point', 'set', 'train', 'datum', 'generalized', 'check', 'additional', 'set', 'call', 'validation', 'set', 'train', 'figure', 'illustration', 'measure', 'error', 'validation', 'point', 'point', 'error', 'increase', 'stop', 'minimum', 'validation', 'set', 'reach', 'introduction', 'neural', 'network', 'weight', 'initialization', 'training', 'weight', 'initialize', 'random', 'value', 'value', 'uniform', 'distribution', 'prefer', 'initial', 'weight', 'lead', 'value', 'transition', 'area', 'activation', 'low', 'high', 'shallow', 'slope', 'long', 'time', 'push', 'change', 'activation', 'feeding', 'value', 'range', 'activation', 'function', 'lead', 'activation', 'value', 'range', 'activation', 'commonly', 'formula', 'weight', 'layer', 'network', 'size', 'previous', 'layer', 'hide', 'choose', 'weight', 'range', 'size', 'previous', 'size', 'next', 'layer', 'momentum', 'term', 'consider', 'case', 'weight', 'value', 'far', 'optimum', 'train', 'exam', 'push', 'weight', 'value', 'still', 'small', 'update', 'accumulate', 'weight', 'reach', 'optimum', 'common', 'trick', 'momentum', 'term', 'speed', 'train', 'momentum', 'term', 'update', 'time', 'step', 'train', 'combine', 'previous', 'value', 'momentum', 'term', 'current', 'raw', 'weight', 'update', 'value', 'result', 'momentum', 'term', 'value', 'update', 'weight', 'decay', 'rate', 'update', 'formula', 'change', 'adapt', 'learn', 'rate', 'parameter', 'common', 'training', 'strategy', 'reduce', 'learn', 'rate', 'time', 'begin', 'parameter', 'far', 'away', 'optimal', 'value', 'change', 'later', 'train', 'stage', 'concern', 'large', 'learn', 'rate', 'cause', 'parameter', 'bounce', 'optimum', 'different', 'parameter', 'different', 'stage', 'path', 'optimal', 'different', 'learn', 'rate', 'parameter', 'helpful', 'call', 'record', 'accumulate', 'square', 'value', 'adjust', 'learn', 'rate', 'chapter', 'neural', 'machine', 'translation', 'update', 'formula', 'base', 'sum', 'gradient', 'error', 'respect', 'weight', 'time', 'divide', 'learn', 'rate', 'weight', 'accumulate', 'sum', 'big', 'change', 'parameter', 'value', 'big', 'gradient', 'lead', 'reduction', 'learn', 'rate', 'weight', 'parameter', 'combine', 'idea', 'momentum', 'term', 'adjust', 'parameter', 'update', 'change', 'inspiration', 'adam', 'method', 'transform', 'raw', 'gradient', 'parameter', 'update', 'idea', 'equation', 'idea', 'learn', 'rate', 'raw', 'accumulation', 'risk', 'become', 'large', 'hence', 'permanently', 'depress', 'learn', 'exponential', 'momentum', 'term', 'hyper', 'parameter', 'set', 'typically', 'close', 'mean', 'early', 'training', 'value', 'close', 'initialization', 'value', 'adjust', 'correct', 'bias', 'increase', 'training', 'time', 'step', 'correction', 'piece', 'hand', 'rate', 'momentum', 'accumulate', 'change', 'weight', 'update', 'compute', 'common', 'value', 'hyper', 'parameter', 'various', 'adaptation', 'scheme', 'active', 'area', 'research', 'second', 'order', 'gradient', 'give', 'useful', 'information', 'rate', 'change', 'often', 'expensive', 'shortcut', 'introduction', 'neural', 'network', 'dropout', 'parameter', 'space', 'back', 'propagation', 'learn', 'variant', 'operate', 'light', 'local', 'hill', 'climb', 'stick', 'instead', 'move', 'climb', 'high', 'mountain', 'various', 'method', 'propose', 'train', 'local', 'optimum', 'one', 'currently', 'popular', 'method', 'neural', 'machine', 'translation', 'call', 'drop', 'sound', 'bit', 'simplistic', 'wacky', 'neural', 'network', 'ignore', 'value', 'set', 'associated', 'parameter', 'update', 'choose', 'account', 'much', 'nod', 'training', 'resume', 'number', 'iteration', 'different', 'set', 'drop', 'select', 'play', 'useful', 'role', 'model', 'train', 'point', 'ignore', 'end', 'result', 'robust', 'model', 'several', 'share', 'similar', 'role', 'layer', 'normalization', 'layer', 'normalization', 'address', 'problem', 'arise', 'especially', 'deep', 'neural', 'network', 'proceed', 'large', 'sequence', 'layer', 'train', 'average', 'value', 'layer', 'become', 'feed', 'follow', 'produce', 'large', 'output', 'especially', 'problem', 'activation', 'function', 'limit', 'output', 'narrow', 'linear', 'unit', 'train', 'example', 'average', 'value', 'layer', 'small', 'cause', 'problem', 'train', 'recall', 'equation', 'gradient', 'update', 'strongly', 'effect', 'node', 'value', 'large', 'node', 'value', 'lead', 'explode', 'gradient', 'small', 'value', 'lead', 'diminish', 'gradient', 'remedy', 'idea', 'normalize', 'value', 'layer', 'basis', 'add', 'additional', 'computational', 'step', 'neural', 'network', 'recall', 'feed', 'layer', 'consist', 'matrix', 'multiplication', 'weight', 'matrix', 'value', 'previous', 'layer', 'result', 'weight', 'sum', 'function', 'variance', 'value', 'weight', 'vector', 'translation', 'normalize', 'vector', 'additional', 'bias', 'vector', 'element', 'wise', 'multiplication', 'difference', 'subtract', 'scalar', 'average', 'vector', 'element', 'formula', 'normalize', 'value', 'shift', 'average', 'hence', 'ensure', 'average', 'afterwards', 'result', 'vector', 'divide', 'variance', 'additional', 'bias', 'vector', 'give', 'share', 'multiple', 'layer', 'multiple', 'time', 'step', 'recurrent', 'neural', 'network', 'introduce', 'section', 'page', 'mini', 'batch', 'train', 'example', 'yield', 'set', 'weight', 'update', 'process', 'train', 'example', 'afterwards', 'apply', 'update', 'neural', 'network', 'advantage', 'immediately', 'learn', 'train', 'example', 'training', 'method', 'update', 'model', 'train', 'example', 'call', 'online', 'learn', 'online', 'learn', 'variant', 'gradient', 'descent', 'training', 'call', 'gradient', 'descent', 'online', 'learn', 'generally', 'pass', 'train', 'train', 'constantly', 'change', 'hard', 'process', 'train', 'datum', 'accumulate', 'weight', 'apply', 'collectively', 'small', 'set', 'train', 'example', 'call', 'mini', 'batch', 'distinguish', 'approach', 'batch', 'train', 'entire', 'training', 'set', 'consider', 'batch', 'variation', 'organize', 'process', 'training', 'typically', 'motivate', 'restriction', 'parallel', 'process', 'process', 'training', 'datum', 'mini', 'computation', 'weight', 'update', 'value', 'synchronize', 'summation', 'application', 'weight', 'distribute', 'training', 'number', 'computationally', 'convenient', 'break', 'training', 'datum', 'equally', 'size', 'perform', 'online', 'learn', 'part', 'small', 'mini', 'average', 'weight', 'break', 'training', 'often', 'lead', 'result', 'straightforward', 'linear', 'processing', 'scheme', 'call', 'several', 'train', 'thread', 'immediately', 'update', 'thread', 'still', 'weight', 'value', 'clearly', 'violate', 'safe', 'guard', 'typically', 'parallel', 'hurt', 'practical', 'experience', 'vector', 'matrix', 'operation', 'express', 'calculation', 'handle', 'neural', 'network', 'vector', 'matrix', 'operation', 'computation', 'graph', 'forward', 'activation', 'error', 'error', 'weight', 'execute', 'operation', 'computationally', 'expensive', 'layer', 'matrix', 'operation', 'require', 'multiplication', 'matrix', 'common', 'highly', 'area', 'computer', 'graphic', 'process', 'render', 'image', 'geometric', 'property', 'dimensional', 'object', 'process', 'generate', 'color', 'value', 'dimensional', 'image', 'screen', 'high', 'demand', 'fast', 'graphic', 'instance', 'realistic', 'look', 'computer', 'specialize', 'hardware', 'become', 'graphic', 'process', 'unit', 'processor', 'massive', 'number', 'core', 'provide', 'thread', 'lightweight', 'instruction', 'set', 'provide', 'instruction', 'apply', 'datum', 'point', 'exactly', 'vector', 'space', 'computation', 'list', 'program', 'support', 'various', 'become', 'essential', 'part', 'develop', 'large', 'scale', 'neural', 'network', 'application', 'general', 'term', 'matrix', 'tensor', 'sequence', 'matrix', 'pack', 'dimensional', 'tensor', 'large', 'object', 'actually', 'frequently', 'today', 'neural', 'network', 'read', 'introduction', 'modern', 'neural', 'network', 'research', 'textbook', 'book', 'neural', 'network', 'method', 'apply', 'natural', 'language', 'process', 'general', 'number', 'key', 'technique', 'recently', 'develop', 'enter', 'standard', 'neural', 'machine', 'translation', 'research', 'training', 'robust', 'method', 'drop', 'training', 'interval', 'number', 'avoid', 'explode', 'vanish', 'gradient', 'back', 'propagation', 'several', 'gradient', 'layer', 'normalization', 'similar', 'ensure', 'value', 'reasonable', 'bind', 'active', 'topic', 'research', 'optimization', 'method', 'adjust', 'learn', 'rate', 'gradient', 'descent', 'train', 'popular', 'method', 'currently', 'adam', 'computation', 'graph', 'example', 'neural', 'network', 'section', 'painstakingly', 'work', 'gradient', 'gradient', 'descent', 'train', 'hard', 'chapter', 'layer', 'fee', 'forward', 'neural', 'network', 'computation', 'consist', 'input', 'value', 'weight', 'parameter', 'parameter', 'value', 'show', 'leave', 'input', 'computation', 'show', 'input', 'process', 'graph', 'surprise', 'likely', 'never', 'arbitrarily', 'complex', 'neural', 'network', 'architecture', 'number', 'allow', 'network', 'care', 'rest', 'close', 'look', 'work', 'neural', 'network', 'computation', 'graph', 'different', 'look', 'network', 'build', 'previously', 'represent', 'neural', 'network', 'graph', 'consist', 'connection', 'figure', 'page', 'mathematical', 'equation', 'equation', 'describe', 'forward', 'neural', 'network', 'example', 'represent', 'math', 'form', 'computation', 'graph', 'figure', 'illustration', 'computation', 'graph', 'network', 'graph', 'contain', 'parameter', 'weight', 'matrix', 'bias', 'vector', 'input', 'mathematical', 'operation', 'carry', 'next', 'show', 'value', 'computation', 'graph', 'neural', 'view', 'computation', 'arbitrary', 'connect', 'operation', 'input', 'number', 'parameter', 'operation', 'little', 'inspiration', 'stretch', 'term', 'neural', 'network', 'quite', 'bite', 'graph', 'tree', 'structure', 'direct', 'graph', 'long', 'straightforward', 'process', 'direction', 'cycle', 'way', 'view', 'graph', 'fancy', 'way', 'visualize', 'sequence', 'function', 'call', 'argument', 'previously', 'compute', 'combination', 'recursion', 'loop', 'process', 'input', 'neural', 'network', 'require', 'place', 'input', 'value', 'carry', 'computation', 'show', 'input', 'vector', 'result', 'number', 'look', 'familiar', 'previously', 'work', 'example', 'section', 'move', 'let', 'stock', 'computation', 'node', 'graph', 'accomplish', 'consist', 'function', 'execute', 'computation', 'operation', 'link', 'process', 'compute', 'value', 'add', 'item', 'follow', 'section', 'gradient', 'computation', 'show', 'computation', 'graph', 'process', 'input', 'value', 'examine', 'vastly', 'simply', 'model', 'train', 'model', 'training', 'require', 'error', 'function', 'computation', 'gradient', 'derive', 'update', 'rule', 'parameter', 'quite', 'straightforward', 'compute', 'add', 'computation', 'end', 'computation', 'graph', 'computation', 'compute', 'put', 'value', 'give', 'correct', 'output', 'value', 'train', 'datum', 'produce', 'error', 'value', 'typical', 'error', 'function', 'norm', 'view', 'result', 'execution', 'computation', 'graph', 'error', 'value', 'part', 'update', 'rule', 'parameter', 'look', 'computation', 'model', 'update', 'originate', 'error', 'value', 'propagate', 'model', 'parameter', 'call', 'compute', 'date', 'value', 'oppose', 'forward', 'error', 'calculus', 'chain', 'rule', 'formula', 'compute', 'derivative', 'function', 'chain', 'rule', 'ex', 'press', 'derivative', 'composition', 'function', 'map', 'term', 'derivative', 'product', 'function', 'write', 'explicitly', 'term', 'variable', 'let', 'chapter', 'translation', 'consider', 'chain', 'operation', 'weight', 'matrix', 'error', 'com', 'value', 'hide', 'layer', 'result', 'early', 'computation', 'compute', 'update', 'rule', 'matrix', 'view', 'error', 'function', 'parameter', 'derivative', 'respect', 'case', 'break', 'step', 'chain', 'rule', 'purpose', 'compute', 'update', 'rule', 'treat', 'computation', 'target', 'value', 'bias', 'vector', 'hide', 'value', 'constant', 'break', 'derivative', 'error', 'respect', 'parameter', 'chain', 'derivative', 'computation', 'graph', 'gradient', 'computation', 'derivative', 'computation', 'graph', 'example', 'parameter', 'value', 'backward', 'start', 'error', 'term', 'figure', 'illustration', 'give', 'detail', 'computation', 'gradient', 'start', 'computation', 'figure', 'computation', 'graph', 'gradient', 'compute', 'respect', 'input', 'input', 'gradient', 'text', 'detail', 'computation', 'value', 'chapter', 'translation', 'formula', 'give', 'target', 'output', 'value', 'give', 'train', 'value', 'computation', 'low', 'formula', 'recall', 'formula', 'plug', 'forward', 'pass', 'formula', 'give', 'chain', 'rule', 'give', 'low', 'sum', 'simply', 'copy', 'previous', 'note', 'gradient', 'associate', 'respect', 'output', 'prod', 'parameter', 'derivative', 'value', 'gradient', 'case', 'low', 'prod', 'formula', 'deal', 'scalar', 'value', 'encounter', 'vector', 'value', 'hide', 'chain', 'rule', 'require', 'previously', 'compute', 'scalar', 'sum', 'input', 'hence', 'gradient', 'gradient', 'respect', 'output', 'upper', 'similarly', 'compute', 'computation', 'graph', 'gradient', 'read', 'relevant', 'value', 'weight', 'date', 'gradient', 'trainable', 'parameter', 'weight', 'new', 'value', 'step', 'prod', 'remain', 'computation', 'carry', 'similar', 'form', 'simply', 'layer', 'feed', 'forward', 'neural', 'network', 'example', 'include', 'special', 'output', 'computation', 'multiple', 'time', 'subsequent', 'step', 'computation', 'graph', 'multiple', 'output', 'node', 'fee', 'back', 'gradient', 'propagation', 'add', 'step', 'factor', 'add', 'impact', 'let', 'second', 'look', 'node', 'computation', 'graph', 'function', 'value', 'obtain', 'argument', 'process', 'example', 'forward', 'value', 'execute', 'gradient', 'computation', 'link', 'child', 'obtain', 'downstream', 'gradient', 'process', 'example', 'forward', 'compute', 'program', 'computation', 'graph', 'provide', 'forward', 'backward', 'function', 'value', 'gradient', 'computation', 'instantiate', 'computation', 'connect', 'input', 'aware', 'dimension', 'variable', 'value', 'gradient', 'forward', 'backward', 'variable', 'deep', 'learn', 'framework', 'next', 'encounter', 'various', 'network', 'architecture', 'vector', 'matrix', 'computation', 'obtain', 'weight', 'update', 'formula', 'quite', 'tedious', 'write', 'almost', 'identical', 'code', 'deal', 'variant', 'number', 'framework', 'emerge', 'port', 'develop', 'neural', 'network', 'method', 'choose', 'problem', 'time', 'prominent', 'python', 'library', 'generate', 'compile', 'code', 'torch', 'machine', 'learn', 'library', 'base', 'program', 'python', 'variant', 'chapter', 'neural', 'machine', 'translation', 'implementation', 'natural', 'language', 'processing', 'researcher', 'library', 'recent', 'entry', 'genre', 'framework', 'less', 'gear', 'ready', 'neural', 'network', 'provide', 'implementation', 'vector', 'space', 'operation', 'computation', 'seamless', 'support', 'example', 'section', 'implement', 'python', 'show', 'example', 'framework', 'quite', 'execute', 'follow', 'command', 'python', 'command', 'interface', 'install', 'import', 'map', 'input', 'layer', 'hide', 'layer', 'weight', 'matrix', 'bias', 'vector', 'map', 'function', 'consist', 'linear', 'combination', 'activation', 'function', 'note', 'matrix', 'allow', 'process', 'several', 'training', 'example', 'sequence', 'way', 'term', 'functional', 'programming', 'language', 'symbolically', 'operation', 'actually', 'function', 'method', 'function', 'example', 'value', 'hide', 'number', 'table', 'page', 'map', 'hide', 'layer', 'output', 'layer', 'fashion', 'callable', 'function', 'test', 'full', 'network', 'predict', 'neural', 'language', 'model', 'model', 'require', 'cost', 'function', 'formulate', 'variable', 'correct', 'output', 'overall', 'cost', 'compute', 'average', 'training', 'example', 'cost', 'gradient', 'descent', 'training', 'require', 'computation', 'derivative', 'cost', 'function', 'respect', 'model', 'parameter', 'value', 'weight', 'matrix', 'bias', 'vector', 'great', 'compute', 'derivative', 'follow', 'example', 'function', 'multiple', 'input', 'multiple', 'output', 'train', 'function', 'update', 'model', 'parameter', 'return', 'current', 'prediction', 'cost', 'learn', 'rate', 'train', 'let', 'train', 'datum', 'train', 'function', 'return', 'prediction', 'cost', 'update', 'call', 'train', 'function', 'prediction', 'cost', 'change', 'well', 'loop', 'train', 'function', 'convergence', 'train', 'datum', 'mini', 'batch', 'train', 'mini', 'batch', 'time', 'neural', 'language', 'model', 'neural', 'network', 'powerful', 'method', 'model', 'conditional', 'probability', 'distribution', 'multiple', 'input', 'robust', 'unseen', 'datum', 'point', 'unobserve', 'training', 'datum', 'traditional', 'statistical', 'estimation', 'address', 'sparse', 'datum', 'problem', 'back', 'require', 'insight', 'condition', 'context', 'drop', 'arbitrary', 'choice', 'chapter', 'translation', 'word', 'word', 'word', 'word', 'word', 'figure', 'sketch', 'neural', 'language', 'predict', 'word', 'base', 'precede', 'word', 'gram', 'language', 'model', 'reduce', 'probability', 'sentence', 'product', 'word', 'probability', 'context', 'previous', 'word', 'model', 'prime', 'example', 'conditional', 'probability', 'distribution', 'rich', 'conditioning', 'context', 'often', 'data', 'point', 'cluster', 'information', 'statistical', 'language', 'complex', 'discount', 'back', 'scheme', 'balance', 'rich', 'evidence', 'low', 'order', 'model', 'model', 'estimate', 'high', 'order', 'model', 'turn', 'neural', 'network', 'help', 'fee', 'forward', 'neural', 'language', 'model', 'figure', 'give', 'basic', 'sketch', 'gram', 'neural', 'network', 'language', 'represent', 'context', 'word', 'connection', 'hide', 'connect', 'put', 'layer', 'predict', 'word', 'represent', 'word', 'immediately', 'face', 'network', 'carry', 'real', 'numbered', 'word', 'discrete', 'item', 'large', 'vocabulary', 'simply', 'token', 'neural', 'network', 'assume', 'token', 'similar', 'token', 'practice', 'completely', 'arbitrary', 'argument', 'apply', 'idea', 'bit', 'encode', 'token', 'word', 'similar', 'idea', 'bit', 'vector', 'occasionally', 'appear', 'consider', 'next', 'represent', 'word', 'high', 'dimensional', 'dimension', 'word', 'value', 'dimension', 'match', 'rest', 'type', 'vector', 'call', 'hot', 'vector', 'dog', 'cat', 'eat', 'neural', 'language', 'model', 'word', 'word', 'word', 'word', 'word', 'figure', 'full', 'architecture', 'feed', 'forward', 'neural', 'network', 'language', 'model', 'context', 'word', 'represent', 'hot', 'project', 'continuous', 'space', 'word', 'weight', 'matrix', 'predict', 'word', 'compute', 'hot', 'vector', 'hide', 'layer', 'large', 'continue', 'wrestle', 'impact', 'choice', 'represent', 'word', 'stopgap', 'limit', 'vocabulary', 'pool', 'word', 'token', 'word', 'class', 'automatic', 'cluster', 'linguistically', 'motivate', 'class', 'part', 'speech', 'reduce', 'vector', 'revisit', 'problem', 'large', 'vocabulary', 'late', 'pool', 'evidence', 'introduce', 'layer', 'input', 'layer', 'hide', 'layer', 'context', 'word', 'individually', 'project', 'low', 'space', 'weight', 'matrix', 'context', 'thus', 'generate', 'continuous', 'space', 'representation', 'independent', 'position', 'condition', 'context', 'representation', 'commonly', 'refer', 'word', 'embed', 'word', 'occur', 'similar', 'context', 'similar', 'word', 'train', 'datum', 'language', 'model', 'frequently', 'contain', 'gram', 'cute', 'dog', 'jump', 'cute', 'cat', 'jump', 'child', 'hug', 'cat', 'tightly', 'child', 'hug', 'dog', 'tightly', 'watch', 'cat', 'video', 'watch', 'dog', 'knowledge', 'cat', 'occur', 'similar', 'context', 'hence', 'somewhat', 'interchangeable', 'predict', 'context', 'dog', 'occur', 'context', 'word', 'cat', 'still', 'treat', 'positive', 'evidence', 'word', 'enable', 'generalize', 'word', 'hence', 'robust', 'prediction', 'unseen', 'chapter', 'neural', 'network', 'architecture', 'figure', 'visualization', 'architecture', 'fully', 'fee', 'neural', 'network', 'consist', 'context', 'word', 'hot', 'vector', 'input', 'word', 'embe', 'hidden', 'layer', 'predict', 'output', 'word', 'layer', 'context', 'word', 'encode', 'hot', 'vector', 'embed', 'matrix', 'result', 'vector', 'point', 'word', 'vector', 'typically', 'order', 'embed', 'matrix', 'context', 'word', 'note', 'mathematically', 'much', 'input', 'multiplication', 'matrix', 'hot', 'input', 'value', 'matrix', 'multiplication', 'select', 'column', 'matrix', 'correspond', 'word', 'activation', 'embed', 'matrix', 'lookup', 'table', 'word', 'index', 'word', 'map', 'hidden', 'layer', 'model', 'require', 'concatenation', 'context', 'word', 'input', 'typical', 'feed', 'forward', 'activation', 'function', 'output', 'layer', 'interpret', 'probability', 'distribution', 'word', 'combination', 'weight', 'hide', 'value', 'indeed', 'proper', 'probability', 'activation', 'function', 'ensure', 'value', 'add', 'describe', 'close', 'neural', 'probabilistic', 'language', 'model', 'propose', 'model', 'add', 'direct', 'connection', 'context', 'word', 'output', 'word', 'equation', 'replace', 'paper', 'report', 'direct', 'connection', 'context', 'word', 'output', 'word', 'speed', 'ultimately', 'improve', 'performance', 'counter', 'idea', 'short', 'cut', 'hide', 'layer', 'bite', 'late', 'discus', 'deep', 'model', 'hide', 'layer', 'call', 'residual', 'connection', 'skip', 'connection', 'highway', 'connection', 'neural', 'language', 'model', 'train', 'train', 'parameter', 'neural', 'language', 'process', 'gram', 'feed', 'context', 'word', 'network', 'match', 'network', 'output', 'hot', 'vector', 'correct', 'word', 'predict', 'weight', 'update', 'propagation', 'detail', 'next', 'language', 'model', 'commonly', 'evaluate', 'related', 'probability', 'give', 'proper', 'english', 'text', 'language', 'model', 'proper', 'english', 'language', 'model', 'train', 'objective', 'language', 'model', 'increase', 'likelihood', 'train', 'datum', 'give', 'context', 'correct', 'value', 'hot', 'vector', 'train', 'note', 'value', 'really', 'probability', 'give', 'correct', 'word', 'likelihood', 'way', 'allow', 'update', 'lead', 'wrong', 'output', 'word', 'word', 'embed', 'move', 'worth', 'role', 'word', 'neural', 'machine', 'tran', 'natural', 'language', 'processing', 'task', 'introduce', 'compact', 'encode', 'word', 'relatively', 'high', 'dimensional', 'point', 'natural', 'language', 'time', 'word', 'acquire', 'reputation', 'almost', 'magical', 'quality', 'consider', 'role', 'play', 'neural', 'language', 'language', 'describe', 'represent', 'context', 'word', 'enable', 'prediction', 'next', 'word', 'sequence', 'recall', 'part', 'early', 'cute', 'dog', 'jump', 'cute', 'cat', 'jump', 'occur', 'similar', 'predict', 'word', 'jump', 'similar', 'different', 'word', 'dress', 'unlikely', 'trigger', 'completion', 'jump', 'idea', 'word', 'occur', 'similar', 'context', 'semantically', 'similar', 'powerful', 'idea', 'lexical', 'semantic', 'point', 'researcher', 'love', 'cite', 'word', 'company', 'keep', 'put', 'bit', 'chapter', 'translation', 'figure', 'word', 'project', 'semantically', 'similar', 'word', 'occur', 'close', 'meaning', 'word', 'mean', 'semantic', 'quite', 'concept', 'largely', 'unresolved', 'idea', 'distributional', 'lexical', 'semantic', 'word', 'mean', 'distributional', 'context', 'occur', 'word', 'occur', 'similar', 'context', 'dog', 'cat', 'similar', 'representation', 'vector', 'space', 'word', 'similarity', 'measure', 'distance', 'cosine', 'distance', 'angle', 'vector', 'project', 'high', 'dimensional', 'word', 'visualize', 'word', 'show', 'figure', 'word', 'similar', 'festival', 'cluster', 'together', 'stop', 'semantic', 'representation', 'carry', 'semantic', 'inference', 'queen', 'king', 'queen', 'indeed', 'evidence', 'word', 'allow', 'stop', 'note', 'word', 'crucial', 'tool', 'neural', 'machine', 'translation', 'neural', 'language', 'model', 'inference', 'train', 'train', 'neural', 'language', 'model', 'computationally', 'expensive', 'word', 'train', 'several', 'day', 'modern', 'language', 'model', 'score', 'component', 'statistical', 'machine', 'translation', 'require', 'computation', 'restrict', 'rank', 'list', 'consider', 'method', 'inference', 'training', 'inference', 'actually', 'possible', 'neural', 'language', 'model', 'decoder', 'word', 'actually', 'carry', 'map', 'ping', 'hot', 'vector', 'word', 'store', 'computation', 'hide', 'layer', 'partly', 'carry', 'note', 'word', 'occur', 'slot', 'condition', 'context', 'gram', 'language', 'matrix', 'multiplication', 'word', 'sum', 'hide', 'layer', 'apply', 'activation', 'function', 'value', 'output', 'insanely', 'output', 'node', 'vocabulary', 'item', 'interested', 'score', 'give', 'word', 'produce', 'translation', 'model', 'last', 'point', 'require', 'long', 'discussion', 'value', 'word', 'score', 'language', 'miss', 'important', 'step', 'obtain', 'proper', 'normalize', 'require', 'computation', 'value', 'simply', 'ignore', 'problem', 'score', 'face', 'value', 'likely', 'word', 'give', 'context', 'high', 'score', 'less', 'likely', 'main', 'objective', 'place', 'constraint', 'work', 'model', 'give', 'high', 'score', 'contexts', 'give', 'preference', 'value', 'layer', 'already', 'normalize', 'probability', 'method', 'enforce', 'train', 'let', 'discuss', 'train', 'move', 'method', 'section', 'noise', 'estimation', 'discuss', 'early', 'problem', 'neural', 'language', 'model', 'expensive', 'due', 'normalize', 'output', 'node', 'value', 'function', 'require', 'compute', 'value', 'output', 'interested', 'score', 'particular', 'gram', 'overcome', 'chapter', 'neural', 'translation', 'explicit', 'normalization', 'train', 'model', 'already', 'value', 'normalize', 'way', 'include', 'constraint', 'normalization', 'factor', 'close', 'objective', 'function', 'instead', 'simple', 'likelihood', 'include', 'norm', 'factor', 'note', 'log', 'way', 'train', 'self', 'normalize', 'model', 'estimation', 'main', 'idea', 'optimize', 'model', 'separate', 'correct', 'training', 'example', 'create', 'noise', 'example', 'method', 'less', 'computation', 'require', 'computation', 'output', 'node', 'value', 'learn', 'model', 'distribution', 'give', 'noise', 'case', 'language', 'model', 'model', 'choice', 'generate', 'set', 'noise', 'example', 'addition', 'correct', 'train', 'example', 'set', 'size', 'probability', 'give', 'example', 'predict', 'correct', 'train', 'example', 'correct', 'objective', 'noise', 'estimation', 'maximize', 'correct', 'correct', 'train', 'example', 'minimize', 'noise', 'example', 'log', 'objective', 'function', 'log', 'correct', 'log', 'correct', 'return', 'original', 'goal', 'self', 'normalize', 'note', 'noise', 'normalize', 'model', 'distribution', 'encourage', 'produce', 'value', 'generally', 'overshoot', 'give', 'high', 'value', 'noise', 'example', 'generally', 'undershoot', 'give', 'low', 'value', 'correct', 'translation', 'example', 'train', 'compute', 'value', 'give', 'train', 'noise', 'example', 'compute', 'function', 'give', 'train', 'objective', 'complete', 'computation', 'graph', 'implement', 'standard', 'deep', 'learning', 'gradient', 'descent', 'training', 'immediately', 'obvious', 'optimize', 'classify', 'correct', 'noise', 'example', 'give', 'rise', 'model', 'predict', 'correct', 'probability', 'gram', 'variant', 'method', 'common', 'statistical', 'machine', 'translation', 'tune', 'phase', 'mira', 'infuse', 'relaxation', 'pro', 'rank', 'follow', 'parameter', 'update', 'neural', 'language', 'model', 'word', 'word', 'word', 'word', 'copy', 'value', 'copy', 'value', 'word', 'word', 'figure', 'recurrent', 'neural', 'language', 'predict', 'word', 'context', 'follow', 'word', 'hide', 'layer', 'correct', 'word', 'predict', 'word', 'hide', 'layer', 'prediction', 'prediction', 'word', 'recurrent', 'neural', 'language', 'model', 'forward', 'neural', 'language', 'model', 'describe', 'able', 'long', 'text', 'traditional', 'statistical', 'back', 'mean', 'deal', 'unknown', 'context', 'word', 'similar', 'robust', 'handling', 'unseen', 'word', 'context', 'position', 'possible', 'condition', 'much', 'large', 'contexts', 'traditional', 'statistical', 'model', 'large', 'gram', 'report', 'instead', 'context', 'word', 'recurrent', 'neural', 'network', 'condition', 'context', 'sequence', 'length', 'trick', 'hidden', 'layer', 'predict', 'word', 'additional', 'input', 'predict', 'word', 'figure', 'illustration', 'model', 'look', 'different', 'feed', 'forward', 'neural', 'language', 'model', 'discuss', 'far', 'network', 'word', 'point', 'indicate', 'start', 'sentence', 'word', 'map', 'hidden', 'layer', 'predict', 'output', 'word', 'model', 'architecture', 'word', 'represent', 'hot', 'word', 'hide', 'layer', 'real', 'value', 'activation', 'function', 'hide', 'layer', 'function', 'output', 'layer', 'thing', 'interesting', 'move', 'predict', 'third', 'word', 'sequence', 'input', 'directly', 'precede', 'word', 'network', 'represent', 'sentence', 'value', 'hide', 'layer', 'previous', 'prediction', 'word', 'translation', 'word', 'word', 'word', 'word', 'word', 'word', 'figure', 'propagation', 'unfold', 'recurrent', 'neural', 'network', 'number', 'prediction', 'step', 'derive', 'update', 'formula', 'base', 'train', 'objective', 'predict', 'output', 'word', 'propagation', 'error', 'gradient', 'descent', 'previous', 'sentence', 'context', 'step', 'information', 'new', 'input', 'word', 'hence', 'condition', 'full', 'history', 'sentence', 'last', 'word', 'sentence', 'condition', 'part', 'word', 'sentence', 'model', 'less', 'weight', 'gram', 'feed', 'forward', 'neural', 'language', 'model', 'train', 'model', 'arbitrarily', 'long', 'initial', 'stage', 'second', 'word', 'architecture', 'hence', 'training', 'procedure', 'fee', 'forward', 'neural', 'network', 'assess', 'error', 'output', 'layer', 'propagate', 'update', 'input', 'layer', 'process', 'training', 'example', 'way', 'essentially', 'treat', 'hide', 'layer', 'previous', 'train', 'example', 'current', 'example', 'never', 'provide', 'feedback', 'representation', 'prior', 'history', 'hide', 'layer', 'back', 'propagation', 'time', 'train', 'procedure', 'figure', 'unfold', 'current', 'neural', 'network', 'number', 'back', 'word', 'note', 'limit', 'unfold', 'time', 'network', 'still', 'able', 'learn', 'dependency', 'long', 'distance', 'back', 'propagation', 'time', 'apply', 'training', 'example', 'call', 'time', 'computationally', 'quite', 'expensive', 'time', 'computation', 'carry', 'several', 'step', 'compute', 'apply', 'weight', 'update', 'mini', 'batch', 'section', 'process', 'large', 'number', 'training', 'example', 'entire', 'update', 'weight', 'give', 'modern', 'compute', 'fully', 'unfold', 'recurrent', 'neural', 'network', 'become', 'common', 'recurrent', 'neural', 'network', 'theory', 'give', 'train', 'size', 'actually', 'fully', 'construct', 'com', 'graph', 'give', 'train', 'error', 'sum', 'word', 'prediction', 'carry', 'back', 'propagation', 'entire', 'sentence', 'require', 'quickly', 'build', 'computation', 'graph', 'call', 'dynamic', 'computation', 'graph', 'currently', 'support', 'neural', 'language', 'model', 'long', 'short', 'term', 'memory', 'model', 'consider', 'follow', 'step', 'word', 'prediction', 'sequential', 'language', 'much', 'economic', 'progress', 'country', 'directly', 'precede', 'word', 'country', 'informative', 'prediction', 'word', 'previous', 'word', 'much', 'less', 'relevant', 'importance', 'word', 'decay', 'distance', 'hide', 'state', 'recurrent', 'neural', 'network', 'always', 'update', 'recent', 'memory', 'old', 'word', 'likely', 'diminish', 'time', 'distant', 'word', 'much', 'follow', 'example', 'country', 'much', 'economic', 'progress', 'year', 'still', 'verb', 'depend', 'country', 'separate', 'long', 'subordinate', 'clause', 'recurrent', 'neural', 'network', 'allow', 'model', 'arbitrarily', 'long', 'sequence', 'architecture', 'simple', 'simplicity', 'cause', 'number', 'problem', 'hide', 'layer', 'play', 'double', 'duty', 'memory', 'network', 'continuous', 'space', 'representation', 'predict', 'output', 'word', 'sometimes', 'pay', 'attention', 'directly', 'previous', 'sometimes', 'pay', 'attention', 'long', 'clear', 'mechanism', 'con', 'train', 'model', 'long', 'update', 'propagate', 'begin', 'sentence', 'propagate', 'step', 'raise', 'concern', 'impact', 'recent', 'information', 'step', 'drown', 'old', 'information', 'confusingly', 'name', 'long', 'short', 'term', 'memory', 'neural', 'network', 'address', 'issue', 'design', 'quite', 'practice', 'core', 'distinction', 'basic', 'building', 'block', 'call', 'cell', 'contain', 'explicit', 'memory', 'state', 'memory', 'state', 'cell', 'motivate', 'digital', 'memory', 'cell', 'ordinary', 'computer', 'digital', 'memory', 'cell', 'offer', 'operation', 'reset', 'digital', 'memory', 'cell', 'store', 'single', 'cell', 'store', 'real', 'number', 'operation', 'cell', 'regulate', 'real', 'call', 'gate', 'gradient', 'long', 'distance', 'gradient', 'value', 'become', 'large', 'typically', 'suppress', 'clip', 'limit', 'maximum', 'value', 'set', 'hyper', 'parameter', 'chapter', 'neural', 'translation', 'time', 'precede', 'layer', 'forget', 'gate', 'next', 'layer', 'layer', 'time', 'figure', 'cell', 'neural', 'network', 'recurrent', 'neural', 'receive', 'input', 'layer', 'hide', 'layer', 'value', 'previous', 'time', 'step', 'memory', 'state', 'update', 'input', 'state', 'previous', 'time', 'value', 'memory', 'state', 'various', 'gate', 'channel', 'information', 'cell', 'output', 'value', 'input', 'gate', 'parameter', 'regulate', 'much', 'new', 'input', 'change', 'memory', 'state', 'forget', 'gate', 'parameter', 'regulate', 'much', 'prior', 'memory', 'state', 'retain', 'output', 'gate', 'parameter', 'regulate', 'strongly', 'memory', 'state', 'pass', 'next', 'layer', 'mark', 'output', 'value', 'time', 'step', 'information', 'follow', 'memory', 'gate', 'input', 'input', 'gate', 'forget', 'memory', 'output', 'gate', 'output', 'memory', 'hide', 'value', 'pass', 'next', 'layer', 'application', 'activation', 'function', 'output', 'value', 'output', 'layer', 'consist', 'vector', 'traditional', 'layer', 'consist', 'vector', 'layer', 'compute', 'way', 'input', 'recurrent', 'neural', 'network', 'give', 'value', 'prior', 'layer', 'value', 'hide', 'layer', 'previous', 'time', 'step', 'input', 'value', 'typical', 'combination', 'matrix', 'multiplication', 'weight', 'activation', 'function', 'input', 'neural', 'language', 'model', 'gate', 'parameter', 'actually', 'play', 'fairly', 'important', 'role', 'par', 'give', 'preference', 'recent', 'input', 'input', 'memory', 'forget', 'pay', 'less', 'attention', 'cell', 'current', 'point', 'time', 'output', 'decision', 'inform', 'broad', 'view', 'context', 'value', 'complex', 'condition', 'treat', 'neural', 'network', 'gate', 'input', 'forget', 'output', 'matrix', 'parameter', 'value', 'weight', 'value', 'previous', 'layer', 'hide', 'layer', 'previous', 'time', 'memory', 'state', 'previous', 'time', 'step', 'memory', 'follow', 'activation', 'function', 'gate', 'memory', 'train', 'way', 'recurrent', 'neural', 'back', 'propagation', 'time', 'fully', 'unroll', 'network', 'operation', 'cell', 'complex', 'recurrent', 'neural', 'operation', 'still', 'base', 'matrix', 'differentiable', 'activation', 'function', 'objective', 'function', 'respect', 'parameter', 'model', 'function', 'gate', 'recurrent', 'unit', 'cell', 'add', 'large', 'number', 'additional', 'parameter', 'gate', 'multiple', 'weight', 'matrix', 'add', 'parameter', 'lead', 'long', 'train', 'time', 'risk', 'simple', 'gate', 'recurrent', 'unit', 'propose', 'neural', 'translation', 'model', 'neural', 'machine', 'still', 'commonly', 'figure', 'illustration', 'cell', 'separate', 'memory', 'hide', 'state', 'serf', 'purpose', 'gate', 'gate', 'predict', 'input', 'previous', 'state', 'update', 'update', 'input', 'update', 'state', 'bias', 'update', 'reset', 'reset', 'input', 'reset', 'state', 'bias', 'reset', 'gate', 'combination', 'input', 'previous', 'state', 'combination', 'identical', 'traditional', 'recurrent', 'neural', 'previous', 'state', 'impact', 'scale', 'reset', 'gate', 'gate', 'value', 'give', 'preference', 'current', 'input', 'combination', 'input', 'reset', 'state', 'update', 'gate', 'interpolation', 'previous', 'state', 'compute', 'combination', 'weight', 'update', 'gate', 'balance', 'chapter', 'neural', 'machine', 'translation', 'reset', 'gate', 'precede', 'layer', 'layer', 'layer', 'figure', 'gate', 'recurrent', 'unit', 'long', 'short', 'term', 'memory', 'cell', 'state', 'update', 'state', 'update', 'combination', 'bias', 'extreme', 'update', 'gate', 'previous', 'state', 'pass', 'directly', 'extreme', 'update', 'gate', 'new', 'state', 'mainly', 'determine', 'much', 'impact', 'previous', 'state', 'reset', 'gate', 'allow', 'bit', 'redundant', 'operation', 'gate', 'combine', 'prior', 'state', 'input', 'play', 'different', 'role', 'operation', 'yield', 'combination', 'classic', 'recurrent', 'neural', 'network', 'component', 'allow', 'complex', 'computation', 'combination', 'input', 'output', 'second', 'operation', 'yield', 'new', 'hidden', 'state', 'output', 'unit', 'allow', 'bypass', 'enable', 'long', 'distant', 'memory', 'simply', 'pass', 'information', 'back', 'pass', 'thus', 'enable', 'long', 'distance', 'dependency', 'deep', 'model', 'currently', 'fashionable', 'name', 'deep', 'learn', 'late', 'neural', 'network', 'research', 'real', 'motivation', 'large', 'gain', 'task', 'vision', 'speech', 'due', 'stack', 'multiple', 'hidden', 'layer', 'together', 'layer', 'allow', 'complex', 'sequence', 'traditional', 'computation', 'component', 'allow', 'complex', 'computation', 'multiplication', 'number', 'generally', 'recognize', 'long', 'modern', 'hardware', 'enable', 'train', 'deep', 'neural', 'network', 'real', 'world', 'neural', 'language', 'model', 'input', 'hide', 'layer', 'output', 'input', 'hide', 'layer', 'hide', 'layer', 'hide', 'layer', 'output', 'input', 'hide', 'layer', 'hide', 'layer', 'hide', 'layer', 'output', 'shallow', 'deep', 'stack', 'deep', 'transition', 'figure', 'deep', 'recurrent', 'neural', 'network', 'input', 'pass', 'hide', 'layer', 'output', 'prediction', 'deep', 'stack', 'hide', 'layer', 'connect', 'layer', 'value', 'time', 'depend', 'value', 'time', 'step', 'well', 'previous', 'layer', 'time', 'step', 'deep', 'transitional', 'layer', 'step', 'sequentially', 'connect', 'hidden', 'layer', 'inform', 'last', 'layer', 'learn', 'experiment', 'vision', 'speech', 'dozen', 'layer', 'give', 'increasingly', 'quality', 'idea', 'deep', 'neural', 'network', 'apply', 'sequence', 'prediction', 'task', 'common', 'several', 'option', 'figure', 'give', 'example', 'shallow', 'neural', 'input', 'pass', 'single', 'hidden', 'output', 'predict', 'sequence', 'hide', 'layer', 'hide', 'layer', 'deeply', 'stack', 'layer', 'act', 'hide', 'layer', 'shallow', 'recurrent', 'neural', 'network', 'state', 'condition', 'value', 'previous', 'time', 'step', 'value', 'previous', 'layer', 'sequence', 'layer', 'prediction', 'last', 'layer', 'hide', 'layer', 'directly', 'connect', 'deep', 'transitional', 'hide', 'layer', 'inform', 'last', 'hidden', 'layer', 'previous', 'time', 'step', 'hide', 'layer', 'connect', 'value', 'previous', 'time', 'step', 'layer', 'prediction', 'last', 'layer', 'chapter', 'translation', 'function', 'layer', 'multiplication', 'activation', 'cell', 'cell', 'experiment', 'neural', 'language', 'model', 'traditional', 'statistical', 'machine', 'show', 'hide', 'layer', 'modern', 'hardware', 'allow', 'train', 'deep', 'stretch', 'computational', 'resource', 'practical', 'limit', 'computation', 'neural', 'convergence', 'training', 'typically', 'slow', 'add', 'skip', 'connection', 'input', 'directly', 'output', 'hide', 'sometimes', 'speed', 'still', 'talk', 'several', 'time', 'long', 'train', 'time', 'shallow', 'network', 'read', 'language', 'model', 'prominent', 'reference', 'neural', 'language', 'model', 'implement', 'gram', 'model', 'feed', 'forward', 'neural', 'network', 'history', 'word', 'input', 'predict', 'word', 'output', 'introduce', 'language', 'model', 'translation', 'call', 'space', 'language', 'similar', 'early', 'work', 'speech', 'recognition', 'propose', 'number', 'speed', 'implementation', 'avail', 'able', 'open', 'source', 'toolkit', 'support', 'train', 'graphical', 'processing', 'unit', 'cluster', 'word', 'class', 'encode', 'word', 'pair', 'class', 'word', 'class', 'reduce', 'computational', 'complexity', 'allow', 'integration', 'neural', 'network', 'language', 'model', 'decoder', 'way', 'reduce', 'computational', 'complexity', 'enable', 'decoder', 'integration', 'noise', 'estimation', 'roughly', 'self', 'normalize', 'output', 'score', 'model', 'hence', 'remove', 'compute', 'value', 'possible', 'output', 'word', 'compare', 'technique', 'class', 'base', 'word', 'encode', 'normalize', 'score', 'estimation', 'normalize', 'score', 'show', 'letter', 'give', 'performance', 'much', 'high', 'speed', 'way', 'allow', 'straightforward', 'convert', 'language', 'model', 'short', 'list', 'word', 'traditional', 'gram', 'language', 'model', 'format', 'present', 'method', 'merge', 'continuous', 'space', 'language', 'model', 'traditional', 'gram', 'language', 'advantage', 'well', 'estimate', 'word', 'short', 'list', 'full', 'coverage', 'traditional', 'model', 'finch', 'recurrent', 'neural', 'network', 'language', 'model', 'list', 'system', 'compare', 'fee', 'forward', 'long', 'short', 'term', 'neural', 'network', 'language', 'variant', 'recurrent', 'neural', 'network', 'language', 'show', 'performance', 'latter', 'speech', 'recognition', 'improvement', 'list', 'machine', 'translation', 'system', 'recurrent', 'neural', 'network', 'language', 'model', 'neural', 'language', 'model', 'deep', 'learn', 'model', 'sense', 'hidden', 'layer', 'show', 'hidden', 'layer', 'improve', 'typical', 'layer', 'language', 'model', 'neural', 'machine', 'traditional', 'statistical', 'machine', 'translation', 'model', 'straightforward', 'mechanism', 'integrate', 'additional', 'knowledge', 'large', 'domain', 'language', 'model', 'hard', 'end', 'neural', 'machine', 'translation', 'add', 'language', 'model', 'train', 'additional', 'monolingual', 'datum', 'form', 'recurrently', 'neural', 'translation', 'give', 'word', 'embed', 'hide', 'state', 'predict', 'word', 'big', 'figure', 'sequence', 'sequence', 'encoder', 'decoder', 'extend', 'language', 'big', 'german', 'output', 'sentence', 'box', 'process', 'end', 'sentence', 'entire', 'input', 'sentence', 'neural', 'network', 'parallel', 'language', 'model', 'rank', 'deep', 'integration', 'gate', 'unit', 'regulate', 'relative', 'contribution', 'language', 'model', 'translation', 'model', 'predict', 'word', 'neural', 'translation', 'model', 'prepare', 'look', 'actual', 'translation', 'model', 'already', 'commonly', 'architecture', 'neural', 'machine', 'translation', 'extension', 'neural', 'language', 'model', 'alignment', 'model', 'encoder', 'neural', 'translation', 'model', 'language', 'model', 'recall', 'idea', 'recurrent', 'neural', 'network', 'model', 'language', 'sequential', 'process', 'give', 'previous', 'model', 'predict', 'next', 'word', 'reach', 'end', 'proceed', 'predict', 'translation', 'word', 'time', 'figure', 'illustration', 'train', 'simply', 'concatenate', 'input', 'output', 'sentence', 'method', 'train', 'language', 'model', 'feed', 'input', 'prediction', 'model', 'predict', 'end', 'sentence', 'token', 'network', 'processing', 'reach', 'end', 'input', 'sentence', 'predict', 'end', 'sentence', 'marker', 'hide', 'state', 'encode', 'mean', 'vector', 'hold', 'value', 'hide', 'layer', 'input', 'encoder', 'phase', 'model', 'hide', 'state', 'produce', 'translation', 'decoder', 'phase', 'chapter', 'translation', 'ask', 'hide', 'state', 'recurrent', 'neural', 'network', 'encoder', 'incorporate', 'information', 'input', 'sentence', 'forget', 'word', 'end', 'sentence', 'decoder', 'enough', 'information', 'predict', 'next', 'account', 'part', 'input', 'sentence', 'already', 'still', 'cover', 'propose', 'model', 'work', 'reasonable', 'short', 'sentence', 'fail', 'long', 'sentence', 'minor', 'input', 'hide', 'state', 'decoder', 'phase', 'model', 'decoder', 'structurally', 'different', 'encoder', 'reduce', 'load', 'hide', 'state', 'remember', 'anymore', 'input', 'idea', 'reverse', 'order', 'output', 'last', 'word', 'input', 'sentence', 'close', 'last', 'word', 'output', 'sentence', 'follow', 'improvement', 'explicitly', 'alignment', 'output', 'word', 'input', 'word', 'add', 'alignment', 'model', 'time', 'state', 'art', 'neural', 'machine', 'translation', 'sequence', 'sequence', 'encoder', 'decoder', 'model', 'attention', 'essentially', 'model', 'describe', 'previous', 'explicitly', 'alignment', 'mechanism', 'deep', 'learn', 'alignment', 'call', 'attention', 'word', 'alignment', 'attention', 'interchangeable', 'attention', 'mechanism', 'add', 'bit', 'complexity', 'slowly', 'build', 'look', 'attention', 'mechanism', 'encoder', 'task', 'encoder', 'provide', 'representation', 'input', 'sentence', 'sentence', 'sequence', 'consult', 'embed', 'matrix', 'basic', 'language', 'model', 'describe', 'process', 'word', 'recurrent', 'neural', 'network', 'result', 'hide', 'state', 'encode', 'word', 'leave', 'precede', 'word', 'build', 'recurrent', 'neural', 'network', 'end', 'sentence', 'begin', 'figure', 'illustrate', 'model', 'recurrent', 'neural', 'network', 'direction', 'call', 'bidirectional', 'recurrent', 'neural', 'network', 'encoder', 'input', 'word', 'map', 'step', 'hide', 'state', 'neural', 'translation', 'model', 'input', 'word', 'leave', 'recurrent', 'leave', 'recurrent', 'figure', 'neural', 'machine', 'translation', 'part', 'input', 'encoder', 'consist', 'recurrent', 'neural', 'leave', 'leave', 'recurrent', 'neural', 'encoder', 'state', 'combination', 'hidden', 'state', 'recurrent', 'neural', 'network', 'equation', 'generic', 'function', 'cell', 'recurrent', 'neural', 'net', 'work', 'function', 'typical', 'fee', 'forward', 'neural', 'network', 'layer', 'ax', 'complex', 'gate', 'recurrent', 'unit', 'long', 'short', 'term', 'memory', 'cell', 'original', 'paper', 'approach', 'lately', 'become', 'popular', 'note', 'train', 'model', 'add', 'step', 'predict', 'next', 'word', 'actually', 'train', 'context', 'full', 'machine', 'translation', 'model', 'limit', 'description', 'output', 'sequence', 'word', 'representation', 'hide', 'state', 'decoder', 'decoder', 'recurrent', 'neural', 'network', 'representation', 'input', 'text', 'next', 'section', 'attention', 'previous', 'hidden', 'state', 'output', 'word', 'generate', 'new', 'hidden', 'decoder', 'state', 'new', 'output', 'word', 'prediction', 'figure', 'illustration', 'start', 'recurrent', 'neural', 'network', 'maintain', 'sequence', 'hide', 'state', 'compute', 'previous', 'hidden', 'state', 'embe', 'previous', 'output', 'word', 'input', 'context', 'still', 'several', 'choice', 'function', 'combine', 'input', 'generate', 'next', 'hidden', 'linear', 'transform', 'activation', 'choice', 'match', 'encoder', 'decoder', 'hide', 'state', 'predict', 'output', 'word', 'prediction', 'form', 'probability', 'distribution', 'entire', 'output', 'vocabulary', 'vocabulary', 'prediction', 'dimensional', 'element', 'predict', 'word', 'vocabulary', 'chapter', 'neural', 'translation', 'context', 'state', 'prediction', 'word', 'select', 'word', 'neural', 'machine', 'translation', 'part', 'output', 'decoder', 'give', 'context', 'input', 'embe', 'previously', 'select', 'new', 'decoder', 'state', 'word', 'compute', 'prediction', 'vector', 'condition', 'decoder', 'hide', 'state', 'embe', 'previous', 'output', 'word', 'input', 'context', 'note', 'repeat', 'conditioning', 'hide', 'state', 'separate', 'encoder', 'state', 'progression', 'prediction', 'output', 'word', 'convert', 'raw', 'vector', 'probability', 'sum', 'value', 'high', 'value', 'vector', 'indicate', 'output', 'word', 'word', 'embed', 'inform', 'next', 'time', 'step', 'recurrent', 'neural', 'network', 'correct', 'output', 'word', 'training', 'proceed', 'word', 'training', 'objective', 'give', 'much', 'probability', 'mass', 'possible', 'correct', 'output', 'word', 'cost', 'function', 'drive', 'train', 'hence', 'negative', 'probability', 'give', 'correct', 'word', 'translation', 'cost', 'give', 'correct', 'word', 'probability', 'mean', 'negative', 'log', 'probability', 'typically', 'low', 'hence', 'high', 'cost', 'note', 'cost', 'function', 'tie', 'individual', 'overall', 'sentence', 'cost', 'sum', 'word', 'cost', 'inference', 'new', 'test', 'typically', 'choose', 'word', 'high', 'value', 'embe', 'next', 'step', 'explore', 'beam', 'search', 'next', 'likely', 'word', 'select', 'create', 'different', 'condition', 'context', 'next', 'word', 'later', 'neural', 'translation', 'model', 'encoder', 'state', 'attention', 'input', 'context', 'hide', 'state', 'output', 'word', 'figure', 'neural', 'machine', 'translation', 'part', 'attention', 'compute', 'last', 'hide', 'state', 'decoder', 'word', 'weight', 'encoder', 'state', 'attention', 'mechanism', 'currently', 'loose', 'end', 'decoder', 'give', 'sequence', 'word', 'representation', 'decoder', 'expect', 'context', 'step', 'describe', 'attention', 'mechanism', 'tie', 'end', 'together', 'attention', 'hard', 'visualize', 'typical', 'neural', 'network', 'figure', 'give', 'least', 'idea', 'input', 'output', 'relation', 'attention', 'mechanism', 'inform', 'input', 'word', 'representation', 'previous', 'hidden', 'state', 'decoder', 'produce', 'context', 'state', 'decoder', 'state', 'contain', 'information', 'output', 'sentence', 'input', 'word', 'association', 'word', 'relevant', 'particular', 'input', 'word', 'produce', 'next', 'output', 'weight', 'impact', 'word', 'weight', 'vector', 'bias', 'value', 'output', 'computation', 'scalar', 'indicate', 'important', 'input', 'word', 'produce', 'output', 'word', 'normalize', 'attention', 'attention', 'value', 'input', 'word', 'add', 'normalize', 'attention', 'value', 'weigh', 'contribution', 'input', 'word', 'representation', 'context', 'vector', 'chapter', 'neural', 'machine', 'translation', 'simply', 'add', 'word', 'representation', 'vector', 'odd', 'simplistic', 'thing', 'common', 'practice', 'deep', 'learn', 'natural', 'language', 'processing', 'researcher', 'qualm', 'sentence', 'simply', 'sum', 'word', 'scheme', 'train', 'complete', 'model', 'close', 'look', 'train', 'challenge', 'number', 'step', 'decoder', 'number', 'step', 'encoder', 'vary', 'train', 'consist', 'sentence', 'different', 'computation', 'graph', 'train', 'example', 'instead', 'dynamically', 'create', 'computation', 'graph', 'technique', 'call', 'unroll', 'recurrent', 'neural', 'already', 'discuss', 'regard', 'language', 'model', 'section', 'fully', 'unroll', 'computation', 'graph', 'short', 'sentence', 'figure', 'note', 'couple', 'thing', 'error', 'compute', 'sentence', 'word', 'proceed', 'next', 'word', 'correct', 'word', 'condition', 'context', 'decoder', 'hide', 'state', 'word', 'prediction', 'train', 'objective', 'base', 'probability', 'mass', 'give', 'correct', 'give', 'perfect', 'context', 'attempt', 'different', 'training', 'yet', 'show', 'superior', 'practical', 'training', 'neural', 'machine', 'translation', 'model', 'require', 'suit', 'high', 'degree', 'parallelism', 'inherent', 'deep', 'learn', 'model', 'matrix', 'increase', 'parallelism', 'process', 'several', 'sentence', 'pair', 'imply', 'increase', 'state', 'tensor', 'give', 'example', 'represent', 'input', 'word', 'pair', 'vector', 'already', 'sequence', 'input', 'matrix', 'process', 'batch', 'sentence', 'matrix', 'dimensional', 'tensor', 'give', 'decoder', 'hide', 'state', 'vector', 'output', 'word', 'process', 'batch', 'hide', 'state', 'matrix', 'note', 'case', 'helpful', 'state', 'output', 'state', 'compute', 'sequentially', 'recall', 'computation', 'attention', 'mechanism', 'matrix', 'encoder', 'state', 'dimensional', 'tensor', 'input', 'result', 'matrix', 'attention', 'value', 'sentence', 'dimension', 'input', 'due', 'massive', 'value', 'inherent', 'parallelism', 'show', 'true', 'power', 'feel', 'create', 'glare', 'contradiction', 'argue', 'process', 'training', 'example', 'sentence', 'pair', 'typically', 'different', 'neural', 'translation', 'model', 'big', 'input', 'word', 'leave', 'recurrent', 'leave', 'recurrent', 'attention', 'input', 'context', 'hide', 'state', 'output', 'word', 'prediction', 'error', 'give', 'output', 'word', 'output', 'word', 'fully', 'unrolled', 'computation', 'graph', 'train', 'example', 'input', 'token', 'big', 'output', 'token', 'cost', 'function', 'compute', 'output', 'word', 'sum', 'sentence', 'walk', 'correct', 'previous', 'output', 'word', 'condition', 'context', 'chapter', 'translation', 'figure', 'parallelism', 'process', 'batch', 'training', 'example', 'batch', 'training', 'example', 'set', 'mini', 'batch', 'similar', 'length', 'waste', 'less', 'computation', 'word', 'hence', 'computation', 'graph', 'different', 'size', 'argue', 'sentence', 'pair', 'together', 'exploit', 'parallelism', 'indeed', 'goal', 'figure', 'batch', 'training', 'example', 'consider', 'maxi', 'mum', 'size', 'input', 'output', 'sentence', 'batch', 'unroll', 'computation', 'graph', 'maximum', 'size', 'shorter', 'remain', 'gap', 'non', 'word', 'keep', 'track', 'valid', 'datum', 'mask', 'ensure', 'attention', 'give', 'word', 'length', 'input', 'error', 'gradient', 'update', 'compute', 'output', 'word', 'length', 'output', 'sentence', 'avoid', 'waste', 'computation', 'trick', 'length', 'break', 'mini', 'batch', 'similar', 'length', 'training', 'consist', 'follow', 'step', 'train', 'avoid', 'undue', 'bias', 'due', 'temporal', 'topical', 'batch', 'mini', 'batch', 'process', 'gather', 'gradient', 'apply', 'gradient', 'batch', 'update', 'parameter', 'train', 'neural', 'translation', 'entire', 'training', 'common', 'stop', 'criterion', 'check', 'progress', 'model', 'val', 'set', 'part', 'training', 'halt', 'error', 'validation', 'set', 'improve', 'training', 'long', 'lead', 'improvement', 'degrade', 'performance', 'due', 'beam', 'search', 'translate', 'neural', 'translation', 'model', 'proceed', 'step', 'time', 'predict', 'output', 'word', 'compute', 'probability', 'distribution', 'word', 'bite', 'confusion', 'technical', 'term', 'entire', 'training', 'call', 'contrast', 'batch', 'update', 'online', 'update', 'small', 'batch', 'mini', 'batch', 'section', 'page', 'term', 'batch', 'batch', 'neural', 'translation', 'model', 'cat', 'state', 'prediction', 'word', 'fish', 'select', 'word', 'figure', 'elementary', 'decode', 'model', 'predict', 'word', 'prediction', 'probability', 'distribution', 'select', 'likely', 'word', 'embe', 'part', 'condition', 'context', 'next', 'word', 'prediction', 'decoder', 'pick', 'likely', 'word', 'move', 'next', 'prediction', 'step', 'model', 'condition', 'previous', 'output', 'word', 'equation', 'word', 'embed', 'condition', 'context', 'next', 'step', 'figure', 'illustration', 'time', 'obtain', 'probability', 'distribution', 'word', 'distribution', 'often', 'quite', 'word', 'maybe', 'word', 'amass', 'almost', 'probability', 'word', 'receive', 'high', 'pick', 'output', 'word', 'real', 'example', 'neural', 'machine', 'translation', 'model', 'translate', 'german', 'sentence', 'show', 'figure', 'model', 'tend', 'give', 'almost', 'mass', 'top', 'sentence', 'translation', 'indicate', 'word', 'choice', 'believe', 'different', 'various', 'ambiguity', 'grammatical', 'sentence', 'start', 'discourse', 'connective', 'process', 'suggest', 'perform', 'greedy', 'search', 'vulnerable', 'call', 'garden', 'path', 'problem', 'sometimes', 'follow', 'sequence', 'word', 'realize', 'late', 'mistake', 'early', 'sequence', 'consist', 'less', 'probable', 'word', 'initially', 'redeem', 'subsequent', 'word', 'context', 'full', 'output', 'consider', 'case', 'produce', 'idiomatic', 'phrase', 'word', 'phrase', 'really', 'odd', 'word', 'choice', 'piece', 'cake', 'full', 'phrase', 'choice', 'redeem', 'note', 'face', 'problem', 'traditional', 'statistical', 'machine', 'translation', 'model', 'arguable', 'rely', 'sparse', 'context', 'next', 'word', 'keep', 'list', 'candidate', 'hypothesis', 'expand', 'keep', 'well', 'expand', 'hypothesis', 'neural', 'translation', 'model', 'translation', 'input', 'sentence', 'clever', 'seine', 'art', 'output', 'word', 'prediction', 'alternative', 'however', 'yet', 'believe', 'believe', 'feel', 'clever', 'smart', 'enough', 'around', 'keep', 'maintain', 'hold', 'statement', 'statement', 'testimony', 'message', 'comment', 'vague', 'ambiguous', 'enough', 'interpret', 'interpret', 'differently', 'different', 'various', 'several', 'way', 'way', 'way', 'manner', 'figure', 'word', 'prediction', 'neural', 'translation', 'give', 'top', 'semantically', 'relate', 'word', 'rank', 'believe', 'unit', 'explain', 'section', 'page', 'neural', 'translation', 'model', 'figure', 'beam', 'search', 'neural', 'machine', 'translation', 'commit', 'short', 'list', 'output', 'word', 'beam', 'new', 'word', 'prediction', 'differ', 'commit', 'output', 'word', 'part', 'condition', 'context', 'prediction', 'predict', 'word', 'output', 'keep', 'beam', 'top', 'likely', 'word', 'choice', 'score', 'probability', 'word', 'beam', 'condition', 'context', 'next', 'word', 'due', 'different', 'word', 'prediction', 'score', 'partial', 'translation', 'point', 'probability', 'probability', 'word', 'prediction', 'select', 'high', 'score', 'word', 'pair', 'next', 'beam', 'figure', 'illustration', 'process', 'continue', 'time', 'accumulate', 'word', 'translation', 'give', 'score', 'hypothesis', 'sentence', 'translation', 'end', 'sentence', 'token', 'produce', 'remove', 'complete', 'hypothesis', 'beam', 'reduce', 'beam', 'size', 'search', 'hypothesis', 'leave', 'beam', 'search', 'produce', 'graph', 'show', 'figure', 'start', 'start', 'sentence', 'symbol', 'end', 'sentence', 'symbol', 'give', 'compete', 'result', 'translation', 'obtain', 'follow', 'pointer', 'complete', 'hypothesis', 'end', 'high', 'score', 'point', 'translation', 'choose', 'score', 'product', 'word', 'prediction', 'probability', 'result', 'normalize', 'score', 'output', 'length', 'divide', 'number', 'word', 'carry', 'normalization', 'search', 'complete', 'translation', 'beam', 'normalization', 'difference', 'note', 'traditional', 'statistical', 'machine', 'able', 'combine', 'share', 'condition', 'context', 'future', 'feature', 'function', 'possible', 'anymore', 'recurrent', 'neural', 'network', 'condition', 'entire', 'output', 'word', 'sequence', 'begin', 'search', 'graph', 'generally', 'less', 'diverse', 'search', 'chapter', 'translation', 'figure', 'search', 'graph', 'beam', 'search', 'neural', 'translation', 'model', 'time', 'partial', 'translation', 'select', 'output', 'sentence', 'complete', 'end', 'sentence', 'token', 'predict', 'reduce', 'beam', 'terminate', 'full', 'sentence', 'translation', 'complete', 'follow', 'end', 'sentence', 'token', 'allow', 'read', 'empty', 'box', 'represent', 'hypothesis', 'part', 'complete', 'path', 'graph', 'statistical', 'machine', 'translation', 'model', 'really', 'search', 'tree', 'number', 'complete', 'path', 'size', 'beam', 'read', 'attention', 'model', 'root', 'sequence', 'sequence', 'model', 'recurrent', 'neural', 'network', 'approach', 'short', 'term', 'network', 'reverse', 'order', 'source', 'decode', 'seminal', 'work', 'add', 'alignment', 'model', 'call', 'link', 'generate', 'output', 'word', 'source', 'include', 'condition', 'hide', 'state', 'produce', 'precede', 'target', 'word', 'source', 'word', 'represent', 'hidden', 'state', 'recurrent', 'neural', 'network', 'process', 'source', 'leave', 'leave', 'propose', 'variant', 'attention', 'mechanism', 'call', 'attention', 'hard', 'constraint', 'attention', 'model', 'attention', 'restrict', 'gaussian', 'distribution', 'input', 'word', 'explicitly', 'model', 'trade', 'source', 'context', 'input', 'target', 'context', 'already', 'produce', 'target', 'tu', 'introduce', 'interpolation', 'weight', 'scale', 'impact', 'source', 'context', 'state', 'previous', 'hide', 'state', 'last', 'word', 'predict', 'next', 'hide', 'state', 'decoder', 'augment', 'attention', 'model', 'reconstruction', 'step', 'generate', 'output', 'translate', 'back', 'input', 'language', 'training', 'objective', 'extend', 'include', 'likelihood', 'target', 'likelihood', 'reconstruct', 'input', 'sentence', 'previous', 'section', 'give', 'comprehensive', 'description', 'currently', 'commonly', 'basic', 'neural', 'translation', 'model', 'architecture', 'perform', 'fairly', 'well', 'box', 'ensemble', 'figure', 'method', 'generate', 'alternative', 'system', 'checkpoint', 'model', 'dump', 'various', 'stage', 'train', 'start', 'dent', 'training', 'different', 'initial', 'weight', 'order', 'training', 'datum', 'language', 'pair', 'number', 'propose', 'describe', 'section', 'fairly', 'target', 'particular', 'case', 'datum', 'give', 'perform', 'system', 'recent', 'evaluation', 'campaign', 'ensemble', 'decode', 'pair', 'encode', 'address', 'large', 'add', 'synthetic', 'datum', 'derive', 'monolingual', 'target', 'side', 'datum', 'deep', 'model', 'ensemble', 'decode', 'common', 'technique', 'machine', 'learn', 'build', 'system', 'multiple', 'combine', 'call', 'ensemble', 'system', 'successful', 'strategy', 'various', 'method', 'propose', 'systematically', 'build', 'instance', 'different', 'feature', 'different', 'neural', 'straightforward', 'way', 'different', 'stop', 'different', 'point', 'train', 'process', 'intuitive', 'argument', 'system', 'different', 'mistake', 'system', 'likely', 'mistake', 'general', 'principle', 'play', 'human', 'committee', 'decision', 'democratic', 'voting', 'election', 'apply', 'ensemble', 'method', 'case', 'neural', 'machine', 'address', 'generate', 'alternate', 'combine', 'output', 'generate', 'alternative', 'system', 'figure', 'illustration', 'method', 'generate', 'alter', 'native', 'system', 'train', 'neural', 'translation', 'iterate', 'training', 'datum', 'stop', 'criterion', 'meet', 'typically', 'improvement', 'cost', 'function', 'apply', 'validation', 'set', 'translation', 'performance', 'validation', 'set', 'dump', 'model', 'interval', 'iteration', 'batch', 'training', 'look', 'back', 'performance', 'chapter', 'translation', 'model', 'average', 'cat', 'fish', 'dog', 'figure', 'combine', 'prediction', 'ensemble', 'model', 'independently', 'predict', 'probability', 'distribution', 'output', 'average', 'combine', 'distribution', 'model', 'different', 'stage', 'pick', 'model', 'performance', 'translation', 'quality', 'measure', 'call', 'checkpoint', 'select', 'model', 'different', 'checkpoint', 'train', 'process', 'require', 'build', 'system', 'completely', 'different', 'training', 'mention', 'accomplish', 'different', 'random', 'initialization', 'lead', 'train', 'seek', 'different', 'local', 'optimum', 'randomly', 'train', 'different', 'random', 'order', 'lead', 'different', 'training', 'outcome', 'multi', 'usually', 'work', 'deal', 'computationally', 'much', 'expensive', 'note', 'build', 'checkpoint', 'instead', 'combine', 'end', 'point', 'apply', 'checkpoint', 'combine', 'ensemble', 'combine', 'system', 'output', 'neural', 'translation', 'model', 'allow', 'combination', 'several', 'system', 'fairly', 'deeply', 'recall', 'model', 'predict', 'probability', 'distribution', 'possible', 'output', 'com', 'word', 'combine', 'different', 'train', 'model', 'model', 'predict', 'probability', 'distribution', 'combine', 'prediction', 'combination', 'simple', 'average', 'distribution', 'average', 'distribution', 'basis', 'select', 'output', 'word', 'figure', 'illustration', 'weigh', 'different', 'system', 'way', 'generate', 'similar', 'typically', 'refinement', 'leave', 'decode', 'tweak', 'idea', 'instead', 'build', 'multiple', 'system', 'different', 'random', 'build', 'set', 'system', 'second', 'set', 'system', 'reverse', 'order', 'output', 'sentence', 'second', 'set', 'system', 'call', 'leave', 'arguably', 'name', 'sense', 'language', 'arabic', 'hebrew', 'normal', 'write', 'order', 'leave', 'deep', 'integration', 'describe', 'work', 'anymore', 'combination', 'leave', 'leave', 'produce', 'output', 'different', 'order', 'resort', 'involve', 'several', 'ensemble', 'leave', 'system', 'generate', 'list', 'candidate', 'input', 'score', 'candidate', 'translation', 'individual', 'leave', 'leave', 'combine', 'score', 'different', 'model', 'select', 'candidate', 'score', 'input', 'sentence', 'score', 'give', 'candidate', 'translation', 'leave', 'system', 'require', 'require', 'force', 'decode', 'special', 'mode', 'inference', 'input', 'predict', 'give', 'output', 'sentence', 'mode', 'actually', 'much', 'close', 'training', 'output', 'translation', 'regular', 'inference', 'large', 'vocabulary', 'law', 'tell', 'word', 'language', 'unevenly', 'distribution', 'always', 'large', 'tail', 'rare', 'word', 'new', 'word', 'language', 'time', 'wake', 'deal', 'large', 'inventory', 'include', 'company', 'name', 'microsoft', 'neural', 'method', 'deal', 'large', 'ideal', 'representation', 'neural', 'network', 'continuous', 'space', 'vector', 'discrete', 'object', 'word', 'word', 'ultimately', 'discrete', 'nature', 'word', 'show', 'embed', 'matrix', 'map', 'word', 'output', 'side', 'predict', 'probability', 'distribution', 'output', 'word', 'latter', 'generally', 'big', 'amount', 'computation', 'involve', 'linear', 'size', 'large', 'matrix', 'operation', 'neural', 'translation', 'model', 'typically', 'restrict', 'vocabulary', 'word', 'initial', 'work', 'neural', 'machine', 'frequent', 'word', 'represent', 'unknown', 'tag', 'translation', 'rare', 'word', 'handle', 'back', 'dictionary', 'chapter', 'obama', 'receive', 'relationship', 'obama', 'exactly', 'friendly', 'talk', 'implementation', 'international', 'agreement', 'activity', 'meeting', 'cover', 'palestinian', 'state', 'solution', 'relation', 'obama', 'continuous', 'building', 'peace', 'process', 'relationship', 'deteriorate', 'deal', 'obama', 'negotiate', 'invitation', 'controversial', 'speech', 'partly', 'obama', 'speech', 'agree', 'obama', 'reject', 'meeting', 'reference', 'election', 'time', 'pende', 'figure', 'pair', 'encode', 'applied', 'english', 'word', 'split', 'indicate', 'note', 'datum', 'true', 'case', 'common', 'approach', 'today', 'break', 'rare', 'word', 'unit', 'bit', 'crude', 'actually', 'similar', 'standard', 'approach', 'statistical', 'machine', 'translation', 'handle', 'compound', 'website', 'web', 'site', 'morphology', 'follow', 'convolution', 'convolution', 'decent', 'approach', 'problem', 'transliteration', 'name', 'traditionally', 'handle', 'sub', 'modular', 'letter', 'translation', 'component', 'popular', 'method', 'create', 'inventory', 'unit', 'legitimate', 'word', 'pair', 'encode', 'method', 'train', 'word', 'split', 'character', 'original', 'space', 'special', 'space', 'frequent', 'pair', 'character', 'merge', 'step', 'repeat', 'give', 'number', 'time', 'step', 'increase', 'vocabulary', 'original', 'inventory', 'single', 'character', 'example', 'mirror', 'quite', 'well', 'behavior', 'real', 'world', 'datum', 'set', 'start', 'group', 'together', 'frequent', 'letter', 'combination', 'join', 'frequent', 'word', 'end', 'frequent', 'word', 'emerge', 'single', 'rare', 'word', 'consist', 'still', 'figure', 'unit', 'indicate', 'symbol', 'pair', 'encode', 'vast', 'majority', 'word', 'rarer', 'word', 'break', 'split', 'motivate', 'pende', 'mostly', 'note', 'decomposition', 'relatively', 'rare', 'name', 'reading', 'limitation', 'neural', 'machine', 'translation', 'model', 'burden', 'support', 'large', 'vocabulary', 'avoid', 'typically', 'vocabulary', 'reduce', 'refinement', 'shortlist', 'remain', 'token', 'replace', 'unknown', 'word', 'translate', 'unknown', 'jean', 'resort', 'separate', 'argue', 'neural', 'translation', 'model', 'bad', 'rare', 'word', 'interpolate', 'traditional', 'probabilistic', 'bilingual', 'dictionary', 'prediction', 'neural', 'machine', 'translation', 'model', 'attention', 'mechanism', 'link', 'target', 'word', 'distribution', 'source', 'word', 'weigh', 'word', 'translation', 'accordingly', 'source', 'word', 'name', 'number', 'directly', 'copy', 'target', 'call', 'switch', 'network', 'predict', 'traditional', 'translation', 'operation', 'copying', 'operation', 'aid', 'layer', 'source', 'sentence', 'training', 'datum', 'change', 'target', 'word', 'word', 'position', 'copy', 'source', 'word', 'augment', 'word', 'prediction', 'step', 'neural', 'translation', 'model', 'translate', 'word', 'copy', 'source', 'word', 'observe', 'attention', 'mechanism', 'mostly', 'drive', 'semantic', 'language', 'model', 'case', 'word', 'location', 'case', 'copy', 'speed', 'mi', 'traditional', 'statistical', 'machine', 'translation', 'word', 'phrase', 'translation', 'model', 'target', 'vocabulary', 'mini', 'batch', 'split', 'word', 'sub', 'word', 'character', 'model', 'segmentation', 'base', 'pair', 'encode', 'compression', 'datum', 'key', 'feature', 'statistical', 'machine', 'translation', 'system', 'language', 'train', 'large', 'monolingual', 'datum', 'set', 'large', 'language', 'high', 'translation', 'quality', 'language', 'model', 'train', 'word', 'crawl', 'general', 'web', 'surprise', 'basic', 'neural', 'translation', 'model', 'additional', 'monolingual', 'language', 'model', 'condition', 'previous', 'hidden', 'decoder', 'state', 'previous', 'train', 'jointly', 'translation', 'model', 'main', 'idea', 'propose', 'improve', 'neural', 'translation', 'model', 'transform', 'additional', 'monolingual', 'translation', 'parallel', 'datum', 'syn', 'half', 'integrate', 'language', 'model', 'component', 'neural', 'network', 'architecture', 'translation', 'language', 'model', 'improve', 'output', 'large', 'amount', 'monolingual', 'datum', 'target', 'language', 'give', 'machine', 'evidence', 'common', 'sequence', 'word', 'monolingual', 'target', 'side', 'datum', 'neural', 'translation', 'model', 'source', 'side', 'idea', 'synthesize', 'data', 'translation', 'figure', 'illustration', 'step', 'involve', 'train', 'reverse', 'system', 'translate', 'intend', 'target', 'language', 'source', 'language', 'typically', 'neural', 'machine', 'translation', 'setup', 'source', 'target', 'traditional', 'phrase', 'base', 'system', 'chapter', 'neural', 'machine', 'translation', 'reverse', 'system', 'system', 'figure', 'create', 'synthetic', 'parallel', 'data', 'target', 'side', 'monolingual', 'train', 'system', 'reverse', 'translate', 'target', 'side', 'monolingual', 'datum', 'source', 'combine', 'generate', 'synthetic', 'parallel', 'datum', 'true', 'parallel', 'datum', 'system', 'building', 'reverse', 'system', 'translate', 'target', 'side', 'monolingual', 'create', 'synthetic', 'parallel', 'combine', 'generate', 'synthetic', 'parallel', 'datum', 'true', 'parallel', 'datum', 'building', 'system', 'open', 'question', 'much', 'synthetic', 'parallel', 'datum', 'relation', 'amount', 'exist', 'true', 'parallel', 'datum', 'magnitude', 'monolingual', 'datum', 'drown', 'actual', 'real', 'datum', 'successful', 'idea', 'equal', 'amount', 'synthetic', 'true', 'datum', 'generate', 'much', 'synthetic', 'parallel', 'ensure', 'train', 'process', 'equal', 'amount', 'sample', 'true', 'parallel', 'datum', 'add', 'language', 'model', 'idea', 'train', 'language', 'model', 'separate', 'component', 'neural', 'translation', 'model', 'train', 'large', 'language', 'model', 'recurrent', 'neural', 'net', 'work', 'available', 'include', 'target', 'side', 'parallel', 'corpus', 'add', 'language', 'model', 'neural', 'translation', 'model', 'language', 'model', 'translation', 'model', 'predict', 'output', 'natural', 'point', 'connect', 'model', 'join', 'output', 'prediction', 'network', 'condition', 'expand', 'equation', 'add', 'hide', 'state', 'neural', 'language', 'model', 'hide', 'state', 'neural', 'translation', 'model', 'source', 'context', 'previous', 'english', 'word', 'tm', 'training', 'combine', 'leave', 'parameter', 'large', 'neural', 'language', 'model', 'update', 'parameter', 'translation', 'model', 'layer', 'concern', 'otherwise', 'output', 'side', 'parallel', 'overwrite', 'figure', 'round', 'trip', 'addition', 'train', 'model', 'traditionally', 'parallel', 'optimize', 'model', 'sentence', 'restore', 'monolingual', 'datum', 'add', 'correspond', 'trip', 'start', 'memory', 'large', 'monolingual', 'language', 'model', 'parallel', 'train', 'datum', 'less', 'general', 'question', 'much', 'weight', 'give', 'translation', 'model', 'much', 'weight', 'give', 'language', 'equation', 'consider', 'instance', 'way', 'output', 'word', 'translation', 'model', 'relevant', 'translation', 'content', 'word', 'distinct', 'output', 'word', 'language', 'model', 'relevant', 'introduction', 'relevant', 'function', 'word', 'balance', 'translation', 'model', 'language', 'model', 'achieve', 'type', 'gate', 'unit', 'encounter', 'discussion', 'long', 'short', 'term', 'memory', 'neural', 'network', 'architecture', 'gate', 'unit', 'predict', 'solely', 'model', 'state', 'factor', 'state', 'prediction', 'equation', 'gate', 'gate', 'tm', 'round', 'trip', 'training', 'look', 'idea', 'strict', 'machine', 'learn', 'learn', 'objective', 'objective', 'learn', 'transformation', 'give', 'parallel', 'traditionally', 'goal', 'learn', 'convert', 'output', 'sentence', 'input', 'back', 'output', 'language', 'objective', 'match', 'traditional', 'sentence', 'machine', 'translation', 'model', 'able', 'preserve', 'mean', 'output', 'language', 'sentence', 'map', 'input', 'language', 'back', 'figure', 'illustration', 'machine', 'translation', 'model', 'translate', 'sentence', 'language', 'direction', 'opposite', 'direction', 'system', 'train', 'traditional', 'parallel', 'round', 'trip', 'sentence', 'back', 'objective', 'model', 'train', 'chapter', 'translation', 'give', 'monolingual', 'sentence', 'valid', 'sentence', 'language', 'measure', 'language', 'model', 'reconstruction', 'translation', 'back', 'original', 'language', 'measure', 'translation', 'model', 'objective', 'update', 'model', 'parameter', 'translation', 'typical', 'model', 'update', 'drive', 'correct', 'prediction', 'word', 'trip', 'translation', 'compute', 'usual', 'training', 'model', 'give', 'sentence', 'pair', 'training', 'list', 'translation', 'compute', 'update', 'model', 'monolingual', 'data', 'language', 'scaling', 'date', 'language', 'model', 'cost', 'translation', 'cost', 'translation', 'list', 'monolingual', 'data', 'language', 'training', 'reverse', 'round', 'trip', 'direction', 'detail', 'refer', 'read', 'back', 'translate', 'monolingual', 'datum', 'input', 'obtain', 'synthetic', 'parallel', 'additional', 'training', 'datum', 'monolingual', 'datum', 'dual', 'learning', 'setup', 'machine', 'translation', 'engine', 'train', 'addition', 'regular', 'model', 'training', 'parallel', 'monolingual', 'datum', 'translate', 'round', 'trip', 'evaluate', 'language', 'model', 'language', 'reconstruction', 'match', 'cost', 'function', 'drive', 'gradient', 'descent', 'update', 'model', 'deep', 'research', 'speech', 'recent', 'work', 'machine', 'translation', 'look', 'deep', 'model', 'simply', 'involve', 'add', 'intermediate', 'layer', 'baseline', 'architecture', 'core', 'component', 'neural', 'machine', 'translation', 'encoder', 'input', 'word', 'sequence', 'contextualize', 'representation', 'decoder', 'sequence', 'word', 'recurrent', 'neural', 'network', 'recall', 'already', 'discuss', 'build', 'deep', 'recurrent', 'neural', 'network', 'back', 'section', 'page', 'extend', 'idea', 'recurrent', 'neural', 'network', 'encoder', 'decoder', 'recurrent', 'neural', 'network', 'common', 'process', 'input', 'sequence', 'output', 'time', 'step', 'information', 'new', 'input', 'combine', 'hidden', 'state', 'previous', 'time', 'step', 'predict', 'new', 'hidden', 'state', 'hide', 'state', 'additional', 'prediction', 'word', 'case', 'next', 'word', 'sequence', 'case', 'language', 'hide', 'state', 'otherwise', 'attention', 'mechanism', 'case', 'refinement', 'context', 'decoder', 'stack', 'transition', 'decoder', 'stack', 'transition', 'decoder', 'stack', 'transition', 'decoder', 'stack', 'transition', 'figure', 'deep', 'instead', 'single', 'recurrent', 'neural', 'network', 'layer', 'decoder', 'deep', 'consist', 'several', 'layer', 'illustration', 'show', 'combination', 'deep', 'transition', 'stack', 'omit', 'word', 'word', 'selection', 'output', 'word', 'identical', 'original', 'show', 'figure', 'page', 'decoder', 'figure', 'part', 'decoder', 'neural', 'machine', 'par', 'deep', 'architecture', 'instead', 'single', 'hidden', 'state', 'give', 'time', 'step', 'sequence', 'hide', 'state', 'give', 'time', 'step', 'various', 'option', 'hide', 'state', 'connect', 'present', 'idea', 'stack', 'recurrent', 'neural', 'network', 'hide', 'state', 'condition', 'hide', 'state', 'previous', 'layer', 'hide', 'state', 'depth', 'previous', 'time', 'step', 'deep', 'transition', 'recurrent', 'neural', 'net', 'hide', 'state', 'condition', 'last', 'hide', 'state', 'previous', 'time', 'step', 'hide', 'layer', 'condition', 'previous', 'previous', 'layer', 'figure', 'combine', 'idea', 'layer', 'stack', 'previous', 'time', 'step', 'previous', 'layer', 'deep', 'transition', 'previous', 'layer', 'break', 'stack', 'layer', 'deep', 'transition', 'layer', 'chapter', 'machine', 'translation', 'input', 'word', 'embed', 'encoder', 'layer', 'encoder', 'layer', 'encoder', 'layer', 'encoder', 'layer', 'figure', 'deep', 'alternate', 'combination', 'idea', 'bidirectional', 'recurrent', 'neural', 'network', 'previously', 'propose', 'neural', 'machine', 'translation', 'figure', 'page', 'stack', 'recurrent', 'neural', 'network', 'figure', 'page', 'architecture', 'extend', 'idea', 'deep', 'show', 'decoder', 'figure', 'function', 'sequence', 'function', 'call', 'function', 'implement', 'feed', 'forward', 'neural', 'network', 'layer', 'multiplication', 'activation', 'long', 'short', 'term', 'memory', 'cell', 'gate', 'recurrent', 'unit', 'function', 'set', 'trainable', 'model', 'parameter', 'encoder', 'deep', 'recurrent', 'neural', 'network', 'encoder', 'draw', 'idea', 'baseline', 'neural', 'translation', 'bidirectional', 'recurrent', 'neural', 'network', 'condition', 'leave', 'context', 'deep', 'version', 'encoder', 'figure', 'show', 'idea', 'call', 'alternate', 'recurrent', 'neural', 'network', 'look', 'basically', 'stack', 'recurrent', 'neural', 'hide', 'state', 'layer', 'alternately', 'condition', 'hide', 'state', 'previous', 'time', 'step', 'next', 'time', 'step', 'formulate', 'number', 'hide', 'state', 'condition', 'leave', 'context', 'odd', 'number', 'hide', 'state', 'condition', 'context', 'extend', 'idea', 'deep', 'transition', 'note', 'deep', 'model', 'typically', 'augment', 'direct', 'connection', 'input', 'output', 'case', 'mean', 'direct', 'connection', 'embed', 'encoder', 'connection', 'layer', 'directly', 'die', 'obama', 'figure', 'alignment', 'alignment', 'point', 'traditional', 'word', 'align', 'method', 'show', 'attention', 'box', 'depend', 'alignment', 'value', 'generally', 'match', 'note', 'instance', 'prediction', 'output', 'auxiliary', 'verb', 'pay', 'attention', 'entire', 'verb', 'group', 'strain', 'residual', 'connection', 'help', 'train', 'early', 'deep', 'architecture', 'skip', 'basic', 'function', 'model', 'deep', 'architecture', 'exploit', 'typically', 'residual', 'connection', 'early', 'train', 'stage', 'initial', 'reduction', 'model', 'converge', 'model', 'read', 'recent', 'work', 'show', 'result', 'stack', 'deep', 'transition', 'encoder', 'well', 'alternate', 'network', 'encoder', 'large', 'number', 'variation', 'skip', 'choice', 'number', 'layer', 'still', 'explore', 'empirical', 'various', 'datum', 'condition', 'guide', 'alignment', 'train', 'attention', 'mechanism', 'neural', 'machine', 'translation', 'model', 'motivate', 'align', 'output', 'word', 'input', 'word', 'figure', 'show', 'example', 'attention', 'weight', 'give', 'english', 'input', 'word', 'german', 'output', 'word', 'translation', 'sentence', 'attention', 'value', 'typically', 'match', 'pretty', 'well', 'word', 'alignment', 'statistical', 'machine', 'obtain', 'tool', 'fast', 'align', 'implement', 'variant', 'model', 'several', 'word', 'alignment', 'intrinsic', 'value', 'improve', 'quality', 'translation', 'instance', 'next', 'look', 'attention', 'chapter', 'neural', 'machine', 'translation', 'mechanism', 'explicitly', 'track', 'coverage', 'input', 'override', 'preference', 'neural', 'machine', 'translation', 'translation', 'certain', 'terminology', 'expression', 'measurement', 'well', 'handle', 'rule', 'base', 'require', 'neural', 'model', 'translate', 'source', 'word', 'end', 'user', 'interest', 'alignment', 'computer', 'aid', 'translation', 'tool', 'check', 'output', 'word', 'originate', 'instead', 'trust', 'attention', 'mechanism', 'implicitly', 'acquire', 'role', 'word', 'enforce', 'role', 'idea', 'provide', 'parallel', 'corpus', 'train', 'word', 'alignment', 'traditional', 'mean', 'additional', 'information', 'train', 'model', 'converge', 'fast', 'overcome', 'data', 'sparsity', 'low', 'resource', 'condition', 'straightforward', 'way', 'add', 'give', 'word', 'alignment', 'training', 'process', 'change', 'model', 'modify', 'train', 'objective', 'goal', 'train', 'neural', 'machine', 'translation', 'model', 'generate', 'correct', 'output', 'word', 'add', 'goal', 'match', 'give', 'word', 'alignment', 'assume', 'access', 'alignment', 'matrix', 'alignment', 'point', 'input', 'word', 'output', 'word', 'way', 'output', 'word', 'alignment', 'score', 'add', 'model', 'estimate', 'attention', 'score', 'add', 'output', 'equation', 'page', 'mismatch', 'give', 'alignment', 'score', 'compute', 'attention', 'score', 'measure', 'several', 'cross', 'cost', 'log', 'mean', 'square', 'error', 'cost', 'cost', 'add', 'train', 'objective', 'weight', 'read', 'add', 'supervised', 'word', 'alignment', 'information', 'traditional', 'statistical', 'word', 'alignment', 'training', 'augment', 'objective', 'function', 'optimize', 'match', 'attention', 'mechanism', 'give', 'alignment', 'model', 'coverage', 'impressive', 'aspect', 'neural', 'machine', 'translation', 'model', 'well', 'able', 'late', 'entire', 'input', 'reorder', 'involve', 'aspect', 'occasionally', 'model', 'translate', 'input', 'word', 'multiple', 'sometimes', 'miss', 'problem', 'figure', 'example', 'generation', 'input', 'token', 'social', 'housing', 'attend', 'lead', 'hallucinate', 'output', 'word', 'company', 'end', 'sentence', 'fresh', 'start', 'attend', 'untranslated', 'figure', 'example', 'translation', 'related', 'attention', 'begin', 'phrase', 'alliance', 'receive', 'much', 'result', 'faulty', 'translation', 'hallucinate', 'company', 'society', 'social', 'education', 'input', 'phrase', 'fresh', 'start', 'receive', 'attention', 'hence', 'untranslated', 'output', 'obvious', 'idea', 'strictly', 'model', 'coverage', 'give', 'attention', 'reasonable', 'way', 'coverage', 'add', 'attention', 'state', 'complete', 'sentence', 'roughly', 'expect', 'input', 'word', 'receive', 'similar', 'amount', 'attention', 'input', 'word', 'never', 'receive', 'attention', 'much', 'attention', 'signal', 'problem', 'translation', 'enforce', 'coverage', 'inference', 'restrict', 'enforce', 'proper', 'coverage', 'decoder', 'consider', 'multiple', 'hypothesis', 'beam', 'age', 'pay', 'much', 'attention', 'input', 'word', 'hypothesis', 'penalize', 'pay', 'little', 'attention', 'input', 'various', 'way', 'score', 'function', 'generation', 'generation', 'chapter', 'translation', 'coverage', 'generation', 'min', 'coverage', 'coverage', 'multiple', 'score', 'function', 'decoder', 'common', 'practice', 'traditional', 'machine', 'translation', 'neural', 'translation', 'challenge', 'give', 'proper', 'weight', 'different', 'score', 'function', 'optimize', 'grid', 'search', 'possible', 'value', 'borrow', 'method', 'mira', 'statistical', 'machine', 'translation', 'coverage', 'model', 'vector', 'accumulate', 'coverage', 'input', 'word', 'directly', 'inform', 'attention', 'model', 'attention', 'give', 'input', 'word', 'condition', 'previous', 'state', 'decoder', 'representation', 'input', 'word', 'add', 'condition', 'context', 'accumulate', 'attention', 'give', 'word', 'equation', 'page', 'coverage', 'coverage', 'tracking', 'integrate', 'train', 'objective', 'page', 'guide', 'alignment', 'train', 'previous', 'section', 'augment', 'training', 'function', 'coverage', 'penalty', 'weight', 'coverage', 'note', 'problematic', 'add', 'additional', 'function', 'learn', 'ob', 'distract', 'main', 'goal', 'produce', 'translation', 'fertility', 'describe', 'coverage', 'cover', 'input', 'word', 'roughly', 'evenly', 'early', 'statistical', 'machine', 'translation', 'model', 'consider', 'fertility', 'number', 'output', 'word', 'generate', 'input', 'word', 'consider', 'language', 'require', 'equivalent', 'negate', 'verb', 'word', 'translate', 'multiple', 'output', 'word', 'german', 'translate', 'course', 'thus', 'generate', 'output', 'word', 'augment', 'model', 'coverage', 'add', 'fertility', 'component', 'predict', 'number', 'output', 'word', 'input', 'word', 'example', 'model', 'predict', 'fertility', 'input', 'normalize', 'coverage', 'coverage', 'fertility', 'predict', 'neural', 'network', 'layer', 'condition', 'input', 'word', 'representation', 'activation', 'function', 'result', 'value', 'scale', 'maximum', 'fertility', 'feature', 'engineering', 'machine', 'learn', 'work', 'model', 'coverage', 'machine', 'translation', 'model', 'engineering', 'approach', 'generic', 'machine', 'learn', 'technique', 'engineering', 'way', 'improve', 'system', 'analyze', 'weak', 'point', 'consider', 'change', 'overcome', 'notice', 'generation', 'generation', 'respect', 'add', 'component', 'model', 'overcome', 'problem', 'proper', 'coverage', 'feature', 'translation', 'machine', 'learn', 'able', 'training', 'datum', 'able', 'deep', 'robust', 'estimation', 'way', 'adjustment', 'give', 'amount', 'power', 'problem', 'hard', 'carry', 'analysis', 'generic', 'machine', 'learn', 'give', 'complexity', 'task', 'machine', 'translation', 'argument', 'deep', 'learning', 'require', 'feature', 'add', 'coverage', 'model', 'remain', 'neural', 'machine', 'translation', 'evolve', 'next', 'move', 'engineering', 'machine', 'learn', 'direction', 'read', 'well', 'add', 'coverage', 'state', 'input', 'word', 'sum', 'attention', 'scale', 'fertility', 'value', 'predict', 'input', 'word', 'learn', 'coverage', 'update', 'function', 'feed', 'forward', 'neural', 'network', 'layer', 'coverage', 'state', 'add', 'additional', 'conditioning', 'context', 'prediction', 'attention', 'state', 'condition', 'prediction', 'attention', 'state', 'previous', 'context', 'state', 'introduce', 'coverage', 'state', 'sum', 'source', 'aim', 'subtract', 'cover', 'word', 'step', 'separate', 'hide', 'state', 'keep', 'track', 'source', 'coverage', 'hide', 'state', 'keep', 'track', 'produce', 'output', 'add', 'number', 'bias', 'model', 'alignment', 'inspire', 'traditional', 'statistical', 'machine', 'translation', 'model', 'condition', 'prediction', 'attention', 'state', 'absolute', 'word', 'attention', 'state', 'previous', 'output', 'word', 'limit', 'coverage', 'attention', 'state', 'limit', 'window', 'add', 'fertility', 'model', 'add', 'coverage', 'train', 'objective', 'adaptation', 'text', 'differ', 'degree', 'common', 'problem', 'practical', 'development', 'machine', 'translation', 'system', 'available', 'train', 'datum', 'different', 'datum', 'relevant', 'choose', 'case', 'goal', 'translate', 'chat', 'room', 'realize', 'little', 'translate', 'chat', 'room', 'datum', 'available', 'massive', 'quantity', 'publication', 'international', 'random', 'translation', 'crawl', 'maybe', 'somewhat', 'relevant', 'movie', 'translation', 'chapter', 'translation', 'general', 'training', 'datum', 'initial', 'train', 'general', 'system', 'domain', 'train', 'datum', 'adaptation', 'adapt', 'system', 'figure', 'online', 'train', 'neural', 'machine', 'translation', 'model', 'allow', 'straightforward', 'domain', 'adaptation', 'general', 'domain', 'translation', 'system', 'train', 'general', 'purpose', 'handful', 'additional', 'training', 'epoch', 'domain', 'datum', 'allow', 'domain', 'adapt', 'system', 'problem', 'generally', 'frame', 'problem', 'domain', 'adaptation', 'simple', 'set', 'datum', 'relevant', 'case', 'domain', 'datum', 'set', 'less', 'relevant', 'domain', 'datum', 'traditional', 'statistical', 'machine', 'vast', 'number', 'method', 'domain', 'propose', 'model', 'domain', 'domain', 'sample', 'domain', 'datum', 'train', 'sub', 'sample', 'domain', 'neural', 'machine', 'fairly', 'straightforward', 'method', 'currently', 'pop', 'figure', 'method', 'divide', 'train', 'stage', 'train', 'model', 'available', 'datum', 'convergence', 'iteration', 'train', 'domain', 'datum', 'stop', 'train', 'performance', 'domain', 'validate', 'set', 'peak', 'model', 'training', 'still', 'specialize', 'domain', 'datum', 'practical', 'experience', 'method', 'show', 'second', 'domain', 'training', 'stage', 'converge', 'quickly', 'amount', 'domain', 'datum', 'typically', 'relatively', 'handful', 'train', 'epoch', 'less', 'commonly', 'method', 'idea', 'ensemble', 'decode', 'train', 'separate', 'model', 'different', 'set', 'combine', 'ensemble', 'decode', 'choose', 'weight', 'choose', 'weight', 'trivial', 'task', 'domain', 'domain', 'simply', 'search', 'possible', 'value', 'let', 'look', 'special', 'case', 'arise', 'practical', 'domain', 'datum', 'large', 'collection', 'common', 'problem', 'amount', 'available', 'domain', 'datum', 'train', 'secondary', 'adaptation', 'risk', 'performance', 'datum', 'poor', 'else', 'refinement', 'large', 'random', 'collection', 'parallel', 'text', 'often', 'contain', 'datum', 'closely', 'match', 'domain', 'datum', 'extract', 'domain', 'datum', 'large', 'collection', 'mainly', 'domain', 'datum', 'general', 'idea', 'variety', 'method', 'build', 'domain', 'detector', 'train', 'domain', 'domain', 'detector', 'train', 'domain', 'datum', 'score', 'sentence', 'pair', 'domain', 'datum', 'detector', 'select', 'sentence', 'prefer', 'judge', 'relatively', 'domain', 'detector', 'classic', 'detector', 'language', 'model', 'train', 'source', 'target', 'side', 'domain', 'domain', 'result', 'total', 'language', 'source', 'side', 'domain', 'model', 'target', 'side', 'domain', 'model', 'source', 'side', 'domain', 'model', 'target', 'side', 'domain', 'model', 'give', 'sentence', 'pair', 'domain', 'datum', 'score', 'base', 'relevance', 'traditional', 'gram', 'language', 'model', 'neural', 'recurrent', 'language', 'model', 'work', 'suggest', 'replace', 'open', 'class', 'word', 'part', 'speech', 'tag', 'word', 'sophisticated', 'model', 'consider', 'domain', 'relevance', 'noisiness', 'training', 'datum', 'misalign', 'mistranslate', 'domain', 'domain', 'neural', 'translation', 'model', 'stead', 'source', 'target', 'side', 'sentence', 'isolation', 'datum', 'several', 'way', 'train', 'datum', 'build', 'system', 'secondary', 'adaptation', 'stage', 'outline', 'monolingual', 'domain', 'datum', 'parallel', 'datum', 'domain', 'main', 'idea', 'explore', 'still', 'monolingual', 'source', 'target', 'language', 'parallel', 'datum', 'large', 'pile', 'general', 'outline', 'idea', 'exist', 'parallel', 'datum', 'train', 'domain', 'back', 'translate', 'domain', 'datum', 'section', 'generate', 'synthetic', 'domain', 'datum', 'adapt', 'initial', 'model', 'traditional', 'statistical', 'machine', 'much', 'adaptation', 'success', 'achieve', 'interpolate', 'language', 'idea', 'neural', 'translation', 'equivalent', 'multiple', 'domain', 'multiple', 'collection', 'datum', 'clearly', 'domain', 'typically', 'category', 'information', 'technique', 'describe', 'build', 'specialized', 'translation', 'model', 'domain', 'give', 'test', 'select', 'appropriate', 'model', 'domain', 'test', 'build', 'allow', 'automatically', 'chapter', 'translation', 'determination', 'base', 'method', 'domain', 'detector', 'describe', 'give', 'decision', 'select', 'appropriate', 'model', 'commit', 'single', 'domain', 'instead', 'provide', 'distribution', 'relevance', 'domain', 'model', 'domain', 'domain', 'domain', 'weight', 'ensemble', 'model', 'domain', 'base', 'whole', 'document', 'instead', 'individual', 'bring', 'context', 'robust', 'decision', 'hard', 'give', 'conclusive', 'advice', 'handle', 'adaptation', 'broad', 'topic', 'style', 'text', 'relevant', 'content', 'datum', 'differ', 'narrowly', 'publication', 'chat', 'room', 'publish', 'amount', 'domain', 'domain', 'data', 'differ', 'datum', 'cleanly', 'separate', 'domain', 'massive', 'disorganize', 'pile', 'datum', 'high', 'translation', 'quality', 'pollute', 'noise', 'generate', 'machine', 'translation', 'system', 'reading', 'often', 'domain', 'mismatch', 'train', 'translation', 'test', 'datum', 'deployment', 'rich', 'literature', 'traditional', 'statistical', 'machine', 'translation', 'topic', 'common', 'approach', 'neural', 'model', 'train', 'available', 'training', 'iteration', 'domain', 'datum', 'already', 'pioneer', 'neural', 'language', 'model', 'adaption', 'demonstrate', 'effectiveness', 'adaptation', 'method', 'small', 'domain', 'set', 'consist', 'little', 'sentence', 'pair', 'give', 'small', 'amount', 'domain', 'datum', 'lead', 'suggest', 'mix', 'domain', 'domain', 'datum', 'adaption', 'identify', 'problem', 'suggest', 'ensemble', 'baseline', 'model', 'adapt', 'model', 'avoid', 'consider', 'alternative', 'training', 'method', 'adaptation', 'phase', 'consistently', 'well', 'result', 'traditional', 'gradient', 'descent', 'training', 'inspire', 'domain', 'adaptation', 'work', 'statistical', 'machine', 'translation', 'build', 'domain', 'domain', 'sentence', 'pair', 'training', 'prediction', 'score', 'reduce', 'learn', 'rate', 'sentence', 'pair', 'domain', 'show', 'traditional', 'statistical', 'machine', 'translation', 'outperform', 'neural', 'chine', 'translation', 'train', 'general', 'purpose', 'machine', 'translation', 'system', 'collection', 'test', 'niche', 'domain', 'adaptation', 'technique', 'allow', 'neural', 'machine', 'translation', 'catch', 'multi', 'domain', 'model', 'train', 'inform', 'time', 'domain', 'input', 'sentence', 'apply', 'idea', 'initially', 'propose', 'augment', 'input', 'sentence', 'register', 'politeness', 'feature', 'token', 'domain', 'adaptation', 'problem', 'add', 'domain', 'token', 'training', 'test', 'well', 'result', 'token', 'approach', 'topic', 'give', 'topic', 'membership', 'sentence', 'additional', 'input', 'vector', 'condition', 'context', 'word', 'prediction', 'layer', 'refinement', 'add', 'linguistic', 'annotation', 'big', 'debate', 'machine', 'question', 'key', 'progress', 'develop', 'relatively', 'machine', 'learn', 'method', 'implicitly', 'learn', 'important', 'feature', 'linguistic', 'insight', 'augment', 'datum', 'model', 'recent', 'work', 'statistical', 'machine', 'translation', 'demonstrate', 'motivated', 'model', 'statistical', 'machine', 'translation', 'system', 'major', 'evaluation', 'campaign', 'language', 'pair', 'syntax', 'base', 'translate', 'build', 'syntactic', 'structure', 'output', 'sentence', 'serious', 'effort', 'move', 'deeply', 'semantic', 'machine', 'translation', 'turn', 'neural', 'machine', 'translation', 'hard', 'swing', 'back', 'machine', 'learn', 'ignore', 'much', 'linguistic', 'insight', 'neural', 'translation', 'view', 'translation', 'generic', 'sequence', 'sequence', 'happen', 'involve', 'sequence', 'word', 'different', 'language', 'method', 'pair', 'encode', 'character', 'base', 'model', 'put', 'value', 'concept', 'word', 'basic', 'unit', 'doubt', 'recently', 'attempt', 'add', 'linguistic', 'annotation', 'neural', 'translation', 'step', 'linguistically', 'motivate', 'model', 'look', 'successful', 'effort', 'integrate', 'linguistic', 'annotation', 'input', 'linguistic', 'annotation', 'output', 'build', 'linguistically', 'structure', 'model', 'linguistic', 'annotation', 'input', 'great', 'neural', 'network', 'cope', 'rich', 'context', 'neural', 'machine', 'translation', 'model', 'word', 'prediction', 'condition', 'entire', 'input', 'sentence', 'previously', 'generate', 'put', 'word', 'typically', 'input', 'sequence', 'partially', 'generate', 'output', 'sequence', 'never', 'observe', 'neural', 'model', 'able', 'generalize', 'training', 'datum', 'draw', 'relevant', 'knowledge', 'traditional', 'statistical', 'mod', 'require', 'carefully', 'choose', 'independence', 'assumption', 'back', 'scheme', 'add', 'information', 'condition', 'context', 'neural', 'translation', 'model', 'accommodate', 'information', 'typical', 'linguistic', 'treasure', 'chest', 'contain', 'part', 'speech', 'morphological', 'property', 'syntactic', 'phrase', 'syntactic', 'maybe', 'semantic', 'annotation', 'format', 'annotation', 'individual', 'input', 'word', 'require', 'bit', 'syntactic', 'semantic', 'annotation', 'span', 'multiple', 'word', 'figure', 'example', 'walk', 'linguistic', 'annotation', 'word', 'girl', 'part', 'girl', 'surface', 'form', 'differ', 'watch', 'chapter', 'translation', 'word', 'girl', 'watch', 'attentively', 'beautiful', 'part', 'speech', 'adv', 'girl', 'watch', 'attentive', 'beautiful', 'morphology', 'sing', 'past', 'plural', 'phrase', 'begin', 'begin', 'phrase', 'begin', 'dependency', 'girl', 'watch', 'watch', 'depend', 'relation', 'adv', 'semantic', 'role', 'actor', 'semantic', 'type', 'human', 'view', 'animate', 'figure', 'linguistic', 'annotation', 'format', 'word', 'level', 'factor', 'representation', 'word', 'phrase', 'start', 'word', 'part', 'phrase', 'syntactic', 'head', 'watch', 'dependency', 'relationship', 'head', 'semantic', 'role', 'actor', 'scheme', 'semantic', 'type', 'instance', 'girl', 'man', 'note', 'phrasal', 'annotation', 'handle', 'phrase', 'girl', 'common', 'annotation', 'scheme', 'tag', 'individual', 'word', 'phrasal', 'begin', 'continuation', 'intermediate', 'word', 'phrase', 'encode', 'word', 'level', 'factor', 'recall', 'word', 'initially', 'represent', 'hot', 'vector', 'encode', 'factor', 'factor', 'representation', 'hot', 'vector', 'concatenation', 'vector', 'input', 'word', 'mathematically', 'factor', 'representation', 'factor', 'input', 'neural', 'machine', 'translation', 'system', 'still', 'sequence', 'word', 'change', 'architecture', 'neural', 'machine', 'translation', 'model', 'provide', 'rich', 'input', 'representation', 'hope', 'model', 'able', 'learn', 'advantage', 'debate', 'linguistic', 'machine', 'learn', 'linguistic', 'notation', 'propose', 'arguable', 'learn', 'automatically', 'part', 'word', 'contextualize', 'word', 'hide', 'encoder', 'true', 'provide', 'additional', 'knowledge', 'tool', 'produce', 'notation', 'particularly', 'relevant', 'enough', 'training', 'datum', 'automatically', 'induce', 'job', 'hard', 'machine', 'learn', 'force', 'machine', 'learn', 'discover', 'feature', 'readily', 'refinement', 'sentence', 'girl', 'watch', 'attentively', 'beautiful', 'syntax', 'tree', 'watch', 'adv', 'attentively', 'beautiful', 'girl', 'watch', 'adv', 'attentively', 'beautiful', 'figure', 'phrase', 'structure', 'tree', 'word', 'watch', 'tag', 'question', 'resolve', 'empirically', 'demonstrate', 'actually', 'work', 'datum', 'condition', 'linguistic', 'annotation', 'output', 'input', 'word', 'output', 'word', 'instead', 'discuss', 'point', 'adjustment', 'separate', 'output', 'let', 'look', 'annotation', 'scheme', 'output', 'successfully', 'apply', 'neural', 'machine', 'translation', 'syntax', 'base', 'statistical', 'machine', 'translation', 'model', 'focus', 'add', 'syntax', 'output', 'side', 'traditional', 'gram', 'language', 'model', 'promote', 'neighbor', 'powerful', 'enough', 'ensure', 'overall', 'output', 'sentence', 'design', 'model', 'produce', 'evaluate', 'syntactic', 'parse', 'output', 'syntax', 'base', 'model', 'give', 'mean', 'promote', 'grammatically', 'correct', 'output', 'word', 'level', 'annotation', 'phrase', 'structure', 'syntax', 'suggest', 'figure', 'crude', 'nature', 'language', 'annotate', 'nest', 'phrase', 'handle', 'begin', 'scheme', 'typically', 'tree', 'structure', 'represent', 'syntax', 'figure', 'example', 'show', 'phrase', 'structure', 'syntactic', 'parse', 'tree', 'girl', 'watch', 'attentively', 'beautiful', 'generate', 'tree', 'structure', 'generally', 'quite', 'different', 'process', 'generate', 'sequence', 'typically', 'build', 'recursively', 'bottom', 'algorithm', 'chart', 'parse', 'parse', 'tree', 'sequence', 'word', 'structural', 'token', 'indicate', 'begin', 'end', 'closing', 'parenthesis', 'syntactic', 'phrase', 'force', 'syntactic', 'parse', 'tree', 'annotation', 'sequence', 'sequence', 'neural', 'chine', 'translation', 'model', 'encode', 'parse', 'structure', 'additional', 'output', 'token', 'perfectly', 'idea', 'produce', 'output', 'neural', 'translation', 'sequence', 'sequence', 'mix', 'output', 'word', 'special', 'translation', 'model', 'produce', 'syntactic', 'structure', 'encourage', 'produce', 'syntactically', 'well', 'form', 'output', 'evidence', 'support', 'simplicity', 'approach', 'linguistically', 'structure', 'model', 'syntactic', 'parsing', 'leave', 'untouched', 'recent', 'wave', 'neural', 'network', 'previous', 'section', 'suggest', 'syntactic', 'parse', 'simply', 'frame', 'sequence', 'sequence', 'additional', 'output', 'token', 'perform', 'syntactic', 'model', 'structure', 'recur', 'nature', 'language', 'heart', 'inspire', 'network', 'build', 'tree', 'version', 'leave', 'push', 'main', 'stack', 'open', 'phrase', 'new', 'word', 'extend', 'push', 'stack', 'start', 'new', 'phrase', 'early', 'work', 'integrate', 'syntactic', 'parse', 'machine', 'translation', 'practice', 'emerge', 'yet', 'time', 'clearly', 'still', 'challenge', 'future', 'work', 'read', 'factor', 'word', 'part', 'factor', 'encode', 'hot', 'input', 'recurrent', 'neural', 'network', 'language', 'model', 'representation', 'input', 'output', 'neural', 'machine', 'translation', 'demonstrate', 'translation', 'quality', 'multiple', 'language', 'pair', 'language', 'world', 'train', 'datum', 'language', 'sometimes', 'highly', 'overlap', 'proceeding', 'sometimes', 'unique', 'canadian', 'train', 'datum', 'available', 'language', 'include', 'commercially', 'interesting', 'language', 'pair', 'long', 'history', 'move', 'language', 'encode', 'language', 'sometimes', 'call', 'machine', 'idea', 'map', 'input', 'language', 'map', 'output', 'language', 'build', 'map', 'step', 'step', 'language', 'translate', 'language', 'researcher', 'deep', 'learn', 'often', 'hesitate', 'claim', 'intermediate', 'state', 'neural', 'translation', 'model', 'encode', 'mean', 'train', 'neural', 'machine', 'translation', 'system', 'accept', 'text', 'language', 'input', 'translate', 'refinement', 'multiple', 'input', 'language', 'let', 'parallel', 'train', 'neural', 'machine', 'translation', 'simply', 'concatenate', 'input', 'vocabulary', 'contain', 'german', 'french', 'word', 'input', 'sentence', 'quickly', 'recognize', 'german', 'due', 'sentence', 'word', 'combine', 'model', 'datum', 'set', 'advantage', 'separate', 'mod', 'expose', 'english', 'side', 'parallel', 'hence', 'learn', 'well', 'language', 'model', 'lead', 'robust', 'model', 'multiple', 'output', 'language', 'trick', 'give', 'french', 'input', 'sentence', 'system', 'output', 'language', 'crude', 'effective', 'way', 'signal', 'model', 'add', 'tag', 'spanish', 'token', 'input', 'sentence', 'case', 'double', 'spanish', 'verse', 'con', 'train', 'system', 'mention', 'translate', 'sentence', 'german', 'spanish', 'ever', 'present', 'sentence', 'train', 'datum', 'system', 'spanish', 'verse', 'con', 'representation', 'input', 'sentence', 'tie', 'input', 'language', 'output', 'language', 'experiment', 'show', 'actually', 'somewhat', 'achieve', 'parallel', 'data', 'desire', 'language', 'pair', 'much', 'less', 'standalone', 'model', 'figure', 'summarize', 'idea', 'single', 'neural', 'machine', 'translation', 'train', 'result', 'system', 'translate', 'input', 'output', 'language', 'likely', 'increasingly', 'deep', 'model', 'section', 'well', 'serve', 'multi', 'language', 'deep', 'layer', 'language', 'idea', 'mark', 'token', 'spanish', 'explore', 'widely', 'context', 'system', 'single', 'language', 'token', 'represent', 'domain', 'input', 'sentence', 'require', 'level', 'politeness', 'output', 'sentence', 'chapter', 'translation', 'french', 'spanish', 'figure', 'language', 'translation', 'system', 'train', 'language', 'pair', 'rotate', 'train', 'able', 'translate', 'german', 'spanish', 'share', 'component', 'instead', 'throw', 'datum', 'generic', 'neural', 'machine', 'translation', 'carefully', 'consider', 'component', 'share', 'model', 'idea', 'train', 'model', 'language', 'component', 'identical', 'unique', 'model', 'encoder', 'share', 'model', 'decoder', 'share', 'model', 'output', 'language', 'attention', 'mechanism', 'share', 'share', 'component', 'mean', 'parameter', 'value', 'separate', 'model', 'update', 'train', 'model', 'language', 'pair', 'change', 'pair', 'mark', 'output', 'pair', 'idea', 'share', 'training', 'component', 'push', 'encoder', 'train', 'monolingual', 'input', 'language', 'add', 'train', 'objective', 'language', 'model', 'decoder', 'train', 'isolation', 'monolingual', 'language', 'model', 'datum', 'context', 'state', 'blank', 'lead', 'learn', 'ignore', 'input', 'sentence', 'function', 'target', 'side', 'language', 'model', 'read', 'explore', 'well', 'single', 'canonical', 'neural', 'translation', 'model', 'able', 'learn', 'multiple', 'multiple', 'simultaneously', 'train', 'parallel', 'several', 'language', 'pair', 'show', 'small', 'several', 'input', 'language', 'output', 'mixed', 'result', 'translate', 'multiple', 'output', 'language', 'additional', 'input', 'language', 'interesting', 'result', 'ability', 'model', 'translate', 'language', 'direction', 'parallel', 'thus', 'demonstrate', 'mean', 'representation', 'less', 'well', 'traditional', 'pivot', 'method', 'support', 'language', 'input', 'output', 'train', 'encoder', 'decoder', 'share', 'attention', 'mechanism', 'alternate', 'architecture', 'input', 'word', 'layer', 'layer', 'layer', 'figure', 'encode', 'sentence', 'neural', 'network', 'always', 'size', 'convolution', 'differ', 'decode', 'rever', 'process', 'alternate', 'architecture', 'neural', 'network', 'research', 'focus', 'recurrent', 'neural', 'network', 'attention', 'mean', 'architecture', 'neural', 'network', 'vantage', 'recurrent', 'neural', 'network', 'input', 'side', 'require', 'long', 'sequential', 'process', 'consume', 'input', 'word', 'step', 'prohibit', 'ability', 'processing', 'word', 'thus', 'limit', 'capability', 'alternate', 'suggestion', 'architecture', 'neural', 'machine', 'model', 'present', 'section', 'remain', 'curiosity', 'neural', 'network', 'end', 'neural', 'machine', 'translation', 'model', 'modern', 'era', 'actually', 'base', 'recurrent', 'neural', 'base', 'neural', 'network', 'show', 'successful', 'image', 'thus', 'look', 'application', 'natural', 'next', 'step', 'figure', 'illustration', 'network', 'encode', 'basic', 'building', 'block', 'network', 'convolution', 'merge', 'representation', 'input', 'word', 'single', 'representation', 'matrix', 'apply', 'convolution', 'sequence', 'input', 'word', 'reduce', 'length', 'sentence', 'representation', 'repeat', 'process', 'lead', 'sentence', 'representation', 'single', 'vector', 'illustration', 'show', 'architecture', 'follow', 'layer', 'sequence', 'representation', 'single', 'sentence', 'size', 'depend', 'show', 'word', 'sentence', 'sequence', 'layer', 'long', 'big', 'kernel', 'hierarchical', 'process', 'build', 'sentence', 'bottom', 'well', 'ground', 'linguistic', 'insight', 'recursive', 'nature', 'language', 'similar', 'chart', 'commit', 'single', 'hierarchical', 'structure', 'ask', 'chapter', 'translation', 'input', 'word', 'encode', 'layer', 'encode', 'layer', 'transfer', 'layer', 'select', 'word', 'output', 'word', 'neural', 'network', 'model', 'convolution', 'result', 'single', 'sentence', 'sequence', 'encoder', 'inform', 'recurrent', 'neural', 'network', 'output', 'word', 'awful', 'result', 'sentence', 'represent', 'entire', 'sen', 'arbitrary', 'length', 'generate', 'output', 'sentence', 'translation', 'reverse', 'bottom', 'process', 'problem', 'decoder', 'decide', 'length', 'output', 'sentence', 'option', 'address', 'problem', 'add', 'model', 'predict', 'output', 'length', 'length', 'lead', 'selection', 'size', 'reverse', 'convolution', 'matrix', 'figure', 'illustration', 'idea', 'show', 'architecture', 'always', 'result', 'sequence', 'single', 'sentence', 'embe', 'explicit', 'phrasal', 'representation', 'input', 'word', 'phrasal', 'representation', 'output', 'call', 'transfer', 'layer', 'decoder', 'model', 'include', 'recurrent', 'neural', 'network', 'output', 'side', 'sneak', 'recurrent', 'neural', 'network', 'undermine', 'bit', 'argument', 'claim', 'still', 'hold', 'true', 'encode', 'sequential', 'language', 'model', 'powerful', 'tool', 'disregard', 'describe', 'neural', 'machine', 'translation', 'model', 'help', 'set', 'scene', 'neural', 'network', 'demonstrate', 'achieve', 'competitive', 'result', 'compare', 'traditional', 'approach', 'compression', 'representation', 'single', 'vector', 'especially', 'problem', 'long', 'sentence', 'model', 'successfully', 'candidate', 'translation', 'generate', 'traditional', 'statistical', 'machine', 'translation', 'system', 'alternate', 'architecture', 'input', 'word', 'convolution', 'layer', 'convolution', 'layer', 'convolution', 'layer', 'figure', 'encoder', 'stack', 'layer', 'number', 'layer', 'neural', 'network', 'attention', 'propose', 'architecture', 'neural', 'network', 'combine', 'idea', 'neural', 'network', 'attention', 'mechanism', 'essentially', 'sequence', 'sequence', 'attention', 'describe', 'canonical', 'neural', 'machine', 'translation', 'recurrent', 'neural', 'network', 'replace', 'layer', 'introduce', 'convolution', 'previous', 'section', 'idea', 'combine', 'short', 'sequence', 'neighbor', 'word', 'single', 'representation', 'look', 'convolution', 'encode', 'word', 'leave', 'limit', 'window', 'let', 'describe', 'detail', 'mean', 'encoder', 'neural', 'model', 'encoder', 'figure', 'illustration', 'layer', 'encoder', 'input', 'state', 'layer', 'inform', 'neighbor', 'note', 'layer', 'shorten', 'convolution', 'center', 'pad', 'word', 'position', 'bind', 'start', 'input', 'word', 'ex', 'progress', 'sequence', 'layer', 'different', 'depth', 'maximum', 'depth', 'function', 'feed', 'forward', 'residual', 'connection', 'correspond', 'previous', 'layer', 'state', 'note', 'representation', 'word', 'inform', 'partial', 'sentence', 'context', 'contrast', 'directional', 'recurrent', 'neural', 'network', 'canonical', 'model', 'relevant', 'context', 'word', 'input', 'sentence', 'help', 'disambiguation', 'window', 'computational', 'advantage', 'idea', 'word', 'depth', 'process', 'combine', 'massive', 'tensor', 'operation', 'translation', 'input', 'context', 'output', 'word', 'prediction', 'decoder', 'convolution', 'decoder', 'convolution', 'output', 'word', 'embe', 'select', 'word', 'figure', 'decoder', 'neural', 'network', 'attention', 'decoder', 'state', 'compute', 'sequence', 'layer', 'already', 'predict', 'output', 'word', 'state', 'inform', 'input', 'context', 'compute', 'sentence', 'attention', 'decoder', 'decoder', 'canonical', 'model', 'core', 'recurrent', 'neural', 'network', 'recall', 'state', 'progression', 'equation', 'page', 'encoder', 'embed', 'previous', 'output', 'input', 'context', 'version', 'recurrent', 'decoder', 'depend', 'previous', 'state', 'condition', 'sequence', 'recent', 'previous', 'word', 'decoder', 'convolution', 'encoder', 'layer', 'figure', 'illustration', 'equation', 'main', 'difference', 'canonical', 'neural', 'machine', 'translation', 'model', 'architecture', 'condition', 'state', 'decoder', 'compute', 'sequence', 'always', 'input', 'context', 'attention', 'attention', 'mechanism', 'essentially', 'unchanged', 'canonical', 'neural', 'model', 'word', 'encoder', 'previous', 'state', 'decoder', 'back', 'equation', 'page', 'alternate', 'architecture', 'still', 'encoder', 'decoder', 'score', 'weight', 'input', 'word', 'encoder', 'state', 'encoder', 'state', 'input', 'word', 'embed', 'combine', 'addition', 'compute', 'context', 'vector', 'usual', 'trick', 'residual', 'connection', 'assist', 'train', 'deep', 'neural', 'network', 'self', 'attention', 'critique', 'recurrent', 'neural', 'network', 'require', 'lengthy', 'word', 'entire', 'input', 'time', 'consume', 'limit', 'previous', 'section', 'replace', 'recurrent', 'neural', 'network', 'model', 'context', 'window', 'enrich', 'representation', 'word', 'architectural', 'component', 'allow', 'wide', 'context', 'highly', 'already', 'encounter', 'attention', 'mechanism', 'consider', 'association', 'tween', 'input', 'word', 'output', 'build', 'vector', 'representation', 'entire', 'input', 'sequence', 'idea', 'self', 'attention', 'extend', 'idea', 'encoder', 'instead', 'input', 'output', 'self', 'attention', 'com', 'association', 'input', 'word', 'input', 'word', 'way', 'view', 'mechanism', 'representation', 'input', 'word', 'enrich', 'context', 'word', 'help', 'disambiguate', 'compute', 'self', 'attention', 'self', 'attention', 'sequence', 'vector', 'size', 'pack', 'matrix', 'self', 'attention', 'let', 'look', 'equation', 'detail', 'word', 'representation', 'context', 'word', 'dot', 'product', 'pack', 'matrix', 'transpose', 'result', 'vector', 'value', 'value', 'vector', 'scale', 'size', 'word', 'representation', 'vector', 'value', 'add', 'result', 'value', 'context', 'word', 'way', 'put', 'equation', 'matrix', 'notation', 'word', 'self', 'attention', 'weight', 'chapter', 'machine', 'translation', 'self', 'attention', 'layer', 'self', 'attention', 'step', 'describe', 'step', 'self', 'attention', 'layer', 'encode', 'input', 'sentence', 'step', 'follow', 'combine', 'self', 'attention', 'residual', 'connection', 'pass', 'word', 'representation', 'directly', 'self', 'attention', 'next', 'layer', 'normalization', 'step', 'section', 'page', 'layer', 'normalization', 'self', 'attention', 'standard', 'feed', 'forward', 'step', 'activation', 'function', 'apply', 'augment', 'residual', 'connection', 'layer', 'normalization', 'layer', 'normalization', 'page', 'deep', 'stack', 'several', 'layer', 'top', 'ex', 'start', 'input', 'word', 'embed', 'self', 'attention', 'layer', 'deep', 'model', 'reason', 'residual', 'connection', 'self', 'attention', 'layer', 'residual', 'connection', 'help', 'train', 'allow', 'shortcut', 'input', 'utilize', 'early', 'stage', 'advantage', 'complex', 'deep', 'model', 'enable', 'layer', 'standard', 'train', 'trick', 'help', 'especially', 'deep', 'model', 'attention', 'decoder', 'self', 'attention', 'output', 'word', 'decoder', 'traditional', 'attention', 'total', 'sub', 'layer', 'self', 'output', 'word', 'initially', 'encode', 'word', 'form', 'exactly', 'self', 'attention', 'computation', 'describe', 'equation', 'word', 'limit', 'word', 'previously', 'produce', 'output', 'word', 'let', 'denote', 'result', 'sub', 'layer', 'output', 'word', 'attention', 'mechanism', 'model', 'follow', 'closely', 'self', 'attention', 'difference', 'compute', 'self', 'attention', 'hide', 'state', 'compute', 'attention', 'decoder', 'state', 'encoder', 'state', 'attention', 'sh', 'alternate', 'architecture', 'input', 'word', 'self', 'attention', 'layer', 'self', 'attention', 'layer', 'decoder', 'layer', 'decoder', 'layer', 'output', 'word', 'prediction', 'select', 'output', 'word', 'output', 'word', 'figure', 'attention', 'base', 'machine', 'translation', 'input', 'encode', 'several', 'layer', 'self', 'attention', 'decoder', 'compute', 'attention', 'base', 'representation', 'input', 'several', 'initialize', 'previous', 'word', 'detailed', 'exposition', 'exp', 'attention', 'weight', 'sum', 'attention', 'computation', 'augment', 'add', 'residual', 'layer', 'additional', 'self', 'attention', 'layer', 'describe', 'worth', 'note', 'output', 'attention', 'computation', 'weight', 'sum', 'input', 'word', 'representation', 'add', 'representation', 'decoder', 'state', 'residual', 'connection', 'allow', 'skip', 'deep', 'thus', 'speed', 'train', 'feed', 'forward', 'layer', 'identical', 'sub', 'layer', 'follow', 'add', 'norm', 'step', 'residual', 'layer', 'normalization', 'note', 'description', 'attention', 'sub', 'entire', 'model', 'show', 'machine', 'translation', 'read', 'build', 'comprehensive', 'machine', 'translation', 'model', 'encode', 'source', 'sentence', 'neural', 'generate', 'target', 'sentence', 'reverse', 'process', 'propose', 'multiple', 'layer', 'encoder', 'decoder', 'reduce', 'length', 'encode', 'sequence', 'incorporate', 'wide', 'context', 'layer', 'replace', 'recurrent', 'neural', 'network', 'sequence', 'sequence', 'model', 'multiple', 'self', 'attention', 'encoder', 'well', 'decod', 'number', 'additional', 'call', 'multi', 'encode', 'sentence', 'position', 'current', 'challenge', 'neural', 'machine', 'translation', 'emerge', 'promise', 'machine', 'translation', 'approach', 'recent', 'show', 'superior', 'performance', 'public', 'benchmark', 'rapid', 'adoption', 'deployment', 'report', 'poor', 'system', 'build', 'low', 'resource', 'condition', 'examine', 'number', 'challenge', 'neural', 'machine', 'translation', 'give', 'empirical', 'result', 'technology', 'currently', 'hold', 'compare', 'traditional', 'statistical', 'chine', 'translation', 'show', 'recent', 'neural', 'machine', 'translation', 'still', 'overcome', 'various', 'notably', 'performance', 'domain', 'low', 'resource', 'condition', 'problem', 'common', 'neural', 'translation', 'model', 'show', 'robust', 'behavior', 'confront', 'condition', 'differ', 'train', 'condition', 'due', 'limited', 'exposure', 'train', 'unusual', 'input', 'case', 'domain', 'test', 'unlikely', 'initial', 'word', 'choice', 'beam', 'search', 'solution', 'problem', 'hence', 'lie', 'general', 'approach', 'training', 'step', 'optimize', 'single', 'word', 'prediction', 'give', 'perfectly', 'match', 'prior', 'sequence', 'challenge', 'examine', 'neural', 'machine', 'translation', 'much', 'less', 'answer', 'question', 'training', 'datum', 'lead', 'system', 'decide', 'word', 'choice', 'decode', 'bury', 'large', 'matrix', 'real', 'numbered', 'value', 'clear', 'develop', 'well', 'neural', 'machine', 'translation', 'common', 'neural', 'machine', 'translation', 'traditional', 'phrase', 'base', 'statistical', 'machine', 'translation', 'common', 'datum', 'draw', 'note', 'default', 'beam', 'search', 'single', 'model', 'data', 'process', 'pair', 'encode', 'word', 'vocabulary', 'limit', 'evaluation', 'current', 'challenge', 'statistical', 'machine', 'translation', 'system', 'train', 'build', 'phrase', 'base', 'system', 'standard', 'feature', 'commonly', 'recent', 'submission', 'consider', 'phrase', 'base', 'note', 'statistical', 'machine', 'translation', 'hierarchical', 'phrase', 'base', 'model', 'syntax', 'base', 'model', 'show', 'give', 'superior', 'performance', 'language', 'carry', 'experiment', 'large', 'train', 'datum', 'set', 'available', 'share', 'translation', 'task', 'organize', 'conference', 'machine', 'translation', 'domain', 'test', 'set', 'compose', 'news', 'characterize', 'broad', 'range', 'formal', 'relatively', 'long', 'sentence', 'word', 'high', 'standard', 'style', 'domain', 'mismatch', 'challenge', 'translation', 'different', 'word', 'different', 'mean', 'express', 'different', 'style', 'crucial', 'step', 'develop', 'chine', 'translation', 'system', 'target', 'case', 'domain', 'adaptation', 'expect', 'method', 'domain', 'adaptation', 'develop', 'neural', 'machine', 'translation', 'currently', 'popular', 'approach', 'train', 'general', 'domain', 'follow', 'train', 'domain', 'datum', 'epoch', 'large', 'amount', 'train', 'datum', 'available', 'still', 'seek', 'robust', 'performance', 'test', 'well', 'neural', 'machine', 'translation', 'statistical', 'machine', 'translation', 'hold', 'train', 'different', 'system', 'different', 'corpus', 'obtain', 'opus', 'additional', 'system', 'train', 'train', 'show', 'table', 'note', 'domain', 'quite', 'distant', 'much', 'news', 'global', 'voice', 'train', 'statistical', 'machine', 'translation', 'neural', 'machine', 'translation', 'system', 'domain', 'system', 'train', 'tune', 'test', 'set', 'sub', 'sample', 'datum', 'common', 'pair', 'encode', 'training', 'figure', 'result', 'domain', 'neural', 'statistical', 'machine', 'translation', 'system', 'similar', 'machine', 'translation', 'well', 'statistical', 'machine', 'translation', 'well', 'domain', 'performance', 'machine', 'translation', 'system', 'bad', 'almost', 'sometimes', 'dramatically', 'customary', 'domain', 'machine', 'domain', 'differ', 'domain', 'level', 'chapter', 'word', 'sentence', 'repository', 'system', 'law', 'figure', 'quality', 'system', 'train', 'domain', 'test', 'domain', 'neural', 'machine', 'translation', 'system', 'show', 'degraded', 'performance', 'domain', 'current', 'challenge', 'source', 'reference', 'look', 'around', 'look', 'around', 'look', 'law', 'order', 'implement', 'medical', 'mb', 'final', 'work', 'switch', 'look', 'around', 'look', 'figure', 'example', 'translation', 'sentence', 'translate', 'system', 'train', 'different', 'corpus', 'performance', 'domain', 'dramatically', 'bad', 'neural', 'machine', 'translation', 'instance', 'medical', 'system', 'lead', 'score', 'machine', 'machine', 'law', 'test', 'set', 'figure', 'display', 'example', 'translate', 'sentence', 'look', 'mostly', 'completely', 'unrelated', 'output', 'neural', 'machine', 'translation', 'system', 'translation', 'system', 'switch', 'pause', 'note', 'output', 'neural', 'machine', 'translation', 'system', 'often', 'quite', 'heed', 'soul', 'completely', 'unrelated', 'statistical', 'machine', 'translation', 'output', 'betray', 'cop', 'domain', 'input', 'leave', 'word', 'untranslated', 'particular', 'concern', 'information', 'user', 'mislead', 'hallucinate', 'content', 'neural', 'machine', 'translation', 'output', 'amount', 'training', 'datum', 'well', 'property', 'statistical', 'system', 'increase', 'amount', 'train', 'datum', 'lead', 'well', 'result', 'statistical', 'machine', 'translation', 'previously', 'observe', 'double', 'amount', 'training', 'datum', 'give', 'increase', 'score', 'hold', 'true', 'parallel', 'monolingual', 'datum', 'irvine', 'datum', 'statistical', 'machine', 'translation', 'neural', 'neural', 'machine', 'translation', 'promise', 'generalize', 'word', 'condition', 'large', 'context', 'input', 'prior', 'output', 'chapter', 'machine', 'translation', 'score', 'vary', 'amount', 'train', 'datum', 'phrase', 'base', 'big', 'phrase', 'base', 'size', 'figure', 'score', 'english', 'spanish', 'system', 'train', 'word', 'parallel', 'datum', 'quality', 'neural', 'machine', 'translation', 'start', 'much', 'outperform', 'statistical', 'machine', 'translation', 'beat', 'statistical', 'machine', 'translation', 'system', 'big', 'word', 'domain', 'language', 'model', 'high', 'resource', 'condition', 'current', 'challenge', 'ratio', 'word', 'election', 'obama', 'figure', 'translation', 'sentence', 'test', 'set', 'neural', 'machine', 'translation', 'system', 'train', 'vary', 'amount', 'train', 'datum', 'low', 'resource', 'neural', 'machine', 'translation', 'produce', 'output', 'unrelated', 'input', 'build', 'english', 'spanish', 'system', 'english', 'word', 'language', 'model', 'train', 'spanish', 'part', 'respectively', 'addition', 'neural', 'statistical', 'machine', 'translation', 'system', 'train', 'additionally', 'provide', 'monolingual', 'datum', 'big', 'language', 'model', 'statistical', 'machine', 'translation', 'system', 'result', 'show', 'figure', 'neural', 'machine', 'translation', 'exhibit', 'much', 'steep', 'outperform', 'statistical', 'machine', 'translation', 'beat', 'statistical', 'machine', 'translation', 'system', 'big', 'language', 'model', 'full', 'datum', 'set', 'neural', 'machine', 'statistical', 'machine', 'statistical', 'big', 'language', 'contrast', 'neural', 'statistical', 'machine', 'translation', 'learn', 'curve', 'quite', 'strike', 'neural', 'machine', 'translation', 'able', 'exploit', 'increase', 'amount', 'train', 'datum', 'unable', 'ground', 'training', 'size', 'word', 'less', 'strategy', 'election', 'start', 'translation', 'become', 'respectable', 'noisy', 'datum', 'statistical', 'machine', 'translation', 'fairly', 'robust', 'noisy', 'datum', 'quality', 'system', 'hold', 'fairly', 'large', 'part', 'train', 'datum', 'corrupt', 'various', 'align', 'content', 'wrong', 'badly', 'translate', 'statistical', 'chine', 'translation', 'model', 'build', 'probability', 'distribution', 'estimate', 'occur', 'word', 'phrase', 'unsystematic', 'noise', 'training', 'affect', 'tail', 'end', 'distribution', 'spanish', 'last', 'represent', 'translation', 'ratio', 'table', 'impact', 'noise', 'training', 'part', 'contain', 'pair', 'neural', 'machine', 'translation', 'degrade', 'statistical', 'machine', 'translation', 'hold', 'fairly', 'well', 'still', 'case', 'neural', 'machine', 'consider', 'kind', 'sentence', 'pair', 'experiment', 'large', 'parallel', 'corpus', 'target', 'side', 'part', 'train', 'table', 'show', 'result', 'statistical', 'machine', 'translation', 'system', 'hold', 'fairly', 'well', 'datum', 'quality', 'drop', 'expect', 'valid', 'training', 'datum', 'neural', 'machine', 'translation', 'system', 'degrade', 'drop', 'com', 'par', 'point', 'drop', 'statistical', 'system', 'possible', 'explanation', 'poor', 'behavior', 'neural', 'machine', 'translation', 'model', 'prediction', 'balance', 'language', 'model', 'input', 'context', 'main', 'driver', 'training', 'observe', 'increase', 'ratio', 'train', 'input', 'meaningless', 'generally', 'learn', 'rely', 'output', 'language', 'model', 'hence', 'hallucinate', 'inadequate', 'output', 'word', 'alignment', 'key', 'contribution', 'attention', 'model', 'neural', 'translation', 'imposition', 'alignment', 'output', 'word', 'input', 'word', 'shape', 'probability', 'distribution', 'input', 'word', 'weigh', 'bag', 'word', 'representation', 'input', 'sentence', 'attention', 'model', 'functionally', 'play', 'role', 'word', 'alignment', 'source', 'least', 'way', 'analog', 'statistical', 'machine', 'translation', 'alignment', 'latent', 'variable', 'obtain', 'probability', 'distribution', 'word', 'arguably', 'attention', 'model', 'broad', 'role', 'translate', 'attention', 'pay', 'object', 'disambiguate', 'complicate', 'word', 'representation', 'product', 'bidirectional', 'gate', 'recurrent', 'neural', 'network', 'effect', 'word', 'representation', 'inform', 'entire', 'sentence', 'context', 'clear', 'alignment', 'mechanism', 'source', 'target', 'word', 'prior', 'work', 'alignment', 'provide', 'attention', 'model', 'interpolate', 'word', 'translation', 'decision', 'traditional', 'probabilistic', 'dictionary', 'introduction', 'coverage', 'fertility', 'model', 'current', 'challenge', 'die', 'relationship', 'obama', 'year', 'desire', 'alignment', 'mismatch', 'alignment', 'figure', 'word', 'alignment', 'compare', 'attention', 'model', 'box', 'probability', 'percent', 'alignment', 'obtain', 'fast', 'align', 'attention', 'model', 'fact', 'proper', 'examine', 'compare', 'soft', 'alignment', 'matrix', 'sequence', 'attention', 'word', 'alignment', 'obtain', 'traditional', 'word', 'alignment', 'method', 'incremental', 'fast', 'align', 'align', 'input', 'output', 'neural', 'machine', 'system', 'figure', 'illustration', 'compare', 'word', 'attention', 'state', 'word', 'alignment', 'obtain', 'fast', 'align', 'match', 'pretty', 'well', 'attention', 'state', 'fast', 'align', 'alignment', 'point', 'bite', 'fuzzy', 'function', 'word', 'attention', 'model', 'correspond', 'tuition', 'alignment', 'point', 'obtain', 'fast', 'align', 'figure', 'reverse', 'language', 'alignment', 'point', 'appear', 'position', 'aware', 'intuitive', 'explanation', 'divergent', 'behavior', 'translation', 'quality', 'high', 'system', 'measure', 'well', 'soft', 'alignment', 'neural', 'machine', 'system', 'match', 'alignment', 'fast', 'align', 'match', 'score', 'check', 'output', 'align', 'input', 'word', 'accord', 'fast', 'align', 'indeed', 'input', 'word', 'receive', 'high', 'attention', 'probability', 'mass', 'score', 'probability', 'give', 'alignment', 'point', 'obtain', 'fast', 'align', 'chapter', 'neural', 'table', 'score', 'indicate', 'overlap', 'attention', 'probability', 'alignment', 'obtain', 'fast', 'align', 'handle', 'byte', 'pair', 'encode', 'alignment', 'neural', 'machine', 'translation', 'model', 'provide', 'fast', 'align', 'parallel', 'datum', 'set', 'obtain', 'alignment', 'model', 'align', 'input', 'output', 'neural', 'machine', 'translation', 'system', 'table', 'show', 'alignment', 'score', 'system', 'result', 'suggest', 'divergence', 'outlier', 'large', 'divergence', 'different', 'datum', 'condition', 'note', 'attention', 'model', 'produce', 'word', 'alignment', 'guide', 'alignment', 'training', 'supervise', 'word', 'alignment', 'produce', 'provide', 'model', 'train', 'beam', 'search', 'task', 'decode', 'full', 'sentence', 'translation', 'high', 'probability', 'machine', 'problem', 'address', 'heuristic', 'search', 'technique', 'explore', 'space', 'possible', 'translation', 'common', 'feature', 'search', 'technique', 'beam', 'size', 'parameter', 'limit', 'number', 'partial', 'translation', 'maintain', 'input', 'word', 'typically', 'straightforward', 'relationship', 'beam', 'size', 'parameter', 'model', 'score', 'result', 'translation', 'quality', 'score', 'diminish', 'return', 'increase', 'beam', 'typically', 'improvement', 'score', 'expect', 'large', 'beam', 'decode', 'neural', 'translation', 'model', 'set', 'similar', 'fashion', 'predict', 'next', 'output', 'commit', 'high', 'score', 'word', 'prediction', 'neural', 'machine', 'translation', 'operate', 'fast', 'align', 'full', 'word', 'input', 'word', 'split', 'byte', 'pair', 'add', 'attention', 'score', 'output', 'word', 'split', 'average', 'attention', 'vector', 'match', 'score', 'probability', 'mass', 'score', 'compute', 'average', 'output', 'word', 'level', 'score', 'output', 'word', 'fast', 'align', 'alignment', 'ignore', 'computation', 'output', 'word', 'fast', 'align', 'multiple', 'input', 'match', 'count', 'correct', 'align', 'word', 'top', 'high', 'scoring', 'word', 'accord', 'attention', 'probability', 'mass', 'add', 'attention', 'score', 'current', 'challenge', 'normalize', 'beam', 'size', 'beam', 'size', 'normalize', 'beam', 'size', 'normalize', 'beam', 'size', 'normalize', 'normalize', 'beam', 'size', 'beam', 'size', 'normalize', 'normalize', 'beam', 'size', 'beam', 'size', 'figure', 'translation', 'quality', 'vary', 'beam', 'size', 'large', 'quality', 'especially', 'normalize', 'score', 'sentence', 'length', 'machine', 'translation', 'maintain', 'next', 'scoring', 'word', 'list', 'partial', 'translation', 'record', 'partial', 'translation', 'word', 'translation', 'probability', 'extend', 'partial', 'translation', 'subsequent', 'word', 'prediction', 'accumulate', 'score', 'number', 'partial', 'translation', 'explode', 'exponentially', 'new', 'output', 'prune', 'beam', 'high', 'score', 'partial', 'translation', 'traditional', 'statistical', 'machine', 'translation', 'increase', 'beam', 'size', 'allow', 'explore', 'large', 'set', 'space', 'possible', 'translation', 'hence', 'translation', 'score', 'figure', 'increase', 'beam', 'size', 'consistently', 'improve', 'translation', 'quality', 'almost', 'bad', 'translation', 'find', 'optimal', 'beam', 'size', 'set', 'optimal', 'beam', 'size', 'vary', 'normalize', 'sentence', 'level', 'model', 'score', 'length', 'output', 'alleviate', 'problem', 'somewhat', 'lead', 'well', 'optimal', 'quality', 'case', 'language', 'pair', 'optimal', 'beam', 'size', 'range', 'almost', 'quality', 'still', 'drop', 'large', 'beam', 'main', 'cause', 'deteriorate', 'quality', 'short', 'translation', 'wide', 'beam', 'reading', 'study', 'look', 'comparable', 'performance', 'neural', 'statistical', 'machine', 'translation', 'system', 'consider', 'different', 'linguistic', 'category', 'german', 'compare', 'different', 'broad', 'aspect', 'reorder', 'language', 'direction', 'additional', 'topic', 'especially', 'early', 'work', 'neural', 'network', 'machine', 'translation', 'aim', 'build', 'neural', 'compo', 'traditional', 'statistical', 'machine', 'translation', 'system', 'translation', 'model', 'include', 'align', 'source', 'word', 'condition', 'enrich', 'feed', 'forward', 'neural', 'network', 'language', 'model', 'source', 'context', 'add', 'sentence', 'embe', 'conditional', 'context', 'learn', 'variant', 'neural', 'network', 'map', 'language', 'complex', 'neural', 'network', 'encode', 'input', 'sentence', 'gate', 'layer', 'incorporate', 'information', 'output', 'context', 'reorder', 'model', 'reorder', 'model', 'struggle', 'sparse', 'datum', 'problem', 'rich', 'context', 'show', 'neural', 'reorder', 'model', 'condition', 'current', 'previous', 'phrase', 'pair', 'recursive', 'neural', 'network', 'decision', 'orientation', 'type', 'additional', 'topic', 'instead', 'hand', 'reorder', 'decode', 'input', 'sentence', 'output', 'word', 'order', 'input', 'dependency', 'tree', 'learn', 'implement', 'feed', 'forward', 'neural', 'network', 'formulate', 'top', 'leave', 'walk', 'dependency', 'tree', 'reorder', 'decision', 'node', 'model', 'process', 'recurrent', 'neural', 'network', 'include', 'past', 'decision', 'condition', 'context', 'gram', 'translation', 'model', 'view', 'phrase', 'base', 'translation', 'model', 'phrase', 'translation', 'minimal', 'translation', 'employ', 'gram', 'model', 'unit', 'condition', 'minimal', 'translation', 'unit', 'previous', 'treat', 'minimal', 'translation', 'unit', 'atomic', 'symbol', 'train', 'neural', 'language', 'model', 'represent', 'minimal', 'translation', 'unit', 'bag', 'break', 'single', 'input', 'single', 'output', 'single', 'input', 'output', 'word', 'phrase', 'lean', 'auto', 'encoder', 'chapter', 'translation', 'bibliography', 'incorporate', 'discrete', 'translation', 'icon', 'neural', 'machine', 'translation', 'proceeding', 'conference', 'empirical', 'method', 'natural', 'language', 'processing', 'association', 'computational', 'neural', 'machine', 'translation', 'jointly', 'learn', 'align', 'translate', 'neural', 'language', 'translation', 'proceeding', 'conference', 'computational', 'page', 'mathematical', 'linguistic', 'pascal', 'christian', 'neural', 'probabilistic', 'language', 'model', 'learn', 'neural', 'phrase', 'base', 'machine', 'translation', 'case', 'study', 'proceeding', 'conference', 'empirical', 'meth', 'natural', 'language', 'processing', 'association', 'computational', 'page', 'finding', 'machine', 'translation', 'proceed', 'first', 'conference', 'chine', 'computational', 'page', 'enrique', 'vidal', 'machine', 'translation', 'neural', 'network', 'model', 'weight', 'machine', 'translation', 'domain', 'adaptation', 'proceed', 'first', 'method', 'bibliography', 'adaptive', 'training', 'datum', 'selection', 'machine', 'translation', 'annual', 'train', 'topic', 'aware', 'neural', 'translation', 'hierarchical', 'phrase', 'base', 'translation', 'computational', 'property', 'neural', 'machine', 'approach', 'proceed', 'eighth', 'workshop', 'semantic', 'structure', 'statistical', 'translation', 'association', 'computational', 'page', 'empirical', 'comparison', 'domain', 'adaptation', 'method', 'neural', 'machine', 'translation', 'proceed', 'annual', 'meeting', 'association', 'computational', 'computational', 'trevor', 'incorporate', 'structural', 'alignment', 'bias', 'neural', 'translation', 'model', 'proceeding', 'pure', 'neural', 'machine', 'translation', 'system', 'fast', 'accurate', 'neural', 'network', 'proceeding', 'conference', 'computational', 'page', 'robust', 'neural', 'network', 'joint', 'model', 'statistical', 'machine', 'translation', 'proceed', 'annual', 'meeting', 'association', 'computational', 'computational', 'translation', 'system', 'proceed', 'first', 'conference', 'chine', 'computational', 'page', 'online', 'learn', 'stochastic', 'optimization', 'learn', 'effective', 'model', 'proceeding', 'conference', 'computational', 'page', 'bibliography', 'marco', 'neural', 'phrase', 'base', 'machine', 'translation', 'multi', 'domain', 'scenario', 'proceed', 'paper', 'computational', 'page', 'improve', 'attention', 'model', 'implicit', 'distortion', 'fertility', 'translation', 'proceed', 'conference', 'technical', 'paper', 'organize', 'page', 'base', 'machine', 'system', 'recurrent', 'neural', 'network', 'language', 'model', 'proceed', 'name', 'association', 'computational', 'page', 'multilingual', 'neural', 'chine', 'translation', 'share', 'attention', 'mechanism', 'proceeding', 'conference', 'recursive', 'hetero', 'associative', 'memory', 'translation', 'biological', 'neuroscience', 'technology', 'page', 'fast', 'domain', 'adaptation', 'neural', 'training', 'context', 'rich', 'syntactic', 'translation', 'model', 'pro', 'international', 'conference', 'computational', 'linguistic', 'annual', 'meeting', 'translation', 'proceed', 'joint', 'conference', 'human', 'language', 'technology', 'annual', 'meeting', 'sequence', 'sequence', 'learn', 'greg', 'temporal', 'model', 'memory', 'network', 'method', 'natural', 'language', 'processing', 'volume', 'synthesis', 'lecture', 'human', 'language', 'learning', 'mit', 'press', 'hang', 'victor', 'incorporate', 'copy', 'mechanism', 'sequence', 'sequence', 'learn', 'proceed', 'annual', 'meeting', 'computational', 'bibliography', 'bowen', 'point', 'unknown', 'word', 'proceed', 'annual', 'meeting', 'association', 'computational', 'page', 'translation', 'model', 'recurrent', 'neural', 'network', 'proceed', 'computational', 'page', 'irvine', 'combine', 'bilingual', 'low', 'resource', 'machine', 'translation', 'proceed', 'eighth', 'workshop', 'statistical', 'chine', 'translation', 'roland', 'large', 'target', 'vocabulary', 'neural', 'machine', 'translation', 'proceed', 'annual', 'meeting', 'association', 'computational', 'international', 'joint', 'conference', 'natural', 'language', 'process', 'long', 'page', 'roland', 'neural', 'machine', 'translation', 'system', 'proceed', 'tenth', 'workshop', 'statistical', 'machine', 'translation', 'multilingual', 'neural', 'machine', 'translation', 'enable', 'shoot', 'translation', 'neural', 'chine', 'translation', 'ready', 'case', 'study', 'translation', 'direction', 'proceed', 'international', 'workshop', 'speak', 'language', 'translation', 'continuous', 'translation', 'model', 'proceeding', 'conference', 'empirical', 'method', 'natural', 'language', 'processing', 'association', 'neural', 'reorder', 'model', 'consider', 'phrase', 'translation', 'word', 'alignment', 'phrase', 'base', 'translation', 'proceed', 'rd', 'organize', 'optimization', 'control', 'neural', 'open', 'source', 'toolkit', 'statistical', 'machine', 'translation', 'proceed', 'annual', 'meeting', 'computational', 'companion', 'volume', 'proceed', 'demo', 'poster', 'layer', 'neural', 'reorder', 'model', 'phrase', 'base', 'translation', 'proceed', 'international', 'conference', 'com', 'technical', 'paper', 'machine', 'translation', 'supervise', 'attention', 'proceed', 'conference', 'technical', 'paper', 'organize', 'page', 'learn', 'new', 'semi', 'supervise', 'deep', 'auto', 'encoder', 'statistical', 'machine', 'translation', 'proceed', 'annual', 'meeting', 'association', 'computational', 'computational', 'page', 'translation', 'system', 'speak', 'language', 'domain', 'proceed', 'international', 'workshop', 'speak', 'language', 'translation', 'page', 'man', 'deep', 'neural', 'language', 'translation', 'proceed', 'conference', 'computational', 'page', 'christopher', 'man', 'effective', 'approach', 'attention', 'base', 'neural', 'machine', 'translation', 'proceeding', 'conference', 'empirical', 'method', 'natural', 'language', 'processing', 'association', 'computational', 'page', 'address', 'rare', 'word', 'problem', 'neural', 'translation', 'proceed', 'annual', 'meeting', 'association', 'computational', 'international', 'joint', 'conference', 'natural', 'language', 'process', 'long', 'page', 'hang', 'interactive', 'attention', 'neural', 'machine', 'translation', 'proceed', 'paper', 'organize', 'page', 'hang', 'encode', 'source', 'language', 'neural', 'network', 'translation', 'proceed', 'annual', 'meeting', 'association', 'computational', 'international', 'joint', 'conference', 'natural', 'language', 'process', 'long', 'association', 'computational', 'abe', 'vocabulary', 'manipulation', 'neural', 'machine', 'translation', 'proceed', 'annual', 'meeting', 'association', 'computational', 'computational', 'bibliography', 'antonio', 'dependency', 'base', 'reorder', 'recurrent', 'neural', 'network', 'translation', 'proceed', 'annual', 'meeting', 'association', 'computational', 'international', 'joint', 'conference', 'language', 'process', 'long', 'association', 'computational', 'architecture', 'neural', 'machine', 'translation', 'proceed', 'second', 'conference', 'machine', 'volume', 'computational', 'page', 'toma', 'statistical', 'language', 'model', 'neural', 'network', 'toma', 'geoffrey', 'linguistic', 'regularity', 'continuous', 'space', 'word', 'representation', 'proceeding', 'conference', 'computational', 'page', 'toma', 'train', 'recurrent', 'neural', 'network', 'proceed', 'conference', 'machine', 'page', 'online', 'learn', 'neural', 'machine', 'edit', 'continuous', 'space', 'language', 'computer', 'speech', 'language', 'continuous', 'space', 'language', 'model', 'statistical', 'machine', 'translation', 'mathematical', 'linguistic', 'continuous', 'space', 'translation', 'model', 'phrase', 'base', 'statistical', 'machine', 'tran', 'proceed', 'poster', 'organize', 'bilingual', 'gram', 'translation', 'proceed', 'joint', 'conference', 'empirical', 'method', 'natural', 'process', 'computational', 'natural', 'language', 'learning', 'page', 'continuous', 'space', 'language', 'model', 'statistical', 'machine', 'translation', 'proceed', 'main', 'confer', 'poster', 'session', 'computational', 'page', 'prune', 'continuous', 'space', 'language', 'model', 'statistical', 'machine', 'translation', 'proceed', 'ever', 'really', 'replace', 'gram', 'future', 'language', 'linguistic', 'input', 'feature', 'improve', 'neural', 'machine', 'translation', 'proceed', 'first', 'conference', 'machine', 'bibliography', 'neural', 'machine', 'translation', 'side', 'constraint', 'proceeding', 'conference', 'chine', 'translation', 'system', 'proceed', 'first', 'conference', 'machine', 'improve', 'neural', 'machine', 'translation', 'monolingual', 'datum', 'proceed', 'annual', 'meeting', 'association', 'computational', 'translation', 'rare', 'word', 'unit', 'proceed', 'annual', 'meeting', 'domain', 'post', 'training', 'domain', 'adaptation', 'neural', 'machine', 'translation', 'geoffrey', 'simple', 'way', 'prevent', 'neural', 'network', 'learn', 'recurrent', 'network', 'language', 'conference', 'signal', 'processing', 'page', 'le', 'sequence', 'sequence', 'learn', 'neural', 'network', 'advance', 'neural', 'information', 'process', 'system', 'page', 'incremental', 'adaptation', 'strategy', 'neural', 'network', 'language', 'workshop', 'continuous', 'tor', 'computational', 'page', 'parallel', 'tool', 'eighth', 'conference', 'resource', 'evaluation', 'page', 'anthology', 'multifacete', 'evaluation', 'phrase', 'base', 'machine', 'translation', 'language', 'direction', 'proceed', 'volume', 'paper', 'bibliography', 'reconstruction', 'proceed', 'machine', 'translation', 'proceed', 'annual', 'meeting', 'association', 'computational', 'learn', 'performance', 'machine', 'statistical', 'computational', 'analysis', 'proceed', 'third', 'workshop', 'computational', 'page', 'attention', 'large', 'scale', 'neural', 'language', 'model', 'improve', 'translation', 'proceeding', 'conference', 'empirical', 'meth', 'natural', 'language', 'processing', 'association', 'computational', 'page', 'speech', 'speech', 'translation', 'system', 'symbolic', 'processing', 'strategy', 'proceed', 'international', 'conference', 'speech', 'signal', 'processing', 'page', 'continuous', 'space', 'language', 'model', 'language', 'model', 'statistical', 'machine', 'translation', 'proceeding', 'conference', 'empirical', 'method', 'natural', 'language', 'association', 'computational', 'neural', 'network', 'base', 'bilingual', 'language', 'model', 'grow', 'statistical', 'machine', 'translation', 'proceed', 'empirical', 'method', 'natural', 'language', 'processing', 'association', 'computational', 'statistical', 'machine', 'translation', 'system', 'proceed', 'first', 'conference', 'machine', 'improve', 'statistical', 'machine', 'translation', 'context', 'sensitive', 'proceeding', 'conference', 'empirical', 'method', 'natural', 'process', 'association', 'computational', 'bibliography', 'translation', 'bridge', 'gap', 'human', 'machine', 'translation', 'factor', 'recurrent', 'neural', 'network', 'language', 'model', 'transcription', 'pro', 'seventh', 'international', 'workshop', 'speak', 'language', 'translation', 'page', 'dual', 'learning', 'machine', 'translation', 'recurrent', 'neural', 'network', 'base', 'rule', 'sequence', 'model', 'translation', 'proceed', 'annual', 'meeting', 'com', 'international', 'joint', 'conference', 'natural', 'language', 'process', 'computational', 'page', 'matthew', 'adaptive', 'learning', 'rate', 'method', 'local', 'translation', 'prediction', 'global', 'sentence', 'fourth', 'international', 'joint', 'conference', 'intelligence', 'page', 'bibliography', 'author', 'index', 'toma', 'christine', 'matthew']]\n"
     ]
    }
   ],
   "source": [
    "print(data_ready[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64dab654",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(data_ready, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c71f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store just the words + their trained embeddings.\n",
    "word_vectors = model.wv\n",
    "word_vectors.save(\"word2vec.wordvectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "567d94ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building Word2Vec model\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb4d4519",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))\n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "# converting texts to numerical data using Word2Vec\n",
    "vectors_w2v = modelw.transform(data_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a68faaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d70422e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word from Corpus [(0, 1), (1, 1), (2, 2), (3, 14), (4, 1), (5, 1), (6, 1), (7, 1), (8, 2), (9, 2), (10, 1), (11, 2), (12, 9), (13, 1), (14, 1), (15, 6), (16, 2), (17, 3), (18, 32), (19, 2)]\n",
      "Word from id2word abandon\n"
     ]
    }
   ],
   "source": [
    "# lets create our Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "# Create our Corpus Term Doc Frequency\n",
    "corpus = []\n",
    "for text in data_ready:\n",
    "    new = id2word.doc2bow(text)\n",
    "    corpus.append(new)\n",
    "    \n",
    "    \n",
    "# print first word from my corpus\n",
    "print (\"Word from Corpus\", corpus[0][0:20])\n",
    "# lets print the word and see the result\n",
    "word = id2word[[0][:1][0]]\n",
    "print (\"Word from id2word\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b41c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the ladmodel with the best parameter after the tunning\n",
    "# alpha string\n",
    "# for whole articles use 32 but for 50 articles I will use only 10\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics= 10,\n",
    "                                           random_state = 100,\n",
    "                                           update_every = 1,\n",
    "                                           chunksize = 100,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           per_word_topics=True,\n",
    "                                           eta = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce014d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.4124980839751295\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherencemodel = CoherenceModel(model=lda_model, texts=data_ready, dictionary=id2word, coherence='c_v')\n",
    "coherence= coherencemodel.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b2a31ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.025*\"measure\" + 0.019*\"class\" + 0.014*\"predict\" + 0.014*\"true\" + 0.013*\"set\" + 0.012*\"base\" + 0.012*\"pair\" + 0.009*\"hierarchical\" + 0.009*\"figure\" + 0.008*\"system\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.019*\"domain\" + 0.016*\"learn\" + 0.011*\"modality\" + 0.011*\"target\" + 0.010*\"representation\" + 0.010*\"source\" + 0.010*\"feature\" + 0.009*\"datum\" + 0.008*\"set\" + 0.008*\"adaptation\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.014*\"naive\" + 0.013*\"probability\" + 0.012*\"class\" + 0.009*\"baye\" + 0.008*\"text\" + 0.006*\"feature\" + 0.006*\"word\" + 0.006*\"conditional\" + 0.005*\"sample\" + 0.005*\"document\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.014*\"test\" + 0.013*\"model\" + 0.009*\"method\" + 0.009*\"system\" + 0.009*\"word\" + 0.009*\"likelihood\" + 0.008*\"document\" + 0.008*\"sequence\" + 0.007*\"information\" + 0.007*\"language\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.033*\"word\" + 0.028*\"model\" + 0.013*\"language\" + 0.012*\"vector\" + 0.010*\"train\" + 0.009*\"learn\" + 0.008*\"set\" + 0.007*\"datum\" + 0.007*\"representation\" + 0.007*\"table\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.012*\"walk\" + 0.010*\"graph\" + 0.008*\"random\" + 0.008*\"vertex\" + 0.007*\"representation\" + 0.006*\"learn\" + 0.005*\"social\" + 0.005*\"network\" + 0.004*\"label\" + 0.004*\"method\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.024*\"model\" + 0.019*\"neural\" + 0.018*\"network\" + 0.017*\"word\" + 0.014*\"translation\" + 0.011*\"input\" + 0.011*\"learn\" + 0.010*\"output\" + 0.009*\"train\" + 0.009*\"layer\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.014*\"sequence\" + 0.013*\"part\" + 0.012*\"word\" + 0.009*\"mathematical\" + 0.009*\"equation\" + 0.008*\"time\" + 0.008*\"dictionary\" + 0.005*\"result\" + 0.005*\"identify\" + 0.004*\"phrase\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.004*\"solver\" + 0.003*\"factorization\" + 0.003*\"fast\" + 0.002*\"machine\" + 0.002*\"python\" + 0.002*\"library\" + 0.002*\"implementation\" + 0.002*\"interface\" + 0.001*\"code\" + 0.001*\"model\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.019*\"term\" + 0.015*\"weight\" + 0.010*\"text\" + 0.008*\"stem\" + 0.007*\"scheme\" + 0.007*\"document\" + 0.006*\"category\" + 0.006*\"word\" + 0.006*\"network\" + 0.006*\"summary\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets see the Topics \n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f22de0",
   "metadata": {},
   "source": [
    "## Tunning the Parameters of LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b82523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal number of Topics for LDA\n",
    "\n",
    "# def compute_coherence_values(dictionary, corpus,texts, limit, start,step,alpha, beta):\n",
    "#     coherence_values = []\n",
    "#     model_list = []\n",
    "#     for num_topics in range(start, limit, step):\n",
    "#         model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "#                                            id2word=id2word,\n",
    "#                                            num_topics=num_topics, \n",
    "#                                            random_state=100,\n",
    "#                                            update_every=1,\n",
    "#                                            chunksize=100,\n",
    "#                                            passes=10,\n",
    "#                                            alpha=alpha,\n",
    "#                                            per_word_topics=True,\n",
    "#                                            eta = beta)\n",
    "#         model_list.append(model)\n",
    "#         coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "#         coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "#     return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d271cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "\n",
    "#alpha_list = ['symmetric',0.3,0.5,0.7]\n",
    "#beta_list = ['auto',0.3,0.5,0.7]\n",
    "\n",
    "\n",
    "#for alpha in alpha_list:\n",
    "        #for beta in beta_list:\n",
    "            #model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=tok, start=2, limit=40, step=6,alpha=alpha,beta =beta)\n",
    "            #print(f\"alpha : {alpha} ; beta : {beta} ; Score : {coherence_values}\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c875578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show graph\n",
    "# limit=40; start=2; step=6;\n",
    "# x = range(start, limit, step)\n",
    "# plt.plot(x, coherence_values)\n",
    "# plt.xlabel(\"Num Topics\")\n",
    "# plt.ylabel(\"Coherence score\")\n",
    "# plt.legend((\"coherence_values\"), loc='best')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2960cf47",
   "metadata": {},
   "source": [
    "### End of the Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582e2306",
   "metadata": {},
   "source": [
    "# Understanding the LDA topic Model through DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc45927",
   "metadata": {},
   "source": [
    "# Finding the dominant topic in each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54e97442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>model, neural, network, word, translation, inp...</td>\n",
       "      <td>[statistical, machine, translation, draft, cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6962</td>\n",
       "      <td>domain, learn, modality, target, representatio...</td>\n",
       "      <td>[find, learn, robust, joint, representation, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>domain, learn, modality, target, representatio...</td>\n",
       "      <td>[available, online, system, engineering, route...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>term, weight, text, stem, scheme, document, ca...</td>\n",
       "      <td>[inverse, category, frequency, base, supervise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>test, model, method, system, word, likelihood,...</td>\n",
       "      <td>[find, structure, genome, symbolic, sequence, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.8483</td>\n",
       "      <td>term, weight, text, stem, scheme, document, ca...</td>\n",
       "      <td>[stem, ultra, stem, improve, automatic, text, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.9864</td>\n",
       "      <td>sequence, part, word, mathematical, equation, ...</td>\n",
       "      <td>[detect, phrase, university, apply, informatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6916</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[fast, accurate, sentiment, classification, en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>measure, class, predict, true, set, base, pair...</td>\n",
       "      <td>[evaluation, measure, hierarchical, view, nove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.5489</td>\n",
       "      <td>term, weight, text, stem, scheme, document, ca...</td>\n",
       "      <td>[generate, sequence, recurrent, neural, networ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0             6.0              0.9999   \n",
       "1            1             1.0              0.6962   \n",
       "2            2             1.0              0.9993   \n",
       "3            3             9.0              0.9996   \n",
       "4            4             3.0              0.9999   \n",
       "5            5             9.0              0.8483   \n",
       "6            6             7.0              0.9864   \n",
       "7            7             4.0              0.6916   \n",
       "8            8             0.0              0.9998   \n",
       "9            9             9.0              0.5489   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  model, neural, network, word, translation, inp...   \n",
       "1  domain, learn, modality, target, representatio...   \n",
       "2  domain, learn, modality, target, representatio...   \n",
       "3  term, weight, text, stem, scheme, document, ca...   \n",
       "4  test, model, method, system, word, likelihood,...   \n",
       "5  term, weight, text, stem, scheme, document, ca...   \n",
       "6  sequence, part, word, mathematical, equation, ...   \n",
       "7  word, model, language, vector, train, learn, s...   \n",
       "8  measure, class, predict, true, set, base, pair...   \n",
       "9  term, weight, text, stem, scheme, document, ca...   \n",
       "\n",
       "                                                Text  \n",
       "0  [statistical, machine, translation, draft, cha...  \n",
       "1  [find, learn, robust, joint, representation, c...  \n",
       "2  [available, online, system, engineering, route...  \n",
       "3  [inverse, category, frequency, base, supervise...  \n",
       "4  [find, structure, genome, symbolic, sequence, ...  \n",
       "5  [stem, ultra, stem, improve, automatic, text, ...  \n",
       "6  [detect, phrase, university, apply, informatio...  \n",
       "7  [fast, accurate, sentiment, classification, en...  \n",
       "8  [evaluation, measure, hierarchical, view, nove...  \n",
       "9  [generate, sequence, recurrent, neural, networ...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "            # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = ldamodel.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f597d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dominant_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1331adde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>measure, class, predict, true, set, base, pair...</td>\n",
       "      <td>[evaluation, measure, hierarchical, view, nove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6962</td>\n",
       "      <td>domain, learn, modality, target, representatio...</td>\n",
       "      <td>[find, learn, robust, joint, representation, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>domain, learn, modality, target, representatio...</td>\n",
       "      <td>[available, online, system, engineering, route...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9689</td>\n",
       "      <td>domain, learn, modality, target, representatio...</td>\n",
       "      <td>[learn, research, publish, domain, adversarial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8527</td>\n",
       "      <td>naive, probability, class, baye, text, feature...</td>\n",
       "      <td>[naive, bayes, text, introduction, theory, int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>test, model, method, system, word, likelihood,...</td>\n",
       "      <td>[find, structure, genome, symbolic, sequence, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.7510</td>\n",
       "      <td>test, model, method, system, word, likelihood,...</td>\n",
       "      <td>[relation, extraction, framework, wa, informat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.7700</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[language, model, engineering, apply, abstract...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[caption, visual, concept, abstract, paper, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[effective, word, order, text, categorization,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.5572</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[large, target, vocabulary, neural, machine, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[accept, workshop, contribution, university, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.5839</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[accept, workshop, contribution, metric, netwo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.7168</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[show, neural, image, caption, generator, auto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[fast, accurate, dependency, parser, new, abst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6210</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[address, rare, word, problem, neural, abstrac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.5332</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[ask, dynamic, memory, network, natural, langu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[embed, paragraph, vector, abstract, paragraph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[find, function, character, model, open, vocab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9906</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[text, understand, scratch, nary, interested, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6695</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[end, memory, network, dept, computer, fergus,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8507</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[machine, translation, rare, word, unit, abstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8563</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[network, abstract, report, series, experiment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9983</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[morphology, word, abstract, paper, present, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[distribute, representation, sentence, documen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[open, question, answer, weakly, new, build, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9056</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[generalize, language, model, combination, ski...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9865</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[word, benchmark, measure, progress, statistic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9958</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[exploit, similarity, language, machine, trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6916</td>\n",
       "      <td>word, model, language, vector, train, learn, s...</td>\n",
       "      <td>[fast, accurate, sentiment, classification, en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.4644</td>\n",
       "      <td>walk, graph, random, vertex, representation, l...</td>\n",
       "      <td>[online, learn, social, representation, comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9847</td>\n",
       "      <td>model, neural, network, word, translation, inp...</td>\n",
       "      <td>[collaborative, deep, learn, system, science, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.4964</td>\n",
       "      <td>model, neural, network, word, translation, inp...</td>\n",
       "      <td>[deep, bag, feature, model, music, auto, tag, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>model, neural, network, word, translation, inp...</td>\n",
       "      <td>[end, end, attention, base, large, vocabulary,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9696</td>\n",
       "      <td>model, neural, network, word, translation, inp...</td>\n",
       "      <td>[effective, approach, attention, base, neural,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>model, neural, network, word, translation, inp...</td>\n",
       "      <td>[semantically, condition, natural, language, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>model, neural, network, word, translation, inp...</td>\n",
       "      <td>[attend, brain, abstract, present, attend, neu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9952</td>\n",
       "      <td>model, neural, network, word, translation, inp...</td>\n",
       "      <td>[critical, review, recurrent, neural, network,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>model, neural, network, word, translation, inp...</td>\n",
       "      <td>[statistical, machine, translation, draft, cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.8882</td>\n",
       "      <td>model, neural, network, word, translation, inp...</td>\n",
       "      <td>[sequence, sequence, learn, neural, network, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.7251</td>\n",
       "      <td>model, neural, network, word, translation, inp...</td>\n",
       "      <td>[improve, semantic, representation, tree, stru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>model, neural, network, word, translation, inp...</td>\n",
       "      <td>[accept, workshop, contribution, earn, emory, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9795</td>\n",
       "      <td>model, neural, network, word, translation, inp...</td>\n",
       "      <td>[learn, phrase, representation, statistical, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>model, neural, network, word, translation, inp...</td>\n",
       "      <td>[publish, conference, paper, neural, machine, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.9864</td>\n",
       "      <td>sequence, part, word, mathematical, equation, ...</td>\n",
       "      <td>[detect, phrase, university, apply, informatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.3937</td>\n",
       "      <td>solver, factorization, fast, machine, python, ...</td>\n",
       "      <td>[learn, research, submit, revise, publish, fas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.8483</td>\n",
       "      <td>term, weight, text, stem, scheme, document, ca...</td>\n",
       "      <td>[stem, ultra, stem, improve, automatic, text, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>term, weight, text, stem, scheme, document, ca...</td>\n",
       "      <td>[inverse, category, frequency, base, supervise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>term, weight, text, stem, scheme, document, ca...</td>\n",
       "      <td>[evaluation, word, new, widely, automatic, eva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.5489</td>\n",
       "      <td>term, weight, text, stem, scheme, document, ca...</td>\n",
       "      <td>[generate, sequence, recurrent, neural, networ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "8             8             0.0              0.9998   \n",
       "1             1             1.0              0.6962   \n",
       "2             2             1.0              0.9993   \n",
       "37           37             1.0              0.9689   \n",
       "22           22             2.0              0.8527   \n",
       "4             4             3.0              0.9999   \n",
       "35           35             3.0              0.7510   \n",
       "48           48             4.0              0.7700   \n",
       "25           25             4.0              0.9994   \n",
       "26           26             4.0              0.9970   \n",
       "27           27             4.0              0.5572   \n",
       "28           28             4.0              0.9863   \n",
       "29           29             4.0              0.5839   \n",
       "24           24             4.0              0.7168   \n",
       "33           33             4.0              0.9996   \n",
       "23           23             4.0              0.6210   \n",
       "39           39             4.0              0.5332   \n",
       "40           40             4.0              0.9992   \n",
       "43           43             4.0              0.9989   \n",
       "31           31             4.0              0.9906   \n",
       "34           34             4.0              0.6695   \n",
       "49           49             4.0              0.8507   \n",
       "18           18             4.0              0.8563   \n",
       "16           16             4.0              0.9983   \n",
       "15           15             4.0              0.9945   \n",
       "14           14             4.0              0.9996   \n",
       "13           13             4.0              0.9056   \n",
       "11           11             4.0              0.9865   \n",
       "10           10             4.0              0.9958   \n",
       "7             7             4.0              0.6916   \n",
       "12           12             5.0              0.4644   \n",
       "20           20             6.0              0.9847   \n",
       "46           46             6.0              0.4964   \n",
       "45           45             6.0              0.9995   \n",
       "44           44             6.0              0.9696   \n",
       "42           42             6.0              0.9992   \n",
       "41           41             6.0              0.9987   \n",
       "38           38             6.0              0.9952   \n",
       "0             0             6.0              0.9999   \n",
       "21           21             6.0              0.8882   \n",
       "32           32             6.0              0.7251   \n",
       "30           30             6.0              0.9995   \n",
       "17           17             6.0              0.9795   \n",
       "19           19             6.0              0.9996   \n",
       "6             6             7.0              0.9864   \n",
       "36           36             8.0              0.3937   \n",
       "5             5             9.0              0.8483   \n",
       "3             3             9.0              0.9996   \n",
       "47           47             9.0              0.5875   \n",
       "9             9             9.0              0.5489   \n",
       "\n",
       "                                             Keywords  \\\n",
       "8   measure, class, predict, true, set, base, pair...   \n",
       "1   domain, learn, modality, target, representatio...   \n",
       "2   domain, learn, modality, target, representatio...   \n",
       "37  domain, learn, modality, target, representatio...   \n",
       "22  naive, probability, class, baye, text, feature...   \n",
       "4   test, model, method, system, word, likelihood,...   \n",
       "35  test, model, method, system, word, likelihood,...   \n",
       "48  word, model, language, vector, train, learn, s...   \n",
       "25  word, model, language, vector, train, learn, s...   \n",
       "26  word, model, language, vector, train, learn, s...   \n",
       "27  word, model, language, vector, train, learn, s...   \n",
       "28  word, model, language, vector, train, learn, s...   \n",
       "29  word, model, language, vector, train, learn, s...   \n",
       "24  word, model, language, vector, train, learn, s...   \n",
       "33  word, model, language, vector, train, learn, s...   \n",
       "23  word, model, language, vector, train, learn, s...   \n",
       "39  word, model, language, vector, train, learn, s...   \n",
       "40  word, model, language, vector, train, learn, s...   \n",
       "43  word, model, language, vector, train, learn, s...   \n",
       "31  word, model, language, vector, train, learn, s...   \n",
       "34  word, model, language, vector, train, learn, s...   \n",
       "49  word, model, language, vector, train, learn, s...   \n",
       "18  word, model, language, vector, train, learn, s...   \n",
       "16  word, model, language, vector, train, learn, s...   \n",
       "15  word, model, language, vector, train, learn, s...   \n",
       "14  word, model, language, vector, train, learn, s...   \n",
       "13  word, model, language, vector, train, learn, s...   \n",
       "11  word, model, language, vector, train, learn, s...   \n",
       "10  word, model, language, vector, train, learn, s...   \n",
       "7   word, model, language, vector, train, learn, s...   \n",
       "12  walk, graph, random, vertex, representation, l...   \n",
       "20  model, neural, network, word, translation, inp...   \n",
       "46  model, neural, network, word, translation, inp...   \n",
       "45  model, neural, network, word, translation, inp...   \n",
       "44  model, neural, network, word, translation, inp...   \n",
       "42  model, neural, network, word, translation, inp...   \n",
       "41  model, neural, network, word, translation, inp...   \n",
       "38  model, neural, network, word, translation, inp...   \n",
       "0   model, neural, network, word, translation, inp...   \n",
       "21  model, neural, network, word, translation, inp...   \n",
       "32  model, neural, network, word, translation, inp...   \n",
       "30  model, neural, network, word, translation, inp...   \n",
       "17  model, neural, network, word, translation, inp...   \n",
       "19  model, neural, network, word, translation, inp...   \n",
       "6   sequence, part, word, mathematical, equation, ...   \n",
       "36  solver, factorization, fast, machine, python, ...   \n",
       "5   term, weight, text, stem, scheme, document, ca...   \n",
       "3   term, weight, text, stem, scheme, document, ca...   \n",
       "47  term, weight, text, stem, scheme, document, ca...   \n",
       "9   term, weight, text, stem, scheme, document, ca...   \n",
       "\n",
       "                                                 Text  \n",
       "8   [evaluation, measure, hierarchical, view, nove...  \n",
       "1   [find, learn, robust, joint, representation, c...  \n",
       "2   [available, online, system, engineering, route...  \n",
       "37  [learn, research, publish, domain, adversarial...  \n",
       "22  [naive, bayes, text, introduction, theory, int...  \n",
       "4   [find, structure, genome, symbolic, sequence, ...  \n",
       "35  [relation, extraction, framework, wa, informat...  \n",
       "48  [language, model, engineering, apply, abstract...  \n",
       "25  [caption, visual, concept, abstract, paper, pr...  \n",
       "26  [effective, word, order, text, categorization,...  \n",
       "27  [large, target, vocabulary, neural, machine, r...  \n",
       "28  [accept, workshop, contribution, university, u...  \n",
       "29  [accept, workshop, contribution, metric, netwo...  \n",
       "24  [show, neural, image, caption, generator, auto...  \n",
       "33  [fast, accurate, dependency, parser, new, abst...  \n",
       "23  [address, rare, word, problem, neural, abstrac...  \n",
       "39  [ask, dynamic, memory, network, natural, langu...  \n",
       "40  [embed, paragraph, vector, abstract, paragraph...  \n",
       "43  [find, function, character, model, open, vocab...  \n",
       "31  [text, understand, scratch, nary, interested, ...  \n",
       "34  [end, memory, network, dept, computer, fergus,...  \n",
       "49  [machine, translation, rare, word, unit, abstr...  \n",
       "18  [network, abstract, report, series, experiment...  \n",
       "16  [morphology, word, abstract, paper, present, m...  \n",
       "15  [distribute, representation, sentence, documen...  \n",
       "14  [open, question, answer, weakly, new, build, c...  \n",
       "13  [generalize, language, model, combination, ski...  \n",
       "11  [word, benchmark, measure, progress, statistic...  \n",
       "10  [exploit, similarity, language, machine, trans...  \n",
       "7   [fast, accurate, sentiment, classification, en...  \n",
       "12  [online, learn, social, representation, comput...  \n",
       "20  [collaborative, deep, learn, system, science, ...  \n",
       "46  [deep, bag, feature, model, music, auto, tag, ...  \n",
       "45  [end, end, attention, base, large, vocabulary,...  \n",
       "44  [effective, approach, attention, base, neural,...  \n",
       "42  [semantically, condition, natural, language, g...  \n",
       "41  [attend, brain, abstract, present, attend, neu...  \n",
       "38  [critical, review, recurrent, neural, network,...  \n",
       "0   [statistical, machine, translation, draft, cha...  \n",
       "21  [sequence, sequence, learn, neural, network, a...  \n",
       "32  [improve, semantic, representation, tree, stru...  \n",
       "30  [accept, workshop, contribution, earn, emory, ...  \n",
       "17  [learn, phrase, representation, statistical, m...  \n",
       "19  [publish, conference, paper, neural, machine, ...  \n",
       "6   [detect, phrase, university, apply, informatio...  \n",
       "36  [learn, research, submit, revise, publish, fas...  \n",
       "5   [stem, ultra, stem, improve, automatic, text, ...  \n",
       "3   [inverse, category, frequency, base, supervise...  \n",
       "47  [evaluation, word, new, widely, automatic, eva...  \n",
       "9   [generate, sequence, recurrent, neural, networ...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets sort the dataframe according to the Dominant Topic\n",
    "df_soreted = df_dominant_topic.sort_values(by='Dominant_Topic', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "df_soreted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af9aeca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets Get the topics numbers and articles\n",
    "topics = df_soreted[['Dominant_Topic','Text']].values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd2731f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_numbers = [item[0] for item in topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb094f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets put all articles grouped together by its topic number\n",
    "grouped_df = df_soreted.groupby('Dominant_Topic').agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ca628e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>[8]</td>\n",
       "      <td>[0.9998000264167786]</td>\n",
       "      <td>[measure, class, predict, true, set, base, pai...</td>\n",
       "      <td>[[evaluation, measure, hierarchical, view, nov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>[1, 2, 37]</td>\n",
       "      <td>[0.6962000131607056, 0.9993000030517578, 0.968...</td>\n",
       "      <td>[domain, learn, modality, target, representati...</td>\n",
       "      <td>[[find, learn, robust, joint, representation, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>[22]</td>\n",
       "      <td>[0.8526999950408936]</td>\n",
       "      <td>[naive, probability, class, baye, text, featur...</td>\n",
       "      <td>[[naive, bayes, text, introduction, theory, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>[4, 35]</td>\n",
       "      <td>[0.9998999834060669, 0.7509999871253967]</td>\n",
       "      <td>[test, model, method, system, word, likelihood...</td>\n",
       "      <td>[[find, structure, genome, symbolic, sequence,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>[48, 25, 26, 27, 28, 29, 24, 33, 23, 39, 40, 4...</td>\n",
       "      <td>[0.7699999809265137, 0.9994000196456909, 0.996...</td>\n",
       "      <td>[word, model, language, vector, train, learn, ...</td>\n",
       "      <td>[[language, model, engineering, apply, abstrac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Document_No  \\\n",
       "Dominant_Topic                                                      \n",
       "0.0                                                           [8]   \n",
       "1.0                                                    [1, 2, 37]   \n",
       "2.0                                                          [22]   \n",
       "3.0                                                       [4, 35]   \n",
       "4.0             [48, 25, 26, 27, 28, 29, 24, 33, 23, 39, 40, 4...   \n",
       "\n",
       "                                               Topic_Perc_Contrib  \\\n",
       "Dominant_Topic                                                      \n",
       "0.0                                          [0.9998000264167786]   \n",
       "1.0             [0.6962000131607056, 0.9993000030517578, 0.968...   \n",
       "2.0                                          [0.8526999950408936]   \n",
       "3.0                      [0.9998999834060669, 0.7509999871253967]   \n",
       "4.0             [0.7699999809265137, 0.9994000196456909, 0.996...   \n",
       "\n",
       "                                                         Keywords  \\\n",
       "Dominant_Topic                                                      \n",
       "0.0             [measure, class, predict, true, set, base, pai...   \n",
       "1.0             [domain, learn, modality, target, representati...   \n",
       "2.0             [naive, probability, class, baye, text, featur...   \n",
       "3.0             [test, model, method, system, word, likelihood...   \n",
       "4.0             [word, model, language, vector, train, learn, ...   \n",
       "\n",
       "                                                             Text  \n",
       "Dominant_Topic                                                     \n",
       "0.0             [[evaluation, measure, hierarchical, view, nov...  \n",
       "1.0             [[find, learn, robust, joint, representation, ...  \n",
       "2.0             [[naive, bayes, text, introduction, theory, in...  \n",
       "3.0             [[find, structure, genome, symbolic, sequence,...  \n",
       "4.0             [[language, model, engineering, apply, abstrac...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6fa15422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets change the Topic index name so we dont be confuse by the name\n",
    "index = grouped_df.index\n",
    "grouped_df['Topic_num'] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23e073e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dominant_Topic\n",
       "0.0    0.0\n",
       "1.0    1.0\n",
       "2.0    2.0\n",
       "3.0    3.0\n",
       "4.0    4.0\n",
       "5.0    5.0\n",
       "6.0    6.0\n",
       "7.0    7.0\n",
       "8.0    8.0\n",
       "9.0    9.0\n",
       "Name: Topic_num, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df['Topic_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "25f2618f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_num</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>[measure, class, predict, true, set, base, pai...</td>\n",
       "      <td>[[evaluation, measure, hierarchical, view, nov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[domain, learn, modality, target, representati...</td>\n",
       "      <td>[[find, learn, robust, joint, representation, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[naive, probability, class, baye, text, featur...</td>\n",
       "      <td>[[naive, bayes, text, introduction, theory, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[test, model, method, system, word, likelihood...</td>\n",
       "      <td>[[find, structure, genome, symbolic, sequence,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>[word, model, language, vector, train, learn, ...</td>\n",
       "      <td>[[language, model, engineering, apply, abstrac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Topic_num                                           Keywords  \\\n",
       "Dominant_Topic                                                                 \n",
       "0.0                   0.0  [measure, class, predict, true, set, base, pai...   \n",
       "1.0                   1.0  [domain, learn, modality, target, representati...   \n",
       "2.0                   2.0  [naive, probability, class, baye, text, featur...   \n",
       "3.0                   3.0  [test, model, method, system, word, likelihood...   \n",
       "4.0                   4.0  [word, model, language, vector, train, learn, ...   \n",
       "\n",
       "                                                             Text  \n",
       "Dominant_Topic                                                     \n",
       "0.0             [[evaluation, measure, hierarchical, view, nov...  \n",
       "1.0             [[find, learn, robust, joint, representation, ...  \n",
       "2.0             [[naive, bayes, text, introduction, theory, in...  \n",
       "3.0             [[find, structure, genome, symbolic, sequence,...  \n",
       "4.0             [[language, model, engineering, apply, abstrac...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the result of the grouping of each Text with its topic number on different datagram\n",
    "df_char = grouped_df[['Topic_num','Keywords', 'Text']]\n",
    "df_char.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5707890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the index name of the new dataframe as well so we wont get confused \n",
    "df_char.index.names = ['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ae8063e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_num</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>[measure, class, predict, true, set, base, pai...</td>\n",
       "      <td>[[evaluation, measure, hierarchical, view, nov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[domain, learn, modality, target, representati...</td>\n",
       "      <td>[[find, learn, robust, joint, representation, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[naive, probability, class, baye, text, featur...</td>\n",
       "      <td>[[naive, bayes, text, introduction, theory, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[test, model, method, system, word, likelihood...</td>\n",
       "      <td>[[find, structure, genome, symbolic, sequence,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>[word, model, language, vector, train, learn, ...</td>\n",
       "      <td>[[language, model, engineering, apply, abstrac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[walk, graph, random, vertex, representation, ...</td>\n",
       "      <td>[[online, learn, social, representation, compu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>[model, neural, network, word, translation, in...</td>\n",
       "      <td>[[collaborative, deep, learn, system, science,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>[sequence, part, word, mathematical, equation,...</td>\n",
       "      <td>[[detect, phrase, university, apply, informati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>[solver, factorization, fast, machine, python,...</td>\n",
       "      <td>[[learn, research, submit, revise, publish, fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>[term, weight, text, stem, scheme, document, c...</td>\n",
       "      <td>[[stem, ultra, stem, improve, automatic, text,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Topic_num                                           Keywords  \\\n",
       "index                                                                 \n",
       "0.0          0.0  [measure, class, predict, true, set, base, pai...   \n",
       "1.0          1.0  [domain, learn, modality, target, representati...   \n",
       "2.0          2.0  [naive, probability, class, baye, text, featur...   \n",
       "3.0          3.0  [test, model, method, system, word, likelihood...   \n",
       "4.0          4.0  [word, model, language, vector, train, learn, ...   \n",
       "5.0          5.0  [walk, graph, random, vertex, representation, ...   \n",
       "6.0          6.0  [model, neural, network, word, translation, in...   \n",
       "7.0          7.0  [sequence, part, word, mathematical, equation,...   \n",
       "8.0          8.0  [solver, factorization, fast, machine, python,...   \n",
       "9.0          9.0  [term, weight, text, stem, scheme, document, c...   \n",
       "\n",
       "                                                    Text  \n",
       "index                                                     \n",
       "0.0    [[evaluation, measure, hierarchical, view, nov...  \n",
       "1.0    [[find, learn, robust, joint, representation, ...  \n",
       "2.0    [[naive, bayes, text, introduction, theory, in...  \n",
       "3.0    [[find, structure, genome, symbolic, sequence,...  \n",
       "4.0    [[language, model, engineering, apply, abstrac...  \n",
       "5.0    [[online, learn, social, representation, compu...  \n",
       "6.0    [[collaborative, deep, learn, system, science,...  \n",
       "7.0    [[detect, phrase, university, apply, informati...  \n",
       "8.0    [[learn, research, submit, revise, publish, fa...  \n",
       "9.0    [[stem, ultra, stem, improve, automatic, text,...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a441ff",
   "metadata": {},
   "source": [
    "### Word Counts of Topic Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2210cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the topics from the model and count the words of keywords\n",
    "from collections import Counter\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in data_ready for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "embedding_wieghts = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        embedding_wieghts.append(weight)\n",
    "        out.append([word, i , weight, counter[word]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4da794b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6222bc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_wieghts = np.reshape(embedding_wieghts , (-1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f463f773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02520594, 0.01918529, 0.01364142, 0.01352893, 0.01335979,\n",
       "        0.01205861, 0.01171972, 0.00923755, 0.00901146, 0.00831845],\n",
       "       [0.01928682, 0.01579707, 0.01060551, 0.01052721, 0.00998539,\n",
       "        0.00989439, 0.00977355, 0.00931035, 0.00827993, 0.00763779],\n",
       "       [0.01399363, 0.01258384, 0.01202483, 0.00929958, 0.00775381,\n",
       "        0.00606169, 0.00583469, 0.00572149, 0.00546143, 0.00531534],\n",
       "       [0.01444087, 0.01288268, 0.00943054, 0.00928264, 0.00889764,\n",
       "        0.00868222, 0.00840844, 0.0078552 , 0.00742039, 0.00712041],\n",
       "       [0.03332662, 0.02771569, 0.0131271 , 0.01162938, 0.01014967,\n",
       "        0.00881109, 0.00846772, 0.00708055, 0.00702598, 0.00692899],\n",
       "       [0.0118991 , 0.01003485, 0.00822909, 0.00807226, 0.00663862,\n",
       "        0.00561219, 0.00517596, 0.00478393, 0.00445851, 0.00412425],\n",
       "       [0.02363105, 0.01873809, 0.01751926, 0.01697313, 0.0144425 ,\n",
       "        0.01134231, 0.01053981, 0.00964806, 0.00911196, 0.00904496],\n",
       "       [0.01369436, 0.01286334, 0.01224997, 0.00945194, 0.00886529,\n",
       "        0.00829497, 0.0075232 , 0.00545718, 0.0049958 , 0.00429201],\n",
       "       [0.00369661, 0.00333535, 0.00302606, 0.00236646, 0.00218544,\n",
       "        0.00216076, 0.00163604, 0.0015539 , 0.00142997, 0.00139216],\n",
       "       [0.01875337, 0.01529778, 0.01041482, 0.00812578, 0.00723206,\n",
       "        0.00716763, 0.00600474, 0.00596275, 0.00552899, 0.00550463]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_wieghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c0a86e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025205942\n",
      "0.019286824\n",
      "0.013993629\n",
      "0.014440872\n",
      "0.03332662\n",
      "0.011899096\n",
      "0.023631053\n",
      "0.013694364\n",
      "0.0036966112\n",
      "0.01875337\n"
     ]
    }
   ],
   "source": [
    "# 10 Topics wieghts for each 10 keywords \n",
    "# get the max wieght of each topic \n",
    "topics_embedding_wieghts = []\n",
    "for topic_w in embedding_wieghts:\n",
    "    max = np.amax(topic_w)\n",
    "    topics_embedding_wieghts.append(max)\n",
    "    print(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3bc6f7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['measure', 0, 0.025205942, 421],\n",
       " ['class', 0, 0.019185288, 428],\n",
       " ['predict', 0, 0.013641417, 340],\n",
       " ['true', 0, 0.013528929, 176],\n",
       " ['set', 0, 0.01335979, 1063],\n",
       " ['base', 0, 0.012058615, 803],\n",
       " ['pair', 0, 0.011719716, 328],\n",
       " ['hierarchical', 0, 0.009237546, 154],\n",
       " ['figure', 0, 0.009011456, 590],\n",
       " ['system', 0, 0.008318447, 976],\n",
       " ['domain', 1, 0.019286824, 450],\n",
       " ['learn', 1, 0.015797071, 1271],\n",
       " ['modality', 1, 0.010605507, 139],\n",
       " ['target', 1, 0.010527206, 417],\n",
       " ['representation', 1, 0.009985393, 739],\n",
       " ['source', 1, 0.009894388, 491],\n",
       " ['feature', 1, 0.0097735515, 634],\n",
       " ['datum', 1, 0.009310347, 986],\n",
       " ['set', 1, 0.008279927, 1063],\n",
       " ['adaptation', 1, 0.0076377937, 140],\n",
       " ['naive', 2, 0.013993629, 94],\n",
       " ['probability', 2, 0.01258384, 472],\n",
       " ['class', 2, 0.012024833, 428],\n",
       " ['baye', 2, 0.009299585, 60],\n",
       " ['text', 2, 0.0077538146, 702],\n",
       " ['feature', 2, 0.0060616853, 634],\n",
       " ['word', 2, 0.0058346926, 2908],\n",
       " ['conditional', 2, 0.005721486, 118],\n",
       " ['sample', 2, 0.0054614334, 352],\n",
       " ['document', 2, 0.0053153387, 486],\n",
       " ['test', 3, 0.014440872, 893],\n",
       " ['model', 3, 0.012882676, 3051],\n",
       " ['method', 3, 0.009430539, 833],\n",
       " ['system', 3, 0.0092826355, 976],\n",
       " ['word', 3, 0.00889764, 2908],\n",
       " ['likelihood', 3, 0.008682225, 321],\n",
       " ['document', 3, 0.008408444, 486],\n",
       " ['sequence', 3, 0.007855199, 945],\n",
       " ['information', 3, 0.0074203904, 605],\n",
       " ['language', 3, 0.0071204077, 1363],\n",
       " ['word', 4, 0.03332662, 2908],\n",
       " ['model', 4, 0.027715694, 3051],\n",
       " ['language', 4, 0.013127103, 1363],\n",
       " ['vector', 4, 0.01162938, 891],\n",
       " ['train', 4, 0.010149666, 1201],\n",
       " ['learn', 4, 0.008811092, 1271],\n",
       " ['set', 4, 0.00846772, 1063],\n",
       " ['datum', 4, 0.007080549, 986],\n",
       " ['representation', 4, 0.0070259753, 739],\n",
       " ['table', 4, 0.0069289855, 665],\n",
       " ['walk', 5, 0.011899096, 72],\n",
       " ['graph', 5, 0.01003485, 166],\n",
       " ['random', 5, 0.008229089, 164],\n",
       " ['vertex', 5, 0.008072255, 45],\n",
       " ['representation', 5, 0.006638616, 739],\n",
       " ['learn', 5, 0.0056121936, 1271],\n",
       " ['social', 5, 0.005175965, 51],\n",
       " ['network', 5, 0.004783925, 1400],\n",
       " ['label', 5, 0.0044585126, 261],\n",
       " ['method', 5, 0.004124249, 833],\n",
       " ['model', 6, 0.023631053, 3051],\n",
       " ['neural', 6, 0.01873809, 1338],\n",
       " ['network', 6, 0.017519262, 1400],\n",
       " ['word', 6, 0.01697313, 2908],\n",
       " ['translation', 6, 0.0144425, 1132],\n",
       " ['input', 6, 0.011342312, 796],\n",
       " ['learn', 6, 0.010539813, 1271],\n",
       " ['output', 6, 0.009648063, 677],\n",
       " ['train', 6, 0.009111959, 1201],\n",
       " ['layer', 6, 0.009044956, 725],\n",
       " ['sequence', 7, 0.013694364, 945],\n",
       " ['part', 7, 0.012863338, 321],\n",
       " ['word', 7, 0.012249967, 2908],\n",
       " ['mathematical', 7, 0.009451936, 110],\n",
       " ['equation', 7, 0.008865293, 165],\n",
       " ['time', 7, 0.008294974, 636],\n",
       " ['dictionary', 7, 0.0075232033, 126],\n",
       " ['result', 7, 0.0054571787, 813],\n",
       " ['identify', 7, 0.0049958015, 67],\n",
       " ['phrase', 7, 0.0042920136, 386],\n",
       " ['solver', 8, 0.0036966112, 17],\n",
       " ['factorization', 8, 0.003335352, 28],\n",
       " ['fast', 8, 0.0030260636, 101],\n",
       " ['machine', 8, 0.0023664618, 688],\n",
       " ['python', 8, 0.0021854383, 22],\n",
       " ['library', 8, 0.0021607573, 28],\n",
       " ['implementation', 8, 0.0016360407, 68],\n",
       " ['interface', 8, 0.001553902, 10],\n",
       " ['code', 8, 0.00142997, 63],\n",
       " ['model', 8, 0.0013921638, 3051],\n",
       " ['term', 9, 0.01875337, 724],\n",
       " ['weight', 9, 0.0152977845, 660],\n",
       " ['text', 9, 0.010414823, 702],\n",
       " ['stem', 9, 0.008125783, 141],\n",
       " ['scheme', 9, 0.007232057, 136],\n",
       " ['document', 9, 0.0071676346, 486],\n",
       " ['category', 9, 0.0060047414, 144],\n",
       " ['word', 9, 0.00596275, 2908],\n",
       " ['network', 9, 0.00552899, 1400],\n",
       " ['summary', 9, 0.005504631, 89]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All Keywords for all Topics\n",
    "# ['Word', 'Topic id', 'wieght', 'counter']\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "328274e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to data frame\n",
    "df_keywords = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "185f5990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Word Count and Weights of Topic Keywords\n",
    "# import seaborn as sns\n",
    "# import matplotlib.colors as mcolors \n",
    "\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)\n",
    "# cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "\n",
    "# for i, ax in enumerate(axes.flatten()):\n",
    "#     ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "#     ax_twin = ax.twinx()\n",
    "#     ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "#     ax.set_ylabel('Word Count', color=cols[i])\n",
    "#     ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n",
    "#     ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "#     ax.tick_params(axis='y', left=False)\n",
    "#     ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "#     ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "# fig.tight_layout(w_pad=2)    \n",
    "# fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "744a4054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 50 articles\n",
    "fifty_articles = df_char['Text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e6473fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the 2D list to one big string text so we can apply the sequence methodolgy over the text\n",
    "from itertools import chain\n",
    "\n",
    "articles_string = []\n",
    "\n",
    "for article in fifty_articles:\n",
    "    corpus = ''\n",
    "    # article[0] take the first element since its a nested list we need only the first ele\n",
    "    for string in article[0]:\n",
    "        corpus += \" \" + string  \n",
    "    article = corpus\n",
    "    articles_string.append(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "18d734da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eaxmple of article\n",
    "len(articles_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3c74d333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_char.drop('index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "19291db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7998916e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87712/3978813885.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_char['corpus'] = articles_string\n"
     ]
    }
   ],
   "source": [
    "# save each clean text to our dataframe\n",
    "df_char['corpus'] = articles_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a6f6f116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_num</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>[measure, class, predict, true, set, base, pai...</td>\n",
       "      <td>[[evaluation, measure, hierarchical, view, nov...</td>\n",
       "      <td>evaluation measure hierarchical view novel ec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[domain, learn, modality, target, representati...</td>\n",
       "      <td>[[find, learn, robust, joint, representation, ...</td>\n",
       "      <td>find learn robust joint representation cyclic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[naive, probability, class, baye, text, featur...</td>\n",
       "      <td>[[naive, bayes, text, introduction, theory, in...</td>\n",
       "      <td>naive bayes text introduction theory introduc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[test, model, method, system, word, likelihood...</td>\n",
       "      <td>[[find, structure, genome, symbolic, sequence,...</td>\n",
       "      <td>find structure genome symbolic sequence docto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>[word, model, language, vector, train, learn, ...</td>\n",
       "      <td>[[language, model, engineering, apply, abstrac...</td>\n",
       "      <td>language model engineering apply abstract des...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Topic_num                                           Keywords  \\\n",
       "index                                                                 \n",
       "0.0          0.0  [measure, class, predict, true, set, base, pai...   \n",
       "1.0          1.0  [domain, learn, modality, target, representati...   \n",
       "2.0          2.0  [naive, probability, class, baye, text, featur...   \n",
       "3.0          3.0  [test, model, method, system, word, likelihood...   \n",
       "4.0          4.0  [word, model, language, vector, train, learn, ...   \n",
       "\n",
       "                                                    Text  \\\n",
       "index                                                      \n",
       "0.0    [[evaluation, measure, hierarchical, view, nov...   \n",
       "1.0    [[find, learn, robust, joint, representation, ...   \n",
       "2.0    [[naive, bayes, text, introduction, theory, in...   \n",
       "3.0    [[find, structure, genome, symbolic, sequence,...   \n",
       "4.0    [[language, model, engineering, apply, abstrac...   \n",
       "\n",
       "                                                  corpus  \n",
       "index                                                     \n",
       "0.0     evaluation measure hierarchical view novel ec...  \n",
       "1.0     find learn robust joint representation cyclic...  \n",
       "2.0     naive bayes text introduction theory introduc...  \n",
       "3.0     find structure genome symbolic sequence docto...  \n",
       "4.0     language model engineering apply abstract des...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_char.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "852c729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_char[['Topic_num', 'corpus']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "512c6180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0,\n",
       "        ' evaluation measure hierarchical view novel economic abstract hierarchical address problem classify item hierarchy class important issue hierarchical evaluation complicate hierarchical relation class several measure propose hierarchical hierarchy way paper study problem hierarchical analyze abstract key component exist performance measure propose alternative generic view hierarchical evaluation introduce correspond novel measure propose state art empirically test large domain text empirical result illustrate desirable behavior exist approach propose method overcome method range case introduction hierarchical address problem classify item class past year mainstream research place enough emphasis presence relation case hierarchical relation gradually change put hierarchical partly real world edge system service hierarchical scheme organize datum research hierarchical become algorithm ill address large scale problem hierarchically relate class initial result large scale problem show hierarchical improve information retrieval research question hierarchical remain open issue properly evaluate hierarchical standard problem establish precision establish evaluation measure hierarchical assessment complicate due relation class error upper level hierarchy wrongly classify document class music class food severe deep level classify document progressive rock alternative rock several evaluation measure propose hierarchical hierarchy way none widely compare performance number comparative study performance measure publish literature early study find limited particular type graph distance measure review measure present focus single label task provide empirical multi label task object assign newspaper article belong politic measure role hierarchy emphasize provide comprehensive empirical analysis performance focus evaluation cluster method study provide interest miss important aspect problem evaluate algorithm abstract problem order describe exist evaluation measure common framework work present address issue analyze abstract key component exist performance measure group exist evaluation measure main type provide generic framework base network set theory provide critical overview exist performance measure propose framework introduce new evaluation measure address important state art measure provide comparative empirical result large text variety remainder paper organize follow section introduce problem present general requirement measure pro pose framework present exist evaluation measure propose framework introduce new measure address problem state art measure section present case study analysis propose measure exist describe empirical set datum empirical analysis measure section present discuss empirical result section conclude summarize remain open issue framework hierarchical measure section present new framework performance measure describe characterize support notation general requirement evaluation present base interesting problem appear hierarchical proceed presentation propose section describe analyze measure notation feature vector instance set class instance contrast class consider class organize taxonomy taxonomy usually case nod single parent direct case multiple figure respectively hierarchy cyclic graph case hierarchy impose parent child relation implie instance belong belong ancestor class taxonomy thus usually pair set class subclass relationship follow loss assume subclass relationship case relationship example part assume property always hold relationship tree figure tree class hierarchy graph transitivity property hold article consider hierarchy cycle denote descendant ancestor class respectively parent class denote assume instance class leaf class general problem hierarchical commonly measure accuracy appropriate due relation exist class hierarchical performance measure class hierarchy order evaluate properly algorithm account several type error accord hierarchy consider hierarchy figure assume true class test instance system output predict class evaluation system punish error second system severe prediction unrelated sub tree order measure severity error hierarchical several interest issue address figure present case require special handle node surround circle true node surround rectangle predict case sub group pair problem select pair predict true class account calculation distance measure problem concern way error calculate pair predict true class figure present specialization error predict class descendant true class figure depict ancestor true class select case desire behavior measure reduce penalty accord distance true class predict third case call alternative path present scenario way reach true class start class measure path order evaluate performance system select path minimize distance class measure error reasonable figure predict class true alternative path case involve multiple path ancestor predict class figure present scenario common case measure pair true predict class compare pair possible depend score assign reasonable pairing minimize error figure argue prediction base evidence thus compare figure present case predict class probably match true class typically case predict class true class distant call case long distance problem pair base measure pair base measure assign cost pair predict true class figure class pair class sum correspond cost give total error let set predict true class single test instance index instance omit due set augment default predict default true respectively correspond class predict class pair true class vice distance predict class true class exceed specialization specialization alternative path pair problem long distance problem figure interest case evaluate hierarchical node round circle true class node surround rectangle predict class threshold long distance problem figure predict class pair default true class let cost predict class instead true class matrix contain cost possible pair predict true include default class pair base measure typically calculate cost pair predict class true class minimum distance number edge class short path connect intuition close class similar therefore less severe error elaborate cost measure assign weight weight decrease move top bottom holden distance default class usually set large value spirit fairness aim evaluation sure pair class return system true class way minimize overall error formulate follow optimization problem state denote alignment furthermore state default predict true class align default class solely predict true class parameter low upper bind allow number true class predict class pair set predict class pair exactly true class parameter limit number predict label true class pair constraint directly imply mean default true class align predict class default predict class true class problem correspond pair problem bipartite node respectively predict true class important note pair look several approach problem graph pair optimization exist polynomial solution pair problem second graph framework allow illustrate cost base measure propose far relate model cost minimization problem flow network model class pair network direct graph edge associate low upper capacity denote respectively edge denote network vector vertex bad constraint yield reason similar constraint conservation property denote set edge enter leave vertex respectively edge associate cost represent cost edge total cost minimum cost min satisfy capacity conservation constraint quantity minimize network problem constraint latter problem correspond capacity explain follow theorem state bind capacity interval exist minimal cost quantity edge theorem network capacity integer value exist feasible mini mum cost feasible integer value arc standard minimal cost tee particular pairing problem bipartite graph represent network add source edge source set second set source extra node edge ensure conservation constraint pair base one thus obtain follow network framework figure include predict true default true class default predict include edge source predict class default predict predict class true class default true true class sink sink source edge exist default predict default true require constraint set capacity interval edge express possible number pair predict true class participate interval pair predict true class restrict network indicate pair consider calculation evaluation measure put solve network value evaluation measure show pair solution minimum cost interval source predict class true class sink way pair perform due constraint problem capacity interval predict class true class exclude default capacity interval theorem imply value predict true class predict true class pair pair capacity bind correspond value problem source predict capacity interval meaning predict class align least true true class capacity interval mean true class align least predict predict class default true class capacity interval default predict class true class capacity interval source default predict capacity interval mention footnote sink capacity interval correspond loose set compatible interval give last capacity interval impose constraint necessary ensure exist pair base measure majority exist pair base measure deal tree single label problem condition pair problem become single path exist predict true class complexity problem increase hierarchy problem current measure handle majority phenomena present section simple case pair base measure holden measure trivially pair single prediction single true label note default class exist equivalently correspond cost equal source sink propose network predict true depict respectively figure distance set edge path hierarchy weight call tree induce error cost measure propose tree possible pair predict true class calculation default class cost note extreme pair predict true label weight alternative similarity cosine class predict true distance hierarchy equation source sink dp figure tree induce error network measure dub graph induce error propose second large scale hierarchical text challenge base match pair predict true class handle hierarchy particular instance predict class pair true class default true multiple class pair default true class true class pair exactly predict class default several true class pair default predict class cost compute equation hierarchy multiple hierarchy path link predict class pair true class short path cost pair class default set positive value figure present concept match fail address pair problem section predict class true class figure pair parent one penalize pair distant class multi label graph induce accuracy propose straightforward extension call graph induce accuracy class allow participate pair extension method suitable pairing problem figure present source sink dp figure graph induce error network cost pairing class default set solve network optimization problem constraint default predict class pair default true class category set pair thus pair solve separately pair class default class class set previous pair base solution problem error calculate solve network instead directly error evaluation accuracy base measure value provide solve network measure bind well system close note case predict class true class source sink figure graph induce accuracy network pair respective default reach maximum value equal denominator result value advantage measure pair base measure account correct prediction system true set base measure performance measure category base operation entire set predict true possibly include ancestor oppose pair base consider pair predict true class set base measure distinct augmentation information hierarchy calculation cost measure base augment set augmentation crucial attempt capture hierarchical relation class set augment ancestor true predict class augment set predict true approach mainly adopt calculate symmetric loss hierarchical precision recall symmetric loss calculate set initial set instead measure become standard symmetric note quantity symmetric loss express false positive false negative rate respectively hierarchical precision recall nominator measure express true positive rate write note symmetric loss term set base measure pair problem figure long distance problem figure rely pair true predict class exist set base measure measure mainly way set predict true class augment ancestor predict true class add equation descendant true predict class latter true predict class sub maximum penalty several ancestor correctly predict approach add ancestor augment set alter equation introduce tolerance specialization figure assume true class predict class accord equation add class class base equation remove avoid penalize method equation tolerate figure true class predict class accord equation class add base equation remove avoid penalize method drawback measure tend favor category system stop prediction early hierarchy figure tolerate specialization specialization low common ancestor measure approach propose paper base hierarchical version recall add ancestor predict true class add ancestor undesirable penalize error happen node ancestor attempt address propose low common recall measure measure concept low common ancestor graph theory low common ancestor tree surround circle true class node surround rectangle predict class figure case dag set single possible one necessarily node furth root order dag concept short path give set contain path connect set cost cost path correspond edge unweighted example figure path worth note general case path min set single multi label extend compare true set predict set low common ancestor cost path path figure well give true predict class compute element similarly figure let connect path connect path connect connect connect path connect path second connect path set contain set give set true class set predict class set contain example give set true class set predict class set ex graph path path well similarly graph path path figure augment set true class ex ex graph respectively base graph true predict set class order set base measure calculate case figure next step calculate cost measure base set case example figure set prefer approach symmetric account addition ignore lead system prefer predict single usually cost gain extra behavior observe result real section consider undesirable graph ex ex create sub graph graph connect vice figure remove ex break condition vice figure reduce set figure instead figure minimal graph augment set true predict class word graph comprise node necessary connect node lead remove order obtain minimal solve follow maximization problem minimal graph extension constraint set constraint require class node initial set include constraint enforce existence ex constraint path path min least node constraint limit total number minimum require order able satisfy constraint constraint imply existence least path connect class node constraint imply connect least class node way solve maximization problem create possible respect constraint choose lead high procedure computation ally expensive reason devise approximation present procedure path contain contain end procedure procedure sort number descend order repeat satisfied procedure satisfy check constraint optimization problem top redundancy removal end bottom redundancy removal return end procedure part procedure select path path share common select path end end well end end end return end procedure main procedure decompose procedure return approximation minimum amount order satisfy constraint maximization problem achieve initially descend number connect perform top bottom remove redundant node already include list step select minimum case path exist connect choose lead small possible interesting issue arise class ancestor predict true class set assume example system predict instance belong node system assign ancestor extra ancestor lead high score increase size set happen ancestor ancestor address issue remove set descendant set remove descendant recall purely set base actually bridge pair base set base measure base augment set predict true base set however lead pair predict true node measure characterize combine advantage type measure case study section apply various measure select case order demon representative pair base measure choose graph induce error set base select hierarchical version precision recall measure symmetric loss ancestor predict true label order augment set class propose pair base measure set base version precision recall measure order illustrate advantage well type measure regard provide parenthesis transformation propose subsection order comparable measure implement fast tool write open source available download pair base method require maximum distance pair default case study threshold set base situation present section highlight important challenge hierarchical case study situation appear list case motivate propose measure section present result real system handle pair problem case capture situation number true predict level elementary figure symmetric variant lead symmetric hierarchical precision recall show table true propose version recall result hierarchical version version version ignore low common ancestor nod hierarchical version hand give high result behavior increase result hierarchical measure version measure give result case always compute symmetric augment set tool available metric account sum false false negative case pair base inappropriate problem match fig penalize unmatched predict class maximum ignore fact proximity correct category similarly figure provide suitable allow multiple category match figure number true predict label table result measure figure case figure example show account ancestor undesirable propose approach set base measure hierarchy still tree predict figure bad figure true class set base measure propose measure continue give result hierarchical version recall predict true hybrid augment graph create least common ancestor measure order close way account distance predict true set base measure ignore thus figure augment set set base measure figure number true predict label distance close true class table result measure figure figure augment set become set base measure remain case advantage exist set base measure handle alternative path figure assume single label hierarchy path root worth note simple path length pair base measure remain compare figure desirable behavior due short path exist set base measure calculate error account alternative path hand measure behave pair base due low common advantage exist set base measure table result measure figure figure path length lead combine elementary case subsection study case combination mention problem lead variable behavior set base evaluation measure figure present match discuss case set base method give similar result accord table multiple path phenomenon hierarchical version recall nod low common ancestor share example wish show certain case exist set base method give result case behavior exist set base measure desirable version figure combine pair problem alternative path single none set base method table result measure figure case show figure previous sub graph connect leave node node reason hierarchical version recall show table due common ancestor ignore set base measure count thus penalize error figure combine pair problem alternative path single example previous set base measure behave propose measure table result measure figure multiple path count due pair true predict pair base method often count path multiple counting increase error estimate method figure illustrate case figure edge count length pair base measure case tend overestimate comparison set base extra predict add descendant pair base error estimate increase least size set base measure increase reasonable change hierarchy table result measure figure figure set base count path figure present similar example accord table error propose transformation increase increase decrease whole path form count twice pair base method however double count desirable error severe measure penalize error extra penalization roughly proportional distance set base measure set base measure give result child less severe error child count common advantage pair base measure case distant prediction aim case show type sure handle large distance predict true pair base measure compute distance pair predict true node distance certain standard maximum distance assign set base measure threshold number ancestor predict true node augment set impose common ancestor order connect least predict true distance equal threshold case show figure maximum distance pair base set base measure reasonable request low common ancestor distance add distance low common ancestor true predict distance reach operation hierarchy figure lead figure order directly connect result measure present table example measure maximum distance threshold necessary computational reason due long distance problem discuss section pair base measure threshold set base show example decrease decrease due multiple count time undesirable discuss section figure distant prediction problem table result measure figure specialization previous case predict true category leave always case figure present simple example inner node true predict category show table case receive score table result measure figure figure simple specialization case figure show case specialization describe section evaluation measure treat type error argue predict direct ancestor case show table measure treat error regard simple example show figure consider error severe true category predict example figure predict predict figure measure lead high error estimate case similar example specialization lead observation summary table present advantage disadvantage measure scope case present section pair base measure handle alternative set base propose measure able deal measure handle specialization specialization way set base measure deal pair problem produce augment handle propose consider long distance problem pair base measure handle set base measure handle threshold propose section path count special feature pair base measure time set base measure certain case general conclusion propose measure always behave least well exist measure category wish pair base set base measure suggest propose instead exist case choose measure due multiple count path discuss section multiple count path time undesirable lead penalization case serve order observe newly propose hierarchical evaluation measure conclude discussion behavior measure benchmark case order observe real data system follow section alternative path specialization pair problem long distance problem multiple path count table summary table regard evaluation measure certain situation mean handle raw empirical study section apply various evaluation measure prediction system participate large scale hierarchical text pascal challenge goal section study real datum extent rank system choice hierarchical evaluation measure type hierarchical measure subsection present second subsection discus evaluation measure include comparison subsection result study section demonstrate certain measure behave desirably section show method rank real system practice provide separate task participant participate system base page crawl open directory project human edit hierarchical directory web hierarchy transform tree instance deep level hierarchy transfer thus lead hierarchy maximum depth small regard number category instance base call large respectively large contain almost abstract instance train exception english abstract therefore comprise category large depth small select way lead similar size maximize ratio instance process result much hierarchy small transform remove cycle still appear large way word abstract stem stem map feature category map category instance sparse vector format collection category collection feature frequency instance mapping category stem leaf hierarchy valid allow instance inner inner assign instance dummy leaf create evaluation purpose direct child instance transfer child table present basic statistic almost small multi label less hierarchy however ratio train instance category comparable large ratio train instance category equal accounting multi ratio become similar small much small large much large term train test instance small large inst multi label factor train cat multi train cat depth table basic statistic show number number train testing average number true category instance ratio training instance instance give maximum depth graph evaluation measure statistical test evaluation measure study present section accuracy reproduce report challenge evaluation rank system create order measure correlation kendall rank correlation statistical test evaluation measure literature provide special statistical test hierarchical measure perform micro sign test similar hierarchical measure provide score instance score always average number instance assume performance system instance accord evaluation performance system instance accord evaluation number time number time perform null hypothesis distribution mean system well system accord always case large scale value approximately compute standard normal distribution worth stress test account system perform well ignore much perform alter sign rank test account performance instance reason simplicity paper test result subsection present result discuss measure table present result system participate challenge recall least system evaluate measure rank descend order number bracket indicate system rank correspond measure system mean statistically result accord statistical test table present kendall rank observation rank accuracy hierarchical measure show hierarchical measure treat problem interesting observation rank hierarchical measure handle multiple label instance important aspect method table present average number prediction instance system instance participant treat task single discuss previous section treatment multi label greatly multi label rare much hierarchical measure however example assign multiple label perform accord hierarchical measure accord accuracy perform bad hierarchical measure accuracy result great opportunity hierarchical measure reward penalize system decision discuss previous hierarchical measure vary way handle tree almost reveal tree hierarchy main reason accord table highly correlate main way treat multiple possible tree correlate calculation perform augment set observe correlation main reason limit label provide opportunity deal pair problem propose error perform table present result system table result interest rank change kendall correlation evaluation measure rank characteristic greatly behavior measure important observation hierarchical measure measure perform transformation low correlation hierarchical measure consideration average prediction instance system observe relation rank measure compute error measure account count reason tend penalize system high average prediction likely mistake way rest set base measure handle augment true predict set class transformation perform error much close idea calculation accuracy reason measure correlate measure penalize less system high average table average number prediction instance system prediction manage extra system table small result deviation rank highlight bold table kendall correlation evaluation measure rank small table present result system number system participate much large instead statistical reason experiment lead safe accord table system high av number prediction behavior measure important system allow classify inner train instance directly belong hierarchical measure correlate correlation much low case system inner node treat mistake table average prediction instance small system accuracy correlation much higher fully correlate high correlation continue observe reason explain previously interest case system predict category label average true label instance mean large number predict label predict label still vicinity correct accuracy penalize behavior give low rank system penalize less severely set base measure punish much close look system show fact bad return path root leaf predict instead leaf set base measure still penalize leaf ancestor wrong penalize pair base measure set base pair base measure observe less extreme measure category point rank close respectively measure hybrid measure conduct set operation pair base second kind match true predict node order create augment set characteristic help overcome weakness measure respective category way behavior desirable third face computational issue hierarchical measure problem originate large scale dag reality contain circle remove avoid computational evaluation measure maximum path threshold mean force low common ancestor depth respectively create dummy similar idea long distance problem figure discuss section long distance problem dummy order link threshold order avoid dummy computational reason table present result distance threshold system complex hierarchy system table small result interesting rank table kendall rank correlation evaluation measure rank small label treat measure interestingly rank remain highly correlate accuracy compare number system high enough order safe conclusion interesting observation disagreement system show table predict category stance time predict category true penalize unmatched true natural penalize system problem allow multi instead system notice hierarchy performance rank become less correlated interesting observation com correlate correlated average number table average prediction instance small prediction instance system table large result maximum path threshold interesting rank table kendall rank correlation evaluation measure rank maximum path threshold large table present result maximum path threshold main purpose experiment show measure remain largely parameter indeed rank compare table general advice keep maximum path parameter large possible table present result system main system allow classify inner table show number prediction instance similar interesting observation system rank high accord accuracy hierarchical measure rank bad system accord table system provide label table average prediction instance large system table large result maximum path instance label penalize heavily measure base mention interesting observation fully cor relate anymore fact correlate hierarchy become complicate propose measure behave finally system predict small number instance accord measure rank bad high low compare system computation case compare transformation propose extra reason propose similar experiment maximum path threshold con system result similar one thus omit interest space experiment present section real system hierarchical measure treat compete system measure show present rank system flat evaluation commonly often provide false indication system perform well ignore hierarchical dependency class treat error equally guide research away method incorporate hierarchy process show variant hierarchical measure give rank condition goal choose show hierarchical evaluation measure give absolute value rank system finally show scale task issue require attention table kendall correlation evaluation measure rank maximum path threshold large system table large result maximum path threshold interesting rank conclusion work study problem evaluate performance hierarchical method work abstract present key point exist performance measure propose group pair set base measure former group attempt match prediction true class measure distance contrast set base measure hierarchical relation order augment set predict true set symmetric augment label set order model pair introduce novel generic frame work base set base measure provide frame work base set operation salient feature measure stress present common formalism contribution paper proposal measure address several exist measure pro pose along exist assess way apply select order demonstrate pro study empirically large base wikipedia characteristic result show hierarchical measure behave especially case table kendall correlation evaluation measure rank maximum path threshold large table average prediction instance large system propose measure show robust behavior compare counterpart result support initial premise measure adequate evaluate hierarchical categorization system analysis show certain rare case pair base measure behave case set base method propose paper exhibit desirable behavior propose pair base measure actually hybrid measure pair way select propose instead hierarchical still open issue propose measure combine pro propose reference common ancestor page hierarchical workshop relational datum mining page fernando elena empirical com hierarchical performance measure extraction proceed international conference knowledge base intelligent information engineering system volume part page exploit taxonomy learn overlap concept international joint conference intel page claudio incremental hierarchical journal machine learn research review performance evaluation measure hierarchical vancouver singer large margin hierarchical cation proceed international conference machine learning page hierarchical protein couple receptor swarm intelligence symposium page categorize hide web database international conference management datum page kendall new measure rank correlation volume page functional annotation gene hierarchical text categorization workshop link biological mining biological semantic large scale hierarchical workshop forum measure case study area image proceed international conference multimedia information retrieval page survey hierarchical application domain datum mining knowledge systematic analysis performance task information processing management clare hierarchical predictive clustering tree functional intelligence volume note computer science page sun hierarchical text evaluation ieee international conference datum mining page performance measurement frame work hierarchical text information science technology individual comparison rank examination text categorization method pro annual international conference research development information retrieval page']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6581222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Number 0.0\n",
      "Topic features [['measure', 0, 0.025205942, 421], ['class', 0, 0.019185288, 428], ['predict', 0, 0.013641417, 340], ['true', 0, 0.013528929, 176], ['set', 0, 0.01335979, 1063], ['base', 0, 0.012058615, 803], ['pair', 0, 0.011719716, 328], ['hierarchical', 0, 0.009237546, 154], ['figure', 0, 0.009011456, 590], ['system', 0, 0.008318447, 976]] for article number 1\n",
      "The unique Charchters in the article [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Char to int {' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "Int to char {0: ' ', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
      "Topic features after editing the string to char int [[[13, 5, 1, 19, 21, 18, 5], 0, 0.025205942, 421], [[3, 12, 1, 19, 19], 0, 0.019185288, 428], [[16, 18, 5, 4, 9, 3, 20], 0, 0.013641417, 340], [[20, 18, 21, 5], 0, 0.013528929, 176], [[19, 5, 20], 0, 0.01335979, 1063], [[2, 1, 19, 5], 0, 0.012058615, 803], [[16, 1, 9, 18], 0, 0.011719716, 328], [[8, 9, 5, 18, 1, 18, 3, 8, 9, 3, 1, 12], 0, 0.009237546, 154], [[6, 9, 7, 21, 18, 5], 0, 0.009011456, 590], [[19, 25, 19, 20, 5, 13], 0, 0.008318447, 976]]\n",
      "The number of total characters are 34205\n",
      "\n",
      "The character vocab size is 27\n",
      "Total Patterns:  34105\n",
      "Example of X [[ 0]\n",
      " [ 5]\n",
      " [22]\n",
      " [ 1]\n",
      " [12]\n",
      " [21]\n",
      " [ 1]\n",
      " [20]\n",
      " [ 9]\n",
      " [15]\n",
      " [14]\n",
      " [ 0]\n",
      " [13]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [19]\n",
      " [21]\n",
      " [18]\n",
      " [ 5]\n",
      " [ 0]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [ 5]\n",
      " [18]\n",
      " [ 1]\n",
      " [18]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [12]\n",
      " [ 0]\n",
      " [22]\n",
      " [ 9]\n",
      " [ 5]\n",
      " [23]\n",
      " [ 0]\n",
      " [14]\n",
      " [15]\n",
      " [22]\n",
      " [ 5]\n",
      " [12]\n",
      " [ 0]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [15]\n",
      " [14]\n",
      " [15]\n",
      " [13]\n",
      " [ 9]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [19]\n",
      " [20]\n",
      " [18]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [20]\n",
      " [ 0]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [ 5]\n",
      " [18]\n",
      " [ 1]\n",
      " [18]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [12]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [18]\n",
      " [ 5]\n",
      " [19]\n",
      " [19]\n",
      " [ 0]\n",
      " [16]\n",
      " [18]\n",
      " [15]\n",
      " [ 2]\n",
      " [12]\n",
      " [ 5]\n",
      " [13]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [12]\n",
      " [ 1]\n",
      " [19]\n",
      " [19]\n",
      " [ 9]\n",
      " [ 6]\n",
      " [25]\n",
      " [ 0]]\n",
      "The X input 34105\n",
      "The length of the Y output 34105\n",
      "The Y output [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "The Y output [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "input_features = []\n",
    "output = []\n",
    "total_n_v = 0\n",
    "topic_features = []\n",
    "\n",
    "# Data contain array [number of topic, text of the article]\n",
    "# get the features for all articles so remove the data[0:1] to only data\n",
    "for index, article  in enumerate(data[0:1]):\n",
    "    # Get the text of the article\n",
    "    raw_text = article[1]\n",
    "    # Number of the topic\n",
    "    print(\"Topic Number\", article[0])\n",
    "    # Topic features are the keywords with thier wights and counts\n",
    "    topic_features = [x for x in out if x[1] == article[0]]\n",
    "    print(\"Topic features {} for article number {}\".format(topic_features, index+1))\n",
    "    \n",
    "    # Get the Chars of the article\n",
    "    chars = sorted(list(set(raw_text)))\n",
    "    # make dictionary of each charchter that exist in the article and give it index (Chrachter, index)\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "    # the same as before but here we have (index,Char)\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "    print(\"The unique Charchters in the article\",chars)\n",
    "    print(\"Char to int\" , char_to_int)\n",
    "    print(\"Int to char\", int_to_char)\n",
    "    \n",
    "    # change the word to char int representation\n",
    "    for keyword in topic_features:\n",
    "        word = keyword[0]\n",
    "        data_x_keyword = [char_to_int[char] for char in word]\n",
    "        keyword[0] = data_x_keyword\n",
    "        \n",
    "    print(\"Topic features after editing the string to char int\", topic_features)\n",
    "\n",
    "    \n",
    "    # Prints the total characters and character vocab size\n",
    "    n_chars = len(raw_text)\n",
    "    n_vocab = len(chars)\n",
    "    \n",
    "    total_n_v += n_vocab\n",
    "\n",
    "    print(\"The number of total characters are\", n_chars)\n",
    "    print(\"\\nThe character vocab size is\", n_vocab)\n",
    "    \n",
    "    #Prepares dataset where the input is sequence of 100 characters and target is next character.\n",
    "    seq_length = 100\n",
    "\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "\n",
    "    for i in range(0, n_chars - seq_length, 1):\n",
    "          seq_in = raw_text[i:i + seq_length]\n",
    "          seq_out = raw_text[i + seq_length]\n",
    "          dataX.append([char_to_int[char] for char in seq_in])\n",
    "          dataY.append(char_to_int[seq_out])\n",
    "\n",
    "    n_patterns = len(dataX)\n",
    "    print (\"Total Patterns: \", n_patterns)\n",
    "    \n",
    "    from keras.utils import np_utils\n",
    "    # reshapes X to be [samples, time steps, features]\n",
    "    \n",
    "    X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "    print(\"Example of X\", X[0])\n",
    "    print(\"The X input\", len(X))\n",
    "    # get all the features of the article words\n",
    "    input_features.append(X)\n",
    "    # one hot encodes the output variable\n",
    "    Y = np_utils.to_categorical(dataY)\n",
    "    print(\"The length of the Y output\", len(Y))\n",
    "    print(\"The Y output\", Y[1])\n",
    "    print(\"The Y output\", Y[0])\n",
    "    \n",
    "    output.append(Y)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9dc7f2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34105, 100, 1)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the features are the first element of nested array\n",
    "input_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4747540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features of the Vocab Words\n",
    "X1= input_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be8bcfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Features, keywords only for first Topc since we are trying over first article \n",
    "X2 = []\n",
    "for topic in topic_features:\n",
    "    X2.append(topic[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "afe5b1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[13, 5, 1, 19, 21, 18, 5],\n",
       " [3, 12, 1, 19, 19],\n",
       " [16, 18, 5, 4, 9, 3, 20],\n",
       " [20, 18, 21, 5],\n",
       " [19, 5, 20],\n",
       " [2, 1, 19, 5],\n",
       " [16, 1, 9, 18],\n",
       " [8, 9, 5, 18, 1, 18, 3, 8, 9, 3, 1, 12],\n",
       " [6, 9, 7, 21, 18, 5],\n",
       " [19, 25, 19, 20, 5, 13]]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "45d35cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_wieghts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cfd12092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34105"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of the output\n",
    "len(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "848ca7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_Y = output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2dd8cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim =100\n",
    "max_length =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3ffdff27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_topics = len(topics)\n",
    "n_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "13163366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional,Dropout,Concatenate\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "712c7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Dropout\n",
    "# from keras.layers import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "74abf9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding two embedding layers one for the Vocabulary and one for the topics \n",
    "\n",
    "embedding_layer_1 = Embedding(n_vocab,\n",
    "                            embedding_dim,\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "\n",
    "embedding_layer_2 = Embedding(n_topics,\n",
    "                            10,\n",
    "                            weights= [embedding_wieghts],\n",
    "                            input_length = 10,\n",
    "                            trainable=False)\n",
    "\n",
    "\n",
    "s1rnn = Sequential()\n",
    "s1rnn.add(embedding_layer_1)\n",
    "s1rnn.add(LSTM(256,  input_shape=(input_features[0].shape[1], embedding_dim),return_sequences=True))\n",
    "s1rnn.add(Dropout(0.2))\n",
    "s1rnn.add(Dense(1))\n",
    "\n",
    "s2rnn = Sequential()\n",
    "s2rnn.add(embedding_layer_2)\n",
    "s2rnn.add(LSTM(256,  input_shape=(10, 10),return_sequences=True))\n",
    "s2rnn.add(Dropout(0.2))\n",
    "s2rnn.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0816467d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 100)          2700      \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 100, 256)          365568    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 100, 256)          0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 100, 1)            257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 368,525\n",
      "Trainable params: 365,825\n",
      "Non-trainable params: 2,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "s1rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3884a177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 10, 10)            100       \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 10, 256)           273408    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 10, 256)           0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10, 1)             257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 273,765\n",
      "Trainable params: 273,665\n",
      "Non-trainable params: 100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "s2rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4b7f5673",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The added layer must be an instance of class Layer. Received: layer=KerasTensor(type_spec=TensorSpec(shape=(None, 110, 1), dtype=tf.float32, name=None), name='concatenate_1/concat:0', description=\"created by layer 'concatenate_1'\") of type <class 'keras.engine.keras_tensor.KerasTensor'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_87712/2031865521.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmerged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms2rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#model.compile(loss='mean_squared_error', optimizer='RMSprop', metrics=['accuracy'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       raise TypeError('The added layer must be an instance of class Layer. '\n\u001b[0m\u001b[1;32m    179\u001b[0m                       f'Received: layer={layer} of type {type(layer)}.')\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Received: layer=KerasTensor(type_spec=TensorSpec(shape=(None, 110, 1), dtype=tf.float32, name=None), name='concatenate_1/concat:0', description=\"created by layer 'concatenate_1'\") of type <class 'keras.engine.keras_tensor.KerasTensor'>."
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "merged = Concatenate(axis=1)([s1rnn.output,s2rnn.output])\n",
    "model.add(merged)\n",
    "model.add(Dense(1))\n",
    "#model.compile(loss='mean_squared_error', optimizer='RMSprop', metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',  metrics=['accuracy'])\n",
    "model.fit([X1,X2], Y,batch_size=128, nb_epoch=20, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab959fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(n_vocab, embedding_dim, input_length=max_length))\n",
    "model1.add(Embedding(n_topics, embedding_dim, input_length=max_length))\n",
    "\n",
    "model1.add(LSTM(256, input_shape=(X.shape[1], embedding_dim),return_sequences=True))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(LSTM(256))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(real_Y.shape[1], activation='softmax'))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam',  metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f6c5e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1.fit(input_features, real_Y, epochs = 20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338bcc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the sequence similar to above methods. Gets the generated string using the model.\n",
    "def predict_next_n_chars(pattern, n):\n",
    "    for i in range(n):\n",
    "      x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "      prediction = model1.predict(x, verbose=0)\n",
    "      print (int_to_char[np.argmax(prediction)], end = '')   #get next char index.\n",
    "      seq_in = [int_to_char[value] for value in pattern]\n",
    "      pattern.append(np.argmax(prediction))\n",
    "      pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a2dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#picks a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "input_str = ''.join([int_to_char[value] for value in pattern])\n",
    "print (\"Seed -\",  input_str, sep = '\\n\\n')\n",
    "print (\"\\nGenerated string -\\n\")\n",
    "\n",
    "predict_next_n_chars(pattern, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6721430",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"Many modern service systems rely on a network of hub facilities to help concentrate flows of freight or passengers\"\n",
    "\n",
    "#Uses the first 100 characters from given input_str as input to generate next 200 characters. \n",
    "input_str = input_str.lower()\n",
    "input_string = ''\n",
    "for each in input_str:\n",
    "    if each in chars:\n",
    "           if (len (input_string) < 100):\n",
    "                input_string += each\n",
    "\n",
    "pattern = []\n",
    "pattern.append([char_to_int[char] for char in input_string])\n",
    "\n",
    "print (\"Seed -\",  input_str, sep = '\\n\\n')\n",
    "print (\"\\nGenerated string -\\n\")\n",
    "predict_next_n_chars(pattern[0], 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9777ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"recently most of the research based on the Neural networks models, because they are fast and accurate models \"\n",
    "\n",
    "#Uses the first 100 characters from given input_str as input to generate next 200 characters. \n",
    "input_str = input_str.lower()\n",
    "input_string = ''\n",
    "for each in input_str:\n",
    "    if each in chars:\n",
    "           if (len (input_string) < 100):\n",
    "                input_string += each\n",
    "\n",
    "pattern = []\n",
    "pattern.append([char_to_int[char] for char in input_string])\n",
    "\n",
    "print (\"Seed -\",  input_str, sep = '\\n\\n')\n",
    "print (\"\\nGenerated string -\\n\")\n",
    "predict_next_n_chars(pattern[0], 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6909bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
